\chapter{Multi-variable Differential Calculus} % 多元微分学
\section{Directional Derivatives and Total Differential}
\begin{leftbarTitle}{Directional Derivative}\end{leftbarTitle}
\begin{definition}{Directional Derivative}
    Let \(U\subset \mathbb{R}^n\) be an open set, \(f: U\to \mathbb{R}^{1}\),
    \(\mathbf{e}\) is a unit vector in \(\mathbb{R}^{n}\), \(\mathbf{x}^{0}\in U\). Define
    \[
    u(t) = f(\mathbf{x}^{0} + t\mathbf{e}).
    \]
    If the derivative of \(u\) at \(t=0\) 
    \[ 
        u'(0) = \lim_{t \to 0} \frac{u(t) - u(0)}{t} = 
        \lim_{t \to 0} \frac{f(\mathbf{x}^{0} + t\mathbf{e}) - f(\mathbf{x}^{0})}{t} 
    \] 
    exists and is finite, 
    it is called the \textbf{directional derivative} of \(f\) at \(\mathbf{x}^{0}\) in the direction \(\mathbf{e}\), 
    denoted by \(\frac{\partial f}{\partial \mathbf{e}}(\mathbf{x}^{0})\). 
    It is the rate of change of \(f\) at \(\mathbf{x}^{0}\) in the direction \(\mathbf{e}\).
\end{definition}

Consider the following set of unit coordinate vectors: \(\mathbf{e}_{1},\mathbf{e}_{2},\cdots,\mathbf{e}_{n}\).
Let \(\mathbf{e}_{i}=\left( 0, 0, \cdots, 0, 1, 0, \cdots, 0 \right)  \) denote the standard orthonormal basis 
in \(\mathbb{R}^{n}\), where the 1 appears in the \(i\)-th position. That is,
\[
    \langle \mathbf{e}_{i}, \mathbf{e}_{j} \rangle = \delta_{i j} = \begin{cases}
    1, & i = j, \\
    0, & i \neq j.
    \end{cases}
\]
For a function \( f \), the directional derivative of \( f \) at the point \( \mathbf{x}_{0} \) 
in the direction of \( \mathbf{e}_{i} \) 
is called the \( i \)th first-order \textbf{partial derivative} of \( f \) at \(\mathbf{x}^{0}\), denoted by
\[
\frac{\partial f}{\partial x_i}(\mathbf{x}^{0}) \quad \text{or} 
\quad \mathrm{D}_i f(\mathbf{x}^{0})  \quad \text{or} 
\quad f_{x_i}(\mathbf{x}^{0}) \quad (i = 1, 2, \cdots, n).
\]
\( \mathrm{D}_i = \frac{\partial}{\partial x_i} \) is called the \( i \)th partial differential operator (\( i = 1, 2, \cdots, n \)).

Let \(\mathbf{e}_{i}=\sum_{i=0}^{n} \mathbf{e}_{i}\cos\alpha_{i}\) be a unit vector, 
where \(\sum_{i=0}^{n} \cos^2\alpha_{i} = 1\).
If \(\frac{\partial f}{\partial x_{i}}\) is continuous at \(\mathbf{x}^0\), 
then the directional derivative of \(f\) at \(\mathbf{x}^0\) along the direction \(\mathbf{e}\) is given by:
\[
\frac{\partial f}{\partial \mathbf{e}}(\mathbf{x}^0) = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x}^0) \cos \alpha_{i}.
\]

This is the formula for \textbf{expressing a directional derivative using partial derivatives}.


\begin{note}
    Let \(\mathbf{e}\) be a direction, then \(\|-\mathbf{e}\| = \|\mathbf{e}\| = 1\), 
    which implies that \(-\mathbf{e}\) is also a direction. At this point, we have:
    \[
    \frac{\partial f}{\partial (-\mathbf{e})}(\mathbf{x}^{0}) = -\frac{\partial f}{\partial \mathbf{e}}(\mathbf{x}^{0}).
    \]
\end{note}



\begin{definition}{Jacobian Matrix (Gradient)}
    Let
    \[
    Jf(\mathbf{x}) = (\mathrm{D}_1 f(\mathbf{x}), \mathrm{D}_2 f(\mathbf{x}), \dots, \mathrm{D}_n f(\mathbf{x})),
    \]
    which is called the \textbf{Jacobian matrix} of the function \( f \) at the point \( \mathbf{x} \), 
    (a \( 1 \times n \) matrix) whose counterpart is the first-order derivative of a single-variable function.

    Henceforth, we represent the point \(\mathbf{x}\) in \( \mathbb{R}^n \) 
    and its increments \(\mathbf{h}\) as column vectors.
    In this way, the differential of the function can be expressed using matrix multiplication as follows:
    \[
    \mathrm{d}f(\mathbf{x}^{0})(\mathbf{\Delta x}) = Jf(\mathbf{x}^{0}) \mathbf{\Delta x}.
    \]
    The Jacobian matrix of the function \( f \) is also frequently denoted as 
    \(\mathrm{grad}\,f\) (or \(\nabla f\)), that is,
    \[
    \nabla f(\mathbf{x}) =\mathrm{grad}\,f(\mathbf{x}) = Jf(\mathbf{x}),
    \]
    which is called the \textbf{gradient} of the scalar function \( f \).
\end{definition}



\begin{leftbarTitle}{Total Differential}\end{leftbarTitle}
\begin{definition}{Total Differential}
    Let \(U\subset \mathbb{R}^n\) be an open set, \(f: U\to \mathbb{R}^{1}\), \(\mathbf{x}^{0}\in U\),
    \(\mathbf{\Delta x}=\left( \Delta x_{1},\Delta x_{2},\cdots,\Delta x_{n} \right) \in \mathbb{R}^{n}\). If
    \[
    f(\mathbf{x}^{0} + \mathbf{\Delta x}) - f(\mathbf{x}^{0}) = 
    \sum_{i=1}^n A_{i} \Delta x_{i} + o(\|\mathbf{\Delta x}\|) \qquad (\|\mathbf{\Delta x}\| \to 0),
    \]
    where \(A_{1}, A_{2}, \dots, A_{n}\) are constants independent of \(\mathbf{\Delta x}\), 
    then the function \(f\) is said to be \textbf{differentiable} at the point \(\mathbf{x}^{0}\), 
    and the linear main part \(\sum_{i=1}^n A_{i} \Delta x_{i}\) is called the \textbf{total differential} 
    of \(f\) at \(\mathbf{x}^{0}\), 
    denoted as
    \[
    df(\mathbf{x}^{0})(\mathbf{\Delta x}) = \sum_{i=1}^n A_{i} \Delta x_{i}.
    \]
    If \(f\) is differentiable at every point in the open set \(U\), 
    then \(f\) is called a differentiable function on \(U\).    
\end{definition}

\begin{theorem}{Conditions of Differentiability}
    \begin{description}
        \item[Necessary Condition] If an \(n\)-variable function \(f\) is differentiable at the point \(\mathbf{x}_{0}\), 
        then \(f\) is continuous at \(\mathbf{x}^{0}\) and 
        possesses first-order partial derivatives \(\frac{\partial f}{\partial x_{i}}(\mathbf{x}^{0})\) 
        at \(\mathbf{x}^{0}\) for \(i = 1, 2, \dots, n\), and\footnote{
            It is referred to as the total differential formula, and the more common form is
            \[
                \mathrm{d}f(x_{0},y_{0})=
                \frac{\partial f}{\partial x}(x_{0},y_{0})\,\mathrm{d}x+\frac{\partial f}{\partial y}(x_{0},y_{0})\,\mathrm{d}y.
            \]
        }
        \[
        \mathbf{A} = \left( A_{1}, A_{2}, \dots, A_{n} \right)  = Jf(\mathbf{x}^{0}) = 
        \left(\mathrm{D}_{1}f(\mathbf{x}^{0}), \mathrm{D}_{2}f(\mathbf{x}^{0}), \dots, \mathrm{D}_{n}f(\mathbf{x}^{0}) \right).
        \]
        However, the converse is not true.
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item[Sufficient Condition] Let \(U \subset \mathbb{R}^n\) be an open set, 
        and let \(f: U \to \mathbb{R}^1\) be an \(n\)-variable function. 
        If \(Jf = \left( \mathrm{D}_{1}f, \mathrm{D}_{2}f, \dots, \mathrm{D}_{n}f \right)\) 
        is continuous at \(\mathbf{x}^{0}\) 
        (i.e., \(\frac{\partial f}{\partial x_{i}}\) is continuous at \(\mathbf{x}^{0}\) for \(i = 1, 2, \dots, n\)), 
        then \(f\) is differentiable at \(\mathbf{x}^{0}\)\footnote{
            In fact, this condition can be relaxed to require that one partial derivative exists at the point, 
            while the remaining \(n-1\) partial derivative functions are continuous at that point.
        }.

        However, the converse is not necessarily true.
    \end{description}    
\end{theorem}

\begin{note}
    The continuity of the derivative function at \(\mathbf{x}^{0}\) implies that 
    the original function \(f\) is differentiable in some neighborhood of \(\mathbf{x}^{0}\).
\end{note}
\begin{proof}
    Taking the function of three variables as an example.

    Assume the \(3\)-ary function \(f: \mathbb{R}^3 \to \mathbb{R}\) meets:
    \begin{enumerate}
        \item There exists \(f_{z}(x_{0},y_{0},z_{0})\). 
        \item The partial derivative functions \(f_{x}(x,y,z)\) and \(f_{y}(x,y,z)\) are continuous at \((x_{0},y_{0},z_{0})\),
            i.e. there are partial derivatives in some neighborhood of \((x_{0},y_{0},z_{0})\).
    \end{enumerate}
    Consider the total increment of \(f\) at the point \((x_{0},y_{0},z_{0})\):
    \begin{align*}
        \Delta f 
        &= \underbrace{\left[ f(x_0 + \Delta x, y_0 + \Delta y, z_0 + \Delta z) - f(x_0, y_0 + \Delta y, z_0 + \Delta z) \right]}_{I_1}\\
        &+ \underbrace{\left[ f(x_0, y_0 + \Delta y, z_0 + \Delta z) - f(x_0, y_0, z_0 + \Delta z) \right]}_{I_2}\\
        &+ \underbrace{\left[ f(x_0, y_0, z_0 + \Delta z) - f(x_0, y_0, z_0) \right]}_{I_3}.
    \end{align*}
    For \(I_{1}, I_{2}\), by the Lagrange's Mean Value Theorem of unary functions, 
    there exist \(\theta_{1}, \theta_{2} \in (0,1)\) such that
    \begin{gather*}
        I_{1}=f_{x}(x_{0}+\theta_{1}\Delta x,y_{0}+\Delta y,z_{0}+\Delta z)\Delta x,\\
        I_{2}=f_{y}(x_{0},y_{0}+\theta_{2}\Delta y,z_{0}+\Delta z)\Delta y.
    \end{gather*}
    Then by the continuity of the their partial derivatives at \((x_{0},y_{0},z_{0})\), we have
    \[
        \lim_{\Delta x, \Delta y, \Delta z \to 0} I_{1} = f_{x}(x_{0},y_{0},z_{0})\Delta x, \quad
        \lim_{\Delta x, \Delta y, \Delta z \to 0} I_{2} = f_{y}(x_{0},y_{0},z_{0})\Delta y.
    \]
    They can be expressed in terms of infinitesimals(\(\rho = \sqrt{\Delta x^2 + \Delta y^2 + \Delta z^2}\)):
    \begin{align*}
        I_{1}=f_{x}(x_{0},y_{0},z_{0})\Delta x + \alpha_{1}\Delta x, \quad \alpha_{1}\to 0(\rho\to 0),\\
        I_{2}=f_{y}(x_{0},y_{0},z_{0})\Delta y + \alpha_{2}\Delta y, \quad \alpha_{2}\to 0(\rho\to 0).
    \end{align*}
    For \(I_{3}\), by the definition of the partial derivative \(f_{z}(x,y,z)\) at \((x_{0},y_{0},z_{0})\), we have
    \[
        I_{3}=f_{z}(x_{0},y_{0},z_{0})\Delta z + \alpha_{3}\Delta z, \quad \alpha_{3}\to 0(\rho\to 0).
    \]
    Accordingly, 
    \begin{align*}
        \Delta f &= I_{1} + I_{2} + I_{3} \\
        &= \left[ f_{x}(x_{0},y_{0},z_{0})\Delta x + \alpha_{1}\Delta x \right] + \left[ f_{y}(x_{0},y_{0},z_{0})\Delta y + \alpha_{2}\Delta y \right] + \left[ f_{z}(x_{0},y_{0},z_{0})\Delta z + \alpha_{3}\Delta z \right] \\
        &= f_{x}(x_{0},y_{0},z_{0})\Delta x + f_{y}(x_{0},y_{0},z_{0})\Delta y + f_{z}(x_{0},y_{0},z_{0})\Delta z + \left[ \alpha_{1}\Delta x + \alpha_{2}\Delta y + \alpha_{3}\Delta z \right].
    \end{align*}
    Apparently, 
    \[
        \lim_{\rho \to 0} \frac{\alpha_{1}\Delta x + \alpha_{2}\Delta y + \alpha_{3}\Delta z}{\rho} = 0,
    \]
    i.e. \(\alpha_{1}\Delta x + \alpha_{2}\Delta y + \alpha_{3}\Delta z = o(\rho)\). 
    Therefore, \(f(x,y,z)\) is differentiable at \((x_{0},y_{0},z_{0})\), which completes the proof.
\end{proof}

\begin{note}{(At some point)}
    \begin{enumerate}
        % 偏导数存在, 未必连续
        \item The existence of partial derivatives at a point does not necessarily imply their continuity at that point.
            A classic counterexample is:
            \[
            f(x,y) = \begin{cases}
            \frac{xy}{x^2 + y^2}, & (x,y) \neq (0,0), \\
            0, & (x,y) = (0,0).
            \end{cases}
            \]
            Here, \(f_x(0,0) = 0\) and \(f_y(0,0) = 0\), but \(f_x(x,y)\) and \(f_y(x,y)\) are not continuous at \((0,0)\).
        % 邻域偏导数有界, 必定连续
        \item (\underline{partial derivatives bounded \(\Rightarrow\) continuous}) If the partial derivatives exist and are bounded in a neighborhood of a point, 
            then they are continuous at that point.
        % 一点连续且所有方向导数存在, 不一定可微
        \item Even if all directional derivatives exist at a point and the function is continuous at that point, 
            it does not necessarily imply that the function is differentiable at that point.
            A classic counterexample is:
            \[
            f(x,y) = \begin{cases}
            \frac{x^3}{x^2 + y^2}, & (x,y) \neq (0,0), \\
            0, & (x,y) = (0,0).
            \end{cases}
            \]
            Here, all directional derivatives of \(f\) exist at \((0,0)\), 
            and \(f\) is continuous at \((0,0)\), but \(f\) is not differentiable at \((0,0)\).
            Another counterexample is:
            \[
            f(x,y) = \sqrt{|xy|},
            \]
            which is continuous at \((0,0)\) and has all directional derivatives equal to \(0\) at \((0,0)\),
            but is not differentiable at \((0,0)\).
    \end{enumerate}
\end{note}

\begin{proof}
    Take the function of two variables as an example.
    Assume the bivariate function \(f: \mathbb{R}^2 \to \mathbb{R}\) meets:
    \(\frac{\partial f}{\partial x}\) and \(\frac{\partial f}{\partial y}\) exist and are bounded
    in some neighborhood of \((x_{0},y_{0})\).

    Consider the total increment of \(f\) at the point \((x_{0},y_{0})\):
    \begin{align*}
        \Delta f 
        &= \left[ f(x_0 + \Delta x, y_0 + \Delta y) - f(x_0, y_0 + \Delta y) \right]\\
        &+ \left[ f(x_0, y_0 + \Delta y) - f(x_0, y_0) \right].
    \end{align*}
    By the Lagrange's Mean Value Theorem of unary functions, there exist \(\theta_{1}, \theta_{2} \in (0,1)\) such that
    \begin{gather*}
        \Delta f = f_{x}(x_{0}+\theta_{1}\Delta x,y_{0}+\Delta y)\Delta x + f_{y}(x_{0},y_{0}+\theta_{2}\Delta y)\Delta y.
    \end{gather*}
    Since \(\frac{\partial f}{\partial x}\) and \(\frac{\partial f}{\partial y}\) are bounded 
    in some neighborhood of \((x_{0},y_{0})\),
    \[
    \lim_{(\Delta x, \Delta y) \to (0,0)} \Delta f = 0,
    \]
    i.e. \(f(x,y)\) is continuous at \((x_{0},y_{0})\), which completes the proof.
\end{proof}

\section{Higher-Order Partial Derivatives and Differentiability}
\begin{leftbarTitle}{Higher-Order Partial Derivatives}\end{leftbarTitle}
If the first-order partial derivative of \(f\), \(\frac{\partial f}{\partial x_i}\), 
itself possesses partial derivatives, then the second-order partial derivative of \(f\) is defined, 
and is denoted as follows(the first is also called the mixed partial derivative):
\[
f_{x_i x_j} = \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial}{\partial x_j} \left( \frac{\partial f}{\partial x_i} \right), 
\quad f_{x_i x_i} = \frac{\partial^2 f}{\partial x_i^2} = \frac{\partial}{\partial x_i} \left( \frac{\partial f}{\partial x_i} \right), 
\quad i, j = 1, 2, \dots, n.
\]

Similarly, higher-order partial derivatives of order \(3,4,\cdots m,\cdots\) can be defined.

The following theorem provides the conditions under which mixed partial derivatives are equal.

\begin{theorem}{Conditions for Equality of Mixed Partial Derivatives}
    \begin{enumerate}\label{thm:condition_equality}
        \item Let $U \subset \mathbb{R}^2$ be an open set, and $f: U \to \mathbb{R}$ be a function of two variables. 
            If the partial derivatives $f_{x}, f_{y}$ and $f_{xy}$ exist in some neighborhood of $(x_0, y_0) \in U$, 
            and \(f_{xy}\) is continuous at $(x_0, y_0)$, then \(f_{yx}\) also exists at \((x_0, y_0)\), and
            \[
            f_{yx}(x_0, y_0) = f_{xy}(x_0, y_0).
            \]
        \item Let \(U \subset \mathbb{R}^n\) be an open set, and \(f: U \to \mathbb{R}\) be a function of \(n\) variables. 
            If the partial derivatives $f_{x_i}, f_{x_j}$ and \(f_{x_{i}x_{j}}\) exist in some neighborhood of 
            \(\mathbf{x}^{0} = (x_1^0, x_2^0, \ldots, x_n^0) \in U\), 
            and \(f_{x_{i}x_{j}}\) is continuous at \(\mathbf{x}^{0}\), then \(f_{x_j x_i}\) exist at \(\mathbf{x}^{0}\), and
            \[
            f_{x_j x_i}(\mathbf{x}^{0}) = f_{x_i x_j}(\mathbf{x}^{0}).
            \]
    \end{enumerate}
\end{theorem}


\begin{proof}

\end{proof}

\begin{leftbarTitle}{Higher-Order Differentiability}\end{leftbarTitle}
Suppose \(z=f(x,y)\) has continuous partial derivatives in the domain \(U\subset \mathbb{R}^2\). 
Then \(z\) is differentiable, and
\[
    \mathrm{d}z = \frac{\partial z}{\partial x} \mathrm{d}x + \frac{\partial z}{\partial y} \mathrm{d}y.
\]
If \(z\) also has continuous second-order partial derivatives, 
then \(\frac{\partial z}{\partial x}\) and \(\frac{\partial z}{\partial y}\) are also differentiable, 
and thus \(\mathrm{d}z\) is differentiable. 
We call the differential of \(\mathrm{d}z\) the second-order differential of \(z\), denoted as
\[
    \mathrm{d}^2z = \mathrm{d}(\mathrm{d}z).
\]
In general, based on the \(k\)-th order differential \(\mathrm{d}^kz\) of \(z\), 
its \((k+1)\)-th order differential (if it exists) is defined as
\[
    \mathrm{d}^{k+1}z = \mathrm{d}(\mathrm{d}^kz), \quad k = 1, 2, \cdots .
\]
Due to the fact that for the independent variables \( x \) and \( y \), we always have
\[
    \mathrm{d}^2 x = \mathrm{d}(\mathrm{d}x) = 0, \qquad \mathrm{d}^2 y = \mathrm{d}(\mathrm{d}y) = 0,
\]
the second-order differential of \( z = f(x, y) \) is given by
\begin{align*}
    \mathrm{d}^2 z &= \mathrm{d}(\mathrm{d}z) 
        = \mathrm{d}\left( \frac{\partial z}{\partial x} \mathrm{d}x + \frac{\partial z}{\partial y} \mathrm{d}y \right) \\
    &= \mathrm{d}\left( \frac{\partial z}{\partial x} \right) \mathrm{d}x + \frac{\partial z}{\partial x} \mathrm{d}^2 x 
        + \mathrm{d}\left( \frac{\partial z}{\partial y} \right) \mathrm{d}y + \frac{\partial z}{\partial y} \mathrm{d}^2 y\\
    &= \left( \frac{\partial^2 z}{\partial x^2} \mathrm{d}x + \frac{\partial^2 z}{\partial x \partial y} \mathrm{d}y \right) \mathrm{d}x
        + \left( \frac{\partial^2 z}{\partial y \partial x} \mathrm{d}x + \frac{\partial^2 z}{\partial y^2} \mathrm{d}y \right) \mathrm{d}y\\
    &= \frac{\partial^2 z}{\partial x^2} (\mathrm{d}x)^2 + 2 \frac{\partial^2 z}{\partial x \partial y} \mathrm{d}x \mathrm{d}y + \frac{\partial^2 z}{\partial y^2} (\mathrm{d}y)^2,
\end{align*}
where \( (\mathrm{d}x)^2 \) and \( (\mathrm{d}y)^2 \) denote \( \mathrm{d}^2 x \) and \( \mathrm{d}^2 y \) respectively.
If we treat \( \frac{\partial}{\partial x} \), \( \frac{\partial}{\partial y} \) as operators for partial differentiation 
and define
\[
    \left( \frac{\partial}{\partial x} \right)^2 = \frac{\partial^2}{\partial x^2}, \quad
    \left( \frac{\partial}{\partial y} \right)^2 = \frac{\partial^2}{\partial y^2}, \quad
    \left( \frac{\partial}{\partial x} \frac{\partial}{\partial y} \right) = \frac{\partial^2}{\partial x \partial y},
\]
then the formulas for the first and second differentials can be written as
\[
    \mathrm{d}z = \left( \mathrm{d}x \frac{\partial}{\partial x} + \mathrm{d}y \frac{\partial}{\partial y} \right) z,
\]
\[
    \mathrm{d}^2 z = \left( \mathrm{d}x \frac{\partial}{\partial x} + \mathrm{d}y \frac{\partial}{\partial y} \right)^2 z.
\]
Similarly, we define
\[
    \left( \frac{\partial}{\partial x} \right)^p
    \left( \frac{\partial}{\partial y} \right)^q
    = \frac{\partial^{p+q}}{\partial x^p \partial y^q}
    = \frac{\partial^q}{\partial y^q}
    \left( \frac{\partial}{\partial x} \right)^p,
    \quad (p, q = 1, 2, \dots)
\]
It is easy to use mathematical induction to prove the formula for higher-order differentials:
\[
    \mathrm{d}^k z = \left( \mathrm{d}x \frac{\partial}{\partial x} + \mathrm{d}y \frac{\partial}{\partial y} \right)^k z, 
    \quad k = 1, 2, \cdots.
\]
For an \( n \)-variable function \( u = f(x_1, x_2, \dots, x_n) \), higher-order differentials can be similarly defined, and the following holds:
\[
    \mathrm{d}^k u 
    = \left( \mathrm{d}x_1 \frac{\partial}{\partial x_1} + \mathrm{d}x_2 \frac{\partial}{\partial x_2} 
    + \cdots + \mathrm{d}x_n \frac{\partial}{\partial x_n} \right)^k u, \quad k = 1, 2, \dots.
\]
\section{Differential of Vector-Valued Functions}
Consider an $n$-dimensional vector-valued function defined on a domain $U \subset \mathbb{R}^n$:
\begin{gather*}
    f: U \to \mathbb{R}^m, \\ 
    \mathbf{x} \mapsto \mathbf{y} = f(\mathbf{x})
\end{gather*}
Expressed in coordinate vector form:
\[
    \mathbf{y} =
    \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_m
    \end{pmatrix}
    =
    \begin{pmatrix}
    f_1(x_1, x_2, \dots, x_n) \\
    f_2(x_1, x_2, \dots, x_n) \\
    \vdots \\
    f_m(x_1, x_2, \dots, x_n)
    \end{pmatrix},
    \qquad \mathbf{x} = \begin{pmatrix} 
    x_1 \\ x_2 \\ \vdots \\ x_n 
    \end{pmatrix}  \in U
\]

\begin{enumerate}
    \item  If each component function $f_i(x_1, x_2, \dots, x_n)$ ($i=1,2,\dots,m$) is partially differentiable at $\mathbf{x}^0$, 
        then the vector-valued function $\mathbf{f}$ is differentiable at $\mathbf{x}^0$, and we define the matrix
        \[
            \left( \frac{\partial f}{\partial x_j} (\mathbf{x}^0) \right)_{m \times n}
            =
            \begin{pmatrix}
            \frac{\partial f_1}{\partial x_1}(\mathbf{x}^0) & \frac{\partial f_1}{\partial x_2}(\mathbf{x}^0) & \cdots & \frac{\partial f_1}{\partial x_n}(\mathbf{x}^0) \\
            \frac{\partial f_2}{\partial x_1}(\mathbf{x}^0) & \frac{\partial f_2}{\partial x_2}(\mathbf{x}^0) & \cdots & \frac{\partial f_2}{\partial x_n}(\mathbf{x}^0) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial f_m}{\partial x_1}(\mathbf{x}^0) & \frac{\partial f_m}{\partial x_2}(\mathbf{x}^0) & \cdots & \frac{\partial f_m}{\partial x_n}(\mathbf{x}^0)
            \end{pmatrix}
        \]

        This matrix is called the Jacobian matrix of $\mathbf{f}$ at $\mathbf{x}^0$, 
        denoted by $f'(\mathbf{x}^0)$ (or $\mathrm{D}f(\mathbf{x}^0)$, $J_f(\mathbf{x}^0)$).

        For the special case $m=1$, i.e., $n$-variable scalar function $z=f(x_1,x_2,\dots,x_n)$, 
        the derivative at $\mathbf{x}^0$ is
        \[
            f'(\mathbf{x}^0) = 
            \left( 
                \frac{\partial f}{\partial x_1}(\mathbf{x}^0), \frac{\partial f}{\partial x_2}(\mathbf{x}^0), 
                \cdots, \frac{\partial f}{\partial x_n}(\mathbf{x}^0) 
            \right)
        \]
        If the vector-valued function $\mathbf{f}$ is differentiable at every point in $U$, 
        then $\mathbf{f}$ is said to be differentiable on $U$, and the corresponding relationship is
        \[
        \mathbf{x} \in U \mapsto f'(\mathbf{x}) = J_f(\mathbf{x})
        \]
        where $f'(\mathbf{x})$ (or $\mathrm{D}f(\mathbf{x})$, $J_f(\mathbf{x})$) 
        denotes the derivative of $\mathbf{f}$ at $\mathbf{x}$ in $U$.
    \item  If every component function $f_i(x_1, x_2, \dots, x_n)$ $(i=1,2,\dots,m)$ of $\mathbf{f}$ 
        has continuous partial derivatives at $\mathbf{x}^0$, 
        then every element of the Jacobian matrix of $\mathbf{f}$ is continuous at $\mathbf{x}^0$. 
        In this case, $\mathbf{f}$ is said to have a continuous derivative at $\mathbf{x}^0$ as a vector-valued function.
        
        If the derivative of a vector-valued function $\mathbf{f}$ is continuous at every point in $U$, 
        then $\mathbf{f}$ is said to have a continuous derivative on $U$.
    \item  If there exists an $m \times n$ matrix $A$ that depends only on $\mathbf{x}^0$ (and not on $\Delta \mathbf{x}$), 
        such that in the neighborhood of $\mathbf{x}^0$,
        \[
        \Delta \mathbf{y} = f(\mathbf{x}^0 + \Delta \mathbf{x}) - f(\mathbf{x}^0) = A \Delta \mathbf{x} + o(\|\Delta \mathbf{x}\|)
        \]
        (where $\Delta \mathbf{x} = (\Delta x_1, \Delta x_2, \dots, \Delta x_n)^T$ is a column vector and 
        $\|\Delta \mathbf{x}\|$ denotes its norm), 
        then $f$ is said to be differentiable at $\mathbf{x}^0$ as a vector-valued function, 
        and $A\Delta \mathbf{x}$ is called the differential of $f$ at $\mathbf{x}^0$, denoted as $\mathrm{d}\mathbf{y}$. 
        If we denote $\Delta \mathbf{x}$ by 
        $\mathrm{d}\mathbf{x}$ ($\mathrm{d}\mathbf{x} = (\mathrm{d}x_1, \mathrm{d}x_2, \dots, \mathrm{d}x_n)^T$), then
        \[
            \mathrm{d}\mathbf{y} = A\,\mathrm{d}\mathbf{x}.
        \]

        If the vector-valued function $\mathbf{f}$ is differentiable at every point in $U$, 
        then $\mathbf{f}$ is said to be differentiable on $U$.
\end{enumerate}

Combining the above three points, we obtain the following unified statement:

A vector-valued function \(\mathbf{f}\) is continuous, differentiable, 
and has derivatives if and only if each of its coordinate component functions 
\(f_i(x_1, x_2, \dots, x_n)\) (\(i = 1, 2, \dots, m\)) is continuous, differentiable, and has derivatives.


\section{Derivatives of Composite Mappings (Chain Rule)}
Let \( U \subset \mathbb{R}^l \) and \( V \subset \mathbb{R}^n \) be open sets, and let 
\[
\mathbf{g}: U \to V \quad \text{and} \quad \mathbf{f}: V \to \mathbb{R}^m
\]
be mappings. If \( \mathbf{g} \) is derivative at \( \mathbf{u}^{0} \in U \) 
and \( \mathbf{f} \) is differentiable at \( \mathbf{x}^{0} = \mathbf{g}(\mathbf{u}^{0}) \), 
then the composite mapping \( \mathbf{f} \circ \mathbf{g} \) is differentiable at \( \mathbf{u}^{0} \), and:
\[
J(\mathbf{f} \circ \mathbf{g})(\mathbf{u}^{0}) = 
J\mathbf{f}(\mathbf{x}^{0}) J\mathbf{g}(\mathbf{u}^{0}).
\]

\begin{note}
    \begin{enumerate}
        \item  outer differentiable + inner derivative = total derivative
        \item  outer differentiable + inner differentiable = total differentiable
    \end{enumerate}
\end{note}

Specially, define \( z = f(x, y), (x,y)\subset D_{f}\subset \mathbb{R}^{2} \), 
\(\mathbf{g}:D_{g}\to \mathbb{R}^{2}, (u,v)\mapsto (x(u,v), y(u,v))\), 
and \(g(D_{g})\subset D_{f}\), 
then we have composite function
\[
z = f \circ \mathbf{g} = f\left[x(u,v), y(u,v)\right],\quad (u,v)\in D_{g}.
\]
\[
\mathbb{R}^{2}\xrightarrow{\mathbf{g}:\text{derivative}}\mathbb{R}^{2}\xrightarrow{f:\text{differentiable}}\mathbb{R}
\]
If \(\mathbf{g}\) is derivative at \((u_{0}, v_{0})\in D_{g}\), 
and \(f\) is differentiable at \((x_{0}, y_{0}) = \mathbf{g}(u_{0}, v_{0})\), 
then \(z = f \circ \mathbf{g}\) is differentiable at \((u_{0}, v_{0})\), and at the point,
\[
    \begin{bmatrix} 
        \frac{\partial z}{\partial u}   & \frac{\partial z}{\partial v} 
    \end{bmatrix} 
    =
    \begin{bmatrix}
        \frac{\partial z}{\partial x} & \frac{\partial z}{\partial y}
    \end{bmatrix}
    \begin{bmatrix} 
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{bmatrix} 
\]

\begin{proof}
    
\end{proof}

\begin{leftbarTitle}{Applications}\end{leftbarTitle}
As an important application of the chain rule, we have the following theorem on the \underline{differentiation of} 
\underline{determinants}.
% 行列式求导
\begin{theorem}
    For 
    \[
    \Delta(t) =
    \begin{vmatrix} 
    a_{11}(t) & a_{12}(t) & \cdots & a_{1n}(t) \\
    a_{21}(t) & a_{22}(t) & \cdots & a_{2n}(t) \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1}(t) & a_{n2}(t) & \cdots & a_{nn}(t)
    \end{vmatrix},
    \]
    where each element \(a_{ij}(t)\) is differentiable with respect to \(t\),
    then \(\Delta(t)\) is differentiable with respect to \(t\), and
    \[
    \frac{\mathrm{d}\Delta(t)}{\mathrm{d}t} =
    \sum_{j=1}^{n}
    \begin{vmatrix} 
    a_{11}(t) & a_{12}(t) & \cdots & a_{1n}(t) \\
    a_{21}(t) & a_{22}(t) & \cdots & a_{2n}(t) \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\mathrm{d}}{\mathrm{d}t}a_{1j}(t) & \frac{\mathrm{d}}{\mathrm{d}t}a_{2j}(t) & \cdots & \frac{\mathrm{d}}{\mathrm{d}t}a_{nj}(t) \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1}(t) & a_{n2}(t) & \cdots & a_{nn}(t)
    \end{vmatrix}
    \]
    where in each determinant on the right-hand side,
    the \(j\)-th column is replaced by the derivative of the \(j\)-th column of \(\Delta(t)\).
\end{theorem}

\vspace{0.7cm}
Another important application is \underline{homogeneous functions}.

\begin{proposition}
    The following statements can be generalized for \(n\) variables:
    \begin{enumerate}
        \item Let \(f(x, y)\in C^{1}\), then \(f\) is a homogeneous function of degree \(m\) if and only if
            \[
            x \frac{\partial f}{\partial x} + y \frac{\partial f}{\partial y} = m f(x, y).
            \]
        \item Let \(f(x, y)\in C^{2}\) be a homogeneous function of degree \(m\), then
            \[
            \left( x \frac{\partial}{\partial x} + y \frac{\partial}{\partial y} \right)^2 f(x, y) = m(m-1) f(x, y),
            \]
            where 
            \[
            \left( x \frac{\partial}{\partial x} + y \frac{\partial}{\partial y} \right) = 
            x^2 \frac{\partial^2}{\partial x^2} + 2xy \frac{\partial^2}{\partial x \partial y} + y^2 \frac{\partial^2}{\partial y^2},
            \]
            which is just a formal notation, not an operator multiplication.
        \item Let \(f(x, y)\in C^{2}\) be a homogeneous function of degree \(m\), then \(f_{x}(x, y), f_{y}(x, y)\) 
            are homogeneous functions of degree \(m-1\).
        \item Let \(f(x, y)\in C(\mathbb{R}^{2}\setminus \{(0,0)\})\) be a homogeneous function of degree \(m\), then 
            \[
            \left| f(x,y) \right| \leqslant C \rho^{m}, \quad \rho = \sqrt{x^2 + y^2},
            \]
            where \(C = \max_{\rho=1} |f(x,y)|\).
    \end{enumerate}
\end{proposition}

\begin{example}
    Let \(f(x, y)\) be a differential function on \(\mathbb{R}^2\),
    and satisfy the equation
    \[
    x \frac{\mathrm{d}f}{\mathrm{d}x} + y \frac{\mathrm{d}f}{\mathrm{d}y} = 0,
    \]
    prove that \(f(x, y)\) is always constant.
\end{example}

\section{Mean Value Theorem and Taylor's Formula}
\begin{leftbarTitle}{Mean Value Theorem}\end{leftbarTitle}
\begin{definition}{Convex Region}
    Let \(D \subseteq \mathbb{R}^n\) be a region. 
    If every line segment connecting any two points \(\mathbf{x}_0, \mathbf{x}_1 \in D\) 
    (denoted by \(\overline{\mathbf{x}_0 \mathbf{x}_1}\))
    is entirely contained in \(D\), i.e., for any \(\lambda \in [0, 1]\), we have  
    \[
    \mathbf{x}_0 + \lambda (\mathbf{x}_1 - \mathbf{x}_0) \in D,
    \]
    then \(D\) is called a convex region.
\end{definition}

\begin{theorem}{Lagrange's Mean Value Theorem}\label{thm:Multi_Lagrange}
    Let \(f\) be \underline{differentiable} on \underline{a convex region} \(D \subseteq \mathbb{R}^n\). 
    For any two points \(\mathbf{a}, \mathbf{b} \in D\), 
    there exists a point \(\mathbf{\xi}\in \overline{\mathbf{a} \mathbf{b}}\)
    such that:  
    \[
    f(\mathbf{b}) - f(\mathbf{a}) = Jf(\mathbf{\xi})(\mathbf{b} - \mathbf{a}).
    \]
\end{theorem}

\begin{proof}
    
\end{proof}

For mappings, Lagrange's mean value theorem can not be generalized directly, 
we need introduce inner product:
\begin{theorem}{Lagrange's Mean Value Theorem for Mappings}
    Let \(\mathbf{f}: D \to \mathbb{R}^m\) be \underline{differentiable} on \underline{an open set} \(D \subseteq \mathbb{R}^n\). 
    For any two points \(\mathbf{a}, \mathbf{b} \in D\), 
    there exists a point \(\mathbf{\xi}\in \overline{\mathbf{a} \mathbf{b}}\)
    such that:  
    \[
    \mathbf{a} \cdot [\mathbf{f}(\mathbf{b}) - \mathbf{f}(\mathbf{a})] = 
    \mathbf{a} \cdot [J\mathbf{f}(\mathbf{\xi})(\mathbf{b} - \mathbf{a})],
    \quad \forall \mathbf{a} \in \mathbb{R}^m.
    \]
\end{theorem}
\begin{note}
    If it does not contain the inner product, 
    then it is not necessarily true.
    For example, let
    \[
    \mathbf{f}(t) = (\cos t, \sin t), \quad t \in [0, 2\pi],
    \]
    then 
    \[
    J \mathbf{f}(t) = (-\sin t, \cos t),
    \]
    note that \(\mathbf{f}(2\pi) = \mathbf{f}(0)\),
    then there does not exist \(\theta \in (0, 1)\) such that
    \[
    \mathbf{f}(2\pi) - \mathbf{f}(0) = J\mathbf{f}(\theta \cdot 2\pi)(2\pi - 0).
    \]
    In fact, 
    \[
    J \mathbf{f}(t) \not\equiv 0, \quad \forall t \in [0, 2\pi].
    \]
\end{note}

And we have global estimation for the difference of mappings:
\begin{theorem}{Quasi-Differential Mean Value Theorem for Mappings}
    Let \(\mathbf{f}: D \to \mathbb{R}^m\) be \underline{differentiable} on \underline{a convex region} \(D \subseteq \mathbb{R}^n\). 
    For any two points \(\mathbf{a}, \mathbf{b} \in D\), 
    there exists a point \(\mathbf{\xi}\in \overline{\mathbf{a} \mathbf{b}}\)
    such that:  
    \[
    \|\mathbf{f}(\mathbf{b}) - \mathbf{f}(\mathbf{a})\| 
    \leq \|J\mathbf{f}(\mathbf{\xi})\| \cdot \|\mathbf{b} - \mathbf{a}\|.
    \]
\end{theorem}

\begin{corollary}
    Let \(D\) be a region in \(\mathbb{R}^n\). If for any \(\mathbf{x} \in D\), we have  
    \[
    J\mathbf{f}(\mathbf{x}) = 0,
    \]
    then \(\mathbf{f}\) is constant mapping on \(D\).
\end{corollary}

\begin{proof}
    
\end{proof}

\begin{leftbarTitle}{Taylor's Formula}\end{leftbarTitle}
\begin{theorem}{Taylor's Formula}
    \begin{description}
        \item[Lagrange's Remainder]  Let \(D \subseteq \mathbb{R}^n\) be a convex region, 
            and let \(f: D \to \mathbb{R}\) have \(m+1\) continuous partial derivatives. 
            For \(\mathbf{x}^0 = (x_1^0, x_2^0, \dots, x_n^0) \in D\) and \(\mathbf{x} = (x_1, x_2, \dots, x_n) \in D\), 
            there exists \(\mathbf{\xi} \in \overline{\mathbf{x}^0 \mathbf{x}}\) such that:  
            \[
            f(\mathbf{x}) = f(\mathbf{x}^0) 
            + \sum_{k=1}^m \frac{1}{k!} \left( \sum_{i=1}^n (x_i - x_i^0) \frac{\partial}{\partial x_i} \right)^k f(\mathbf{x}^0) 
            + \frac{1}{(m+1)!} \left( \sum_{i=1}^n (x_i - x_i^0) \frac{\partial}{\partial x_i} \right)^{m+1} f(\mathbf{\xi}).
            \]
        \item[Peano's Remainder] Let \(D \subseteq \mathbb{R}^n\) be a convex region, 
            and let \(f: D \to \mathbb{R}\) have \(m\) continuous partial derivatives. 
            Then:
            \[
            f(\mathbf{x}) = f(\mathbf{x}^0) 
            + \sum_{k=1}^m \frac{1}{k!} \sum_{i_1, i_2, \dots, i_k=1}^n 
            \frac{\partial^k f}{\partial x_{i_1} \partial x_{i_2} \dots \partial x_{i_k}}(\mathbf{x}^0) 
            \prod_{j=1}^{k} (x_{i_j} - x_{i_j}^0) 
            + R_m(\mathbf{x} - \mathbf{x}^0),
            \]
            where \(R_m(\mathbf{x} - \mathbf{x}^0) = O(\|\mathbf{x} - \mathbf{x}^0\|^{m+1})\) or \(o(\|\mathbf{x} - \mathbf{x}^0\|^{m})\), as \(\|\mathbf{x} - \mathbf{x}^0\| \to 0\).

    \end{description}
\end{theorem}

In applications, particularly important is the expression of the first three terms in Taylor's formula, which is given as
(let \(x_1 - x_1^0\) be denoted by \(\Delta x_1\), and similarly for other variables;
\(\Delta \mathbf{x} = (\Delta x_1, \Delta x_2, \dots, \Delta x_n)\)):
\[
    f(\mathbf{x}) = f(\mathbf{x}^0) + Jf(\mathbf{x}^0)(\Delta\mathbf{x})
    + \frac{1}{2!}(\Delta\mathbf{x})Hf(\mathbf{x}^0)(\Delta \mathbf{x})^{\mathrm{T}}+ \cdots,
\]
where the matrix
\[
Hf(\mathbf{x}^0) =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} 
    & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} 
    & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} 
    & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}_{\mathbf{x}^0}
\]
is called the \textbf{Hessian matrix} of the function \(f\).

\section{Implicit Function Theorem}
\begin{leftbarTitle}{Implicit Mapping}\end{leftbarTitle}
\begin{theorem}{Implicit Function Theorem}\label{thm:Implicit Function Theorem}
    Let \(U \subset \mathbb{R}^{n+1}\) be an open set, and \(F: U \to \mathbb{R}\) be an \(n+1\)-variable function. If:  
    \begin{enumerate}
        \item \(F \in C^k(U, \mathbb{R})\), where \(1 \leqslant k \leqslant +\infty\);
        \item \(F(\mathbf{x}^0, y^0) = 0\), 
            where \(\mathbf{x}^0 = (x_1^0, x_2^0, \dots, x_n^0) \in \mathbb{R}^n\), \(y^0 \in \mathbb{R}\), 
            and \((\mathbf{x}^0, y^0) \in U\) 
            (i.e., the equation \(F(\mathbf{x}, y) = 0\) has a solution \((\mathbf{x}^0, y^0)\));
        \item \(F'_y(\mathbf{x}^0, y^0) \neq 0\).
    \end{enumerate}

    Then there exists an open interval \(I \times J\) containing \((\mathbf{x}^0, y^0)\) 
    (\(I\) being an open interval in \(\mathbb{R}^n\) containing \(\mathbf{x}^0\), 
    and \(J\) being an open interval in \(\mathbb{R}\) containing \(y^0\)), 
    as shown in Fig.~\ref{fig:ImplicitFunction}, such that:  
    \begin{enumerate}
        \item \(\forall x \in I\), the equation \(F(\mathbf{x}, y) = 0\) has a unique solution \(y = f(\mathbf{x})\), 
            where \(f: I \to J\) is an \(n\)-variable function 
            (called the \textbf{implicit function} \(f\), hidden within the equation \(F(\mathbf{x}, f(\mathbf{x})) = 0\), 
            though not necessarily explicitly expressed);
        \item \(y^0 = f(\mathbf{x}^0)\);
        \item \(f \in C^k(I, \mathbb{R})\);
        \item When \(x \in I\), 
            \(\frac{\partial f}{\partial x_i} = \frac{\partial y}{\partial x_i} = -\frac{F_x(\mathbf{x}, y)}{F_y(\mathbf{x}, y)}\), 
            \(i = 1, 2, \dots, n\), where \(y = f(x)\).
    \end{enumerate}
\end{theorem}  
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/ImplicitFunction.png}
    \caption{Implicit Function}
    \label{fig:ImplicitFunction}
\end{figure}

\begin{proof}
    Only the single-variable implicit function theorem is proved; 
    the multi-variable case can be derived using mathematical induction.
    
    Without loss of generality, assume \(F_y(x^0, y^0) > 0\).

    First, prove the \underline{existence of the implicit function}.  
    From the continuity of \(F_y(x^{0}, y^{0}) > 0\) and \(F_y(x, y)\), 
    it is known that there exist closed rectangle:
    \[
    D^* = \{(x, y) \mid |x - x_0| \leqslant \alpha, |y - y_0| \leqslant \beta\} \subset U,
    \]
    where the following holds:
    \[
    F_y(x, y) > 0.
    \]
    Thus, for fixed \(x_0\), the function \(F(x^{0}, y)\) is strictly monotonically increasing 
    within \([y^{0} - \beta, y^{0} + \beta]\). Furthermore, since:
    \[
    F(x^{0}, y^{0}) = 0,
    \]
    it follows that:
    \[
    F(x^{0}, y^{0} - \beta) < 0, \quad F(x^{0}, y^{0} + \beta) > 0.
    \]
    Due to the continuity of \(F(x, y)\) within \(D^*\), there exists \(\rho > 0\) such that along the line segment:
    \[
    x = x^{0} + \rho, \, y = y^{0} + \beta,
    \]
    we have \(F(x, y) > 0\), and along the line segment:
    \[
    x = x^{0} + \rho, \, y = y^{0} - \beta,
    \]
    we have \(F(x, y) < 0\).
    Therefore, for any point \(\bar{x} \in (x^{0} - \rho, x^{0} + \rho)\), treat \(F(x, y)\) as a single-variable function of \(y\). 
    Within \([y^{0} - \beta, y^{0} + \beta]\), this function is continuous. From the previous discussion, we know:
    \[
    F(\bar{x}, y^{0} - \beta) < 0, \quad F(\bar{x}, y^{0} + \beta) > 0.
    \]
    According to the zero point existence theorem~\ref{thm:Zero Point Existence Theorem}, 
    there must exist a unique \(\bar{y} \in [y^{0} - \beta, y^{0} + \beta]\) 
    such that \(F(\bar{x}, \bar{y}) = 0\). 
    Furthermore, because \(F_y(x, y) > 0\) within \(D^*\), this \(\bar{y}\) is unique.
    Denote the corresponding relationship as \(\bar{y} = f(\bar{x})\), 
    then the function \(y = f(x)\) is defined within \((x^{0} - \rho, x^{0} + \rho)\), 
    satisfying \(F(x, f(x)) = 0\), and clearly:
    \[
    y^{0} = f(x^{0}).
    \]

    Further proving \underline{the continuity of the implicit function} \(y = f(x)\) on \((x^{0} - \rho, x^{0} + \rho)\):  
    Let \(\bar{x} \in (x^{0} - \rho, x^{0} + \rho)\) be any point. 
    For any given \(\varepsilon > 0\) (\(\varepsilon\) being sufficiently small), 
    since \(F(\bar{x}, \bar{y}) = 0\) (\(\bar{y} = f(\bar{x})\)), from the previous discussion we know:  
    \[
    F(\bar{x}, \bar{y} - \varepsilon) < 0, \quad F(\bar{x}, \bar{y} + \varepsilon) > 0.
    \]
    Furthermore, due to the continuity of \(F(x, y)\) on \(D^*\), there exists \(\delta > 0\) such that:  
    \[
    F(x, \bar{y} - \varepsilon) < 0, \quad F(x, \bar{y} + \varepsilon) > 0, \quad \text{when} \quad x \in O(x^{0}, \delta).
    \]
    By reasoning similar to the previous discussion, 
    it can be obtained that when \(x \in O(x^{0}, \delta)\), 
    the corresponding implicit function value must satisfy \(f(x) \in (\bar{y} - \varepsilon, \bar{y} + \varepsilon)\), i.e.,  
    \[
    \left|f(x) - f(x^{0})\right| < \varepsilon.
    \]
    This implies that \(y = f(x)\) is continuous on \((x^{0} - \rho, x^{0} + \rho)\).  

    Finally, prove the \underline{differentiability} of \(y = f(x)\) on \((x^{0} - \rho, x^{0} + \rho)\):  
    Let \(\bar{x} \in (x^{0} - \rho, x^{0} + \rho)\) be any point. 
    Take \(\Delta x\) sufficiently small such that \(\bar{x} = x + \Delta x \in (x^{0} - \rho, x^{0} + \rho)\). 
    Denote \(\bar{y} = f(\bar{x})\) and \(\bar{y} + \Delta y = f(\bar{x})\). Clearly,  
    \[
    F(\bar{x}, \bar{y}) = 0 \quad \text{and} \quad F(\bar{x}, \bar{y} + \Delta y) = 0.
    \]
    Using the multi-variable function's mean value theorem~\ref{thm:Multi_Lagrange}, we obtain:  
    \begin{align*}
        0   &= F(\bar{x}, \bar{y} + \Delta y) - F(\bar{x}, \bar{y}) \\
            &= F_x(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y) \Delta x + F_y(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y) \Delta y,
    \end{align*}
    where \(0 < \theta < 1\).  
    Note that \(F_y \neq 0\) on \(D^*\), hence:  
    \[
    \frac{\Delta y}{\Delta x} = 
        -\frac{F_x(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y)}{F_y(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y)}.
    \]
    Let \(\Delta x \to 0\). Considering the continuity of \(F_x\) and \(F_y\), we obtain:  
    \[
    \frac{dy}{dx} \Big|_{x = \bar{x}} = -\frac{F_x(\bar{x}, \bar{y})}{F_y(\bar{x}, \bar{y})}.
    \]
    Thus:  
    \[
    f'(\bar{x}) = -\frac{F_x(\bar{x}, \bar{y})}{F_y(\bar{x}, \bar{y})}.
    \]

    The proof is complete.
\end{proof}

\begin{note}
    From the proof process of the implicit function theorem,
    it can be observed that if only require the continuity of the implicit function \(y = f(x)\), 
    then the theorem can be restated as follows:
    \newline If 
    \begin{enumerate}
        \item \(F\in C(U, \mathbb{R})\); 
        \item \(F(\mathbf{x}^0, y^0) = 0\);
        \item For fixed \(\mathbf{x} = \mathbf{x}^0\), \(F(\mathbf{x}^0, y)\) is strictly monotonic with respect to \(y\).
    \end{enumerate}
    Then the function derived from the implicit function \(F(\mathbf{x}, y) = 0\),
    i.e., \(y = f(\mathbf{x})\), is continuous at \(I\).
\end{note}

\begin{theorem}{Implicit Mapping Theorem}\label{thm:Implicit Mapping Theorem}
    Let \( U \subset \mathbb{R}^{n+m} \) be an open set, and \( \mathbf{F}: U \to \mathbb{R}^m \) be a mapping. If:
    \begin{enumerate}
        \item \( \mathbf{F} \in C^k(U, \mathbb{R}^m) \), \( 1 \leqslant k \leqslant \infty \);
        \item \( \mathbf{F}(\mathbf{x}^0, \mathbf{y}^0) = 0 \), 
            where \( \mathbf{x}^0 = (x_1, x_2, \dots, x_n) \), \( \mathbf{y}^0 = (y_1, y_2, \dots, y_m) \), 
            \( (\mathbf{x}^0, \mathbf{y}^0) \in U \) 
            (implying \( \mathbf{F}(\mathbf{x}, \mathbf{y}) = \mathbf{0} \) 
            has a solution at \( (\mathbf{x}^0, \mathbf{y}^0) \));
        \item The determinant
        \[
        \det
        \begin{pmatrix}
            \frac{\partial F_1}{\partial y_1} & \cdots & \frac{\partial F_1}{\partial y_m} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial F_m}{\partial y_1} & \cdots & \frac{\partial F_m}{\partial y_m}
        \end{pmatrix}_{(\mathbf{x}^0, \mathbf{y}^0)}
        = \det J_{\mathbf{y}} \mathbf{F}(\mathbf{x}^0, \mathbf{y}^0) \neq 0,
        \]
    \end{enumerate}
    then there exists an open neighborhood \( I \times J \subset U \subset \mathbb{R}^{n+m} \) 
    containing \( (\mathbf{x}^0, \mathbf{y}^0) \), such that:
    \begin{enumerate}
        \item For all \( \mathbf{x} \in I \), the system \( \mathbf{F}(\mathbf{x}, \mathbf{y}) = \mathbf{0} \) 
            has a unique solution \( \mathbf{y} = \mathbf{f}(\mathbf{x}) \), 
            where \( \mathbf{f}: I \to J \) is a mapping 
            (called \( \mathbf{f} \) the implicit function hidden in 
            \( \mathbf{F}(\mathbf{x}, \mathbf{f}(\mathbf{x})) = \mathbf{0} \));
        \item \( \mathbf{y}^0 = \mathbf{f}(\mathbf{x}^0) \);
        \item \( \mathbf{f} \in C^k(I, \mathbb{R}^m) \);
        \item For \( x \in I \),
        \[
        J\mathbf{f} = 
            - (J_{\mathbf{y}} \mathbf{F})^{-1} J_{\mathbf{x}} \mathbf{F}
        = -
        \begin{pmatrix}
            \frac{\partial F_1}{\partial y_1} & \cdots & \frac{\partial F_1}{\partial y_m} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial F_m}{\partial y_1} & \cdots & \frac{\partial F_m}{\partial y_m}
        \end{pmatrix}^{-1}
        \begin{pmatrix}
            \frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_n} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial F_m}{\partial x_1} & \cdots & \frac{\partial F_m}{\partial x_n}
        \end{pmatrix},
        \]
    \end{enumerate}
    where \( \mathbf{y} = \mathbf{f}(\mathbf{x}) \).
\end{theorem}

\begin{example}
    \[
    \begin{cases}
        x = x(z), \\
        y = y(z),
    \end{cases}
    \]
    is an mapping solved from the implicit function defined by the equations:
    \[
    \begin{cases}
        F(y-z, x+z) = 0, &  \\
        G(\frac{y}{z}, xz) = 0, &  \\
    \end{cases}
    \]
    where \(F, G \in C^1\).
    Find \(\frac{\mathrm{d}x}{\mathrm{d}z}\) and \(\frac{\mathrm{d}y}{\mathrm{d}z}\).
\end{example}
\begin{remark}
    % 用 F_{1} 表示 F 对第一个位置的变量求偏导, 它等价于F(u, v) 中的 F_{u}, 其他类似
    Here, we use \(F_{1}\) to represent the partial derivative of \(F\) with respect to its first variable, 
    which is equivalent to \(F_{u}\) in \(F(u, v)\).
    Other notations follow similarly.
\end{remark}

\begin{solution}

\noindent{\color{violet!80}\textbf{Method 1: Direct Derivative}}
Derivative both sides of the equations with respect to \(z\):
\begin{gather*}
    F_{1}(y' - 1) + F_{2}(x' + 1) = 0, \\
    G_{1}(\frac{y'z - y}{z^2}) + G_{2}(x'z + x) = 0.
\end{gather*}
Solve the above equations to get:
\begin{gather*}
    \frac{\mathrm{d}x}{\mathrm{d}z} = 
    \frac{zG_{1}(F_{1}-F_{2})-F_{1}(yG_{1}-xz^{2}G_{2})}{z(F_{2}G_{1}F_{1}G_{2}z^{2})}, \\
    \frac{\mathrm{d}y}{\mathrm{d}z} =  
    \frac{F_{2}(yG_{1}-xz^{2}G_{2})-G_{2}z^{3}(F_{1}-F_{2})}{z(F_{2}G_{1}-F_{1}G_{2}z^{2})}.
\end{gather*}

\noindent{\color{violet!80}\textbf{Method 2: Implicit Function Theorem}}
By the implicit function theorem, we have:
\begin{align*}
    \begin{pmatrix}
    \frac{\mathrm{d}x}{\mathrm{d}z} \\
    \frac{\mathrm{d}y}{\mathrm{d}z}
\end{pmatrix}
&= - 
\begin{pmatrix}
    \frac{\partial F}{\partial x} & \frac{\partial F}{\partial y} \\
    \frac{\partial G}{\partial x} & \frac{\partial G}{\partial y}
\end{pmatrix}^{-1}
\begin{pmatrix}
    \frac{\partial F}{\partial z} \\
    \frac{\partial G}{\partial z}
\end{pmatrix} \\
&= 
\begin{pmatrix} 
    \frac{zG_{1}(F_{1}-F_{2})-F_{1}(yG_{1}-xz^{2}G_{2})}{z(F_{2}G_{1}-F_{1}G_{2}z^{2})} \\
    \frac{F_{2}(yG_{1}-xz^{2}G_{2})-G_{2}z^{3}(F_{1}-F_{2})}{z(F_{2}G_{1}-F_{1}G_{2}z^{2})}
\end{pmatrix}.
\end{align*}
\end{solution}

\begin{example}
    Let \(u(x, y)\) is a function solved from the implicit function defined by the equation:
    \[
    \begin{cases} u=f(x, y, z, t), \\ g(y, z, t)=0,\\ h(z, t)=0 ,\end{cases}
    \]
    where \(f, g, h \in C^1\) and \(\frac{\partial(g, h)}{\partial(z, t)} \neq 0\).
    Find \(\frac{\partial u}{\partial y}\).
\end{example}
\begin{solution}
    
\noindent{\color{violet!80}\textbf{Method 1.}}{\color{violet!80}}
    Since \(\frac{\partial(g, h)}{\partial(z, t)} \neq 0\),
    and \(g, h \in C^1\), \(g(y, z, t)=0, h(z, t)=0\),
    by the implicit mapping theorem~\ref{thm:Implicit Mapping Theorem},
    we can express \(z\) and \(t\) as functions of \(y\):
    \[
    \begin{cases}
        z = z(y), \\
        t = t(y).
    \end{cases}
    \]
    Derivative both sides with respect to \(y\):
    \begin{gather*}
        g_{y} + g_{z} \frac{\mathrm{d}z}{\mathrm{d}y} + g_{t} \frac{\mathrm{d}t}{\mathrm{d}y} = 0, \\
        h_{z} \frac{\mathrm{d}z}{\mathrm{d}y} + h_{t} \frac{\mathrm{d}t}{\mathrm{d}y} = 0.
    \end{gather*}
    And \(u\) is a function of \(x\) and \(y\): \(u = u(x, y) = f(x, y, z(y), t(y))\).
    Thus:
    \[
    \frac{\partial u}{\partial y} = f_{2} + f_{3} \frac{\mathrm{d}z}{\mathrm{d}y} + f_{4} \frac{\mathrm{d}t}{\mathrm{d}y}.
    \]
    Solve the above equations to get:
    \[
    \frac{\partial u}{\partial y} = 
    f_{y} - g_{y}(f_{z} h_{t} - f_{t} h_{z}) \left( \frac{\partial(g, h)}{\partial(z, t)} \right)^{-1}.
    \]
    
\noindent{\color{violet!80}\textbf{Method 2.}}{\color{violet!80}}
Considering
\[
\begin{cases} F(x, y, z, t, u)=u-f(x,y,z,t)=0, \\ g(y,z,t)=0, \\h(z,t)=0.  \end{cases}
\]
Since \(\frac{\partial(F,g,h)}{\partial(u,z,t)}=\frac{\partial(g,h)}{\partial(z,t)}\neq0\),
by the implicit mapping theorem~\ref{thm:Implicit Mapping Theorem},
we have
\[
\begin{cases} u=u(x,y),  \\ z=z(x,y), \\ t=t(x,y).  \end{cases}
\]
Derivative both sides with respect to \(y\):
\begin{gather*}
    u_{y} -f_{y} - f_{z} z_{y} - f_{t} t_{y} = 0, \\
    g_{y} + g_{z} z_{y} + g_{t} t_{y} = 0, \\
    h_{z} z_{y} + h_{t} t_{y} = 0.
\end{gather*}
Solve the above equations to get the same result.
\end{solution}


\begin{leftbarTitle}{Inverse Mapping}\end{leftbarTitle} % 逆映射定理
\begin{theorem}{Local Inverse Mapping Theorem}\label{thm:Local Inverse Mapping Theorem} % 局部逆映射定理
    Let \( U \subset \mathbb{R}^n \) be an open set, and \( \mathbf{f}: U \to \mathbb{R}^n \) be a mapping. If:
    \begin{enumerate}
        \item \( \mathbf{f} \in C^k(U, \mathbb{R}^n) \), \( 1 \leqslant k \leqslant +\infty \);
        \item At point \( \mathbf{x}^0 \in U \), the Jacobian determinant
        \[
        \det J\mathbf{f}(\mathbf{x}^0) \neq 0.
        \]
    \end{enumerate}
    Then there exist open neighborhoods \( V \subset U \) of \( \mathbf{x}^0 \) 
    and \( W \subset \mathbb{R}^n \) of \( \mathbf{f}(\mathbf{x}^0 = \mathbf{y}^{0}) \), such that:
    \begin{enumerate}
        \item The restriction of \( \mathbf{f} \) to \( V \), denoted as \( \mathbf{f}|_V: V \to W \), is a bijection;
        \item The inverse mapping \( \mathbf{f}^{-1}: W \to V \) exists and belongs to \( C^k(W, \mathbb{R}^n) \);
        \item For any \( \mathbf{y} = \mathbf{f}(\mathbf{x}) \in W \),
        \[
        J\mathbf{f}^{-1}(\mathbf{y}) = [J\mathbf{f}(\mathbf{x})]^{-1},
        \]
        where \( \mathbf{x} = \mathbf{f}^{-1}(\mathbf{y}) \).
    \end{enumerate}
    % 此时, f 为 C^k 微分同胚 
    At this time, \( \mathbf{f} \) is called a \( C^k \) diffeomorphism.
\end{theorem}
% 如果加强条件, 则有全局逆映射定理
If the conditions are strengthened, 
then a global inverse mapping theorem can be established.
\begin{theorem}{Inverse Mapping Theorem}\label{thm:Inverse Mapping Theorem}
    Let \( U \subset \mathbb{R}^n \) be a convex region, and \( \mathbf{f}: U \to \mathbb{R}^n \) be a mapping. If:
    \begin{enumerate}
        \item \( \mathbf{f} \in C^k(U, \mathbb{R}^n) \), \( 1 \leqslant k \leqslant +\infty \);
        \item For any \( \mathbf{x} \in U \), the Jacobian determinant
        \[
        \det J\mathbf{f}(\mathbf{x}) \neq 0.
        \]
    \end{enumerate}
    Then \( \mathbf{f}: U \to \mathbf{f}(U) \) is a bijection, 
    and the inverse mapping \( \mathbf{f}^{-1}: \mathbf{f}(U) \to U \) exists and 
    belongs to \( C^k(\mathbf{f}(U), \mathbb{R}^n) \).
\end{theorem}

\begin{example}
    Here are substitutions:
    \[
    x=t, y=\frac{t}{1+tu}, z=\frac{t}{1+tv}.
    \]
    Transform the following equation to the form of dependent variables \(v\) and independent variables \(t, u\):
    \[
    x^{2} \frac{\partial z}{\partial x} + y^{2} \frac{\partial z}{\partial y} = z^{2}.
    \]
\end{example}


\section{Extremum of Multi-variable Functions}

\begin{leftbarTitle}{Unconditional Extremum}\end{leftbarTitle}



\begin{proposition}{Fermat's Three Villiges Problem} % 费马三村问题
    There are three villages located at points \(A\), \(B\), and \(C\) on a flat plane.
    A supply station needs to be established at point \(P\) on the plane,
    such that the total distance from \(P\) to the three villages \(A\), \(B\), and \(C\) is minimized.
    Such a point \(P\) is called the Fermat point of triangle \(ABC\),
    which can be determined as follows:
    \begin{enumerate}
        \item If any angle of triangle \(ABC\) is greater than or equal to \(120^{\circ}\),
            then the Fermat point is the vertex of that angle.
        \item If all angles of triangle \(ABC\) are less than \(120^{\circ}\),
            then the Fermat point \(P\) is located inside triangle \(ABC\),
            and the angles between the segments \(PA\), \(PB\), and \(PC\) are all equal to \(120^{\circ}\).
    \end{enumerate}
\end{proposition}

\begin{leftbarTitle}{Conditional Extremum}\end{leftbarTitle}
\begin{definition}{Conditional Extremum}
    Let \( f: D \to \mathbb{R} \) be a function with \(n+m\) variables defined on a open set \( D \subseteq \mathbb{R}^{n+m} \), 
    and let \(\mathbf{\Phi}: D \to \mathbb{R}^m\) be a mapping, \( M=\{ \mathbf{x} \in D \mid \mathbf{\Phi}(\mathbf{x}) = 0 \} \).
    If there exists \( \mathbf{x}^0 \in M \) satisfying the constraints such that:  
    \[
    f(\mathbf{x}^0) \leq f(\mathbf{x}) \quad (\text{or } f(\mathbf{x}^0) \geq f(\mathbf{x})),
    \]
    for all \( \mathbf{x} \in M \) that also satisfy the constraints, 
    then \( f \) is said to have a conditional minimum (or maximum) at point \( \mathbf{x}^0 \) under the given constraints.
\end{definition}

\begin{theorem}{Lagrange Multiplier Method}
    Let \( f: D \to \mathbb{R} \) be a function with \( n+m \) variables defined on an open set \( D \subseteq \mathbb{R}^{n+m} \), 
    and let \( \mathbf{\Phi}: D \to \mathbb{R}^m \) be a mapping, 
    \( M = \{ \mathbf{x} \in D \mid \mathbf{\Phi}(\mathbf{x}) = 0 \} \). 
    If:
    \begin{enumerate}
        \item \(f \in C^1(D,\mathbb{R}), \mathbf{\Phi} \in C^1(D,\mathbb{R}^m)\);
        \item \(\operatorname{rank}(J\mathbf{\Phi}(\mathbf{x}^0)) = m\);
        \item \(\mathbf{x}^{0}\) is a conditional extremum point of \(f\) on \(M\);
    \end{enumerate}
    then there exist \(\lambda_1, \lambda_2, \dots, \lambda_m \in \mathbb{R}\), such that:
    \[
    \nabla f(\mathbf{x}^0) + \sum_{i=1}^m \lambda_i \nabla \Phi_i(\mathbf{x}^0) = 0.
    \]
\end{theorem}
