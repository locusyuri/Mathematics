\documentclass[11pt]{../../TexTemplate/elegantbook}

\title{Analyse Mathématique} % 这里放置书名
% \subtitle{Subtitle} % 这里放置副标题

\author{CatMono} % 这里放置作者名
\date{July, 2025} % 这里放置日期
\version{0.1} % 这里放置版本号
% \institute{Elegant\LaTeX{} Program} % 这里放置机构名
% \bioinfo{Custom Key}{Custom Value} % 这里放置自定义信息

% \extrainfo{extra information} % 这里放置额外信息，将显示在最下方中央

\setcounter{tocdepth}{2} % 设置目录深度
\setcounter{secnumdepth}{2} % 设置章节编号深度


% \logo{logo-blue.png} % 这里放置封面logo，默认从figure目录下寻找
% \cover{LogiqueMathematique.png} % 这里放置封面图片，默认从figure目录下寻找

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170} % 自定义颜色
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect} % 保护命令参数不被 LaTeX 解析器过早处理，允许在某些特殊环境中使用脆弱命令（fragile commands）。
\usepackage{xeCJK} % 使用 xeCJK 包支持中文


% ===== 开始文档 =====
\begin{document}

\maketitle %生成文档的标题页，根据之前定义的标题信息（如标题、作者、日期等）自动创建一个格式化的标题页

% === 前言部分 ===
\frontmatter        % 开始前言，页码为 i, ii, iii...
\tableofcontents    % 目录 (页码: i, ii)
% \listoffigures      % 图表目录 (页码: iii)
% \listoftables       % 表格目录 (页码: iv)

\chapter{Preface}   % 前言章节（无编号，页码: v, vi...）
This is the preface of the book...

% \chapter{Acknowledgments}  % 致谢（无编号）
% I would like to thank...
% === 正文部分 ===
\mainmatter         % 开始正文，页码从 1 重新开始

\chapter{Preliminaries} % 这里放置章节标题
\section{Trigonometric Formulas} % 这里放置小节标题
% Trigonometric identities and formulas (Academic English LaTeX)

% Product-to-Sum (Product to Sum and Difference)
\textbf{Product-to-Sum Formulas:}
\begin{align*}
\sin\alpha \cos\beta &= \frac{1}{2} \left[ \sin(\alpha + \beta) + \sin(\alpha - \beta) \right] \\
\cos\alpha \sin\beta &= \frac{1}{2} \left[ \sin(\alpha + \beta) - \sin(\alpha - \beta) \right] \\
\cos\alpha \cos\beta &= \frac{1}{2} \left[ \cos(\alpha + \beta) + \cos(\alpha - \beta) \right] \\
\sin\alpha \sin\beta &= -\frac{1}{2} \left[ \cos(\alpha + \beta) - \cos(\alpha - \beta) \right]
\end{align*}

% Sum and Difference Formulas
\textbf{Sum and Difference Formulas:}
\begin{align*}
\sin(\alpha + \beta) &= \sin\alpha \cos\beta + \cos\alpha \sin\beta \\
\sin(\alpha - \beta) &= \sin\alpha \cos\beta - \cos\alpha \sin\beta \\
\cos(\alpha + \beta) &= \cos\alpha \cos\beta - \sin\alpha \sin\beta \\
\cos(\alpha - \beta) &= \cos\alpha \cos\beta + \sin\alpha \sin\beta
\end{align*}

% Sum-to-Product (Sum and Difference to Product)
\textbf{Sum-to-Product Formulas:}
\begin{align*}
\sin\alpha + \sin\beta &= 2 \sin\left( \frac{\alpha + \beta}{2} \right) \cos\left( \frac{\alpha - \beta}{2} \right) \\
\sin\alpha - \sin\beta &= 2 \sin\left( \frac{\alpha - \beta}{2} \right) \cos\left( \frac{\alpha + \beta}{2} \right) \\
\cos\alpha + \cos\beta &= 2 \cos\left( \frac{\alpha + \beta}{2} \right) \cos\left( \frac{\alpha - \beta}{2} \right) \\
\cos\alpha - \cos\beta &= -2 \sin\left( \frac{\alpha + \beta}{2} \right) \sin\left( \frac{\alpha - \beta}{2} \right)
\end{align*}

% Double Angle Formulas
\textbf{Double Angle Formulas:}
\begin{align*}
\sin 2\alpha &= 2\sin\alpha \cos\alpha \\
\cos 2\alpha &= \cos^2\alpha - \sin^2\alpha = 2\cos^2\alpha - 1 = 1 - 2\sin^2\alpha \\
\tan 2\alpha &= \frac{2\tan\alpha}{1 - \tan^2\alpha}
\end{align*}

% Half Angle Formulas
\textbf{Half Angle Formulas:}
\begin{align*}
\sin \frac{\alpha}{2} &= \pm \sqrt{ \frac{1 - \cos\alpha}{2} } \\
\cos \frac{\alpha}{2} &= \pm \sqrt{ \frac{1 + \cos\alpha}{2} } \\
\tan \frac{\alpha}{2} &= \frac{1 - \cos\alpha}{\sin\alpha} = \frac{\sin\alpha}{1 + \cos\alpha}
\end{align*}

% Power-Reducing Formulas
\textbf{Power-Reducing Formulas:}
\begin{align*}
\sin^2\alpha &= \frac{1 - \cos 2\alpha}{2} \\
\cos^2\alpha &= \frac{1 + \cos 2\alpha}{2}
\end{align*}

% Angle Decomposition Formulas
\textbf{Angle Decomposition Formulas:}
\begin{align*}
\sin^2\alpha - \sin^2\beta &= \sin(\alpha + \beta) \sin(\alpha - \beta) \\
\cos^2\alpha - \sin^2\beta &= \cos(\alpha + \beta) \cos(\alpha - \beta)
\end{align*}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{img/triangle.png}
\end{figure}

% Geometric remarks
\begin{remark}
    \begin{itemize}
        \item On the gray triangle, the sum of the squares of the two numbers above is equal to the square of the number below,
            for instance, \(\tan^{2} x + 1 = \sec^{2}x\)
        \item The three trigonometric functions in the clockwise direction have the following properties: 
            $\tan x = \frac{\sin x}{\cos x}$, etc.
    \end{itemize}
\end{remark}



\chapter{Limits of Sequences and Continuity of Real Number System} 
\section{Convergent Sequences}
\begin{leftbarTitle}{Convergent Sequences}\end{leftbarTitle}

\begin{leftbarTitle}{Properties of Convergent Sequences}\end{leftbarTitle}



\begin{leftbarTitle}{Cauchy Proposition and Fitting Method}\end{leftbarTitle}
\begin{proposition}{Cauchy Proposition}\label{prop:Cauchy Proposition}
    Let \(\lim_{n \to \infty} x_n = l\), then:
    \[
        \lim_{n \to \infty} \frac{x_{1}+x_{2}+ \cdots +x_{n}}{n} = l.
    \]
\end{proposition}

\begin{note}
    \begin{enumerate}
        \item In the proposition, \(l\) can be \(+\infty\) or \(-\infty\).
        \item Let \(\lim_{n \to \infty} x_n = l\), then:
            \[
                \lim_{n \to \infty}\frac{x_{1}+x_{2}+ \cdots +x_{n}}{n}
                =\lim_{n \to \infty} \sqrt[n]{x_{1} x_{2} \cdots x_{n}} 
                =\lim_{n \to \infty} \frac{n}{\frac{1}{x_{1}} + \frac{1}{x_{2}} + \cdots + \frac{1}{x_{n}}}
                = l.
            \]
    \end{enumerate}
\end{note}

It can be proved directly by Stolz theorem~\ref{thm:Stolz Theorem}.
On top of that, it can also be proved by the \textbf{fitting method}.

\begin{proof}
    
\end{proof}

\begin{remark}
    To prove \(\lim_{n \to \infty} x_n = A\), 
    the key is to show that \(|x_n - A|\) can be arbitrarily small. 
    For this purpose, it is generally recommended to simplify the expression of \(x_n\) as much as possible. 
    However, in some cases, \(A\) can also be transformed into a form similar to \(x_n\). 
    This method is called the fitting method. 
    The core idea behind the method of fitting is to appropriately divide into units of \(1\) for analysis.
\end{remark}



\section{Indeterminate Form}
\begin{leftbarTitle}{Infinitely Large Quantities and Infinitesimal Quantities}\end{leftbarTitle}

\begin{leftbarTitle}{Indeterminate Forms}\end{leftbarTitle}

\begin{theorem}{Stolz-Cesàro theorem}\label{thm:Stolz Theorem}
    \begin{description}
        \item[Type \(\frac{0}{0}\)] Let \(\{a_n\}, \{b_n\}\) be two infinitesimal sequences, 
            where \(\{a_n\}\) is also a strictly monotonic decreasing sequence. If  
            \[
            \lim_{n \to \infty} \frac{b_{n+1} - b_n}{a_{n+1} - a_n} = l \, (\text{finite or } \pm\infty),
            \]  
            then  
            \[
            \lim_{n \to \infty} \frac{a_n}{b_n} = l.
            \] 
        \item[Type \(\frac{\text{*}}{\infty}\)] Let \(\{a_n\}\) be a strictly monotonic increasing sequence 
            of divergent large quantities. If  
            \[
            \lim_{n \to \infty} \frac{b_{n+1} - b_n}{a_{n+1} - a_n} = l \, (\text{finite or } \pm\infty),
            \]  
            then  
            \[
            \lim_{n \to \infty} \frac{a_n}{b_n} = l.
            \]
    \end{description}
\end{theorem}
\begin{note}
    \begin{enumerate}
        \item The inverse proposition of Stolz's Theorem does not hold.
        \item If \(a_1\) is an undefined infinite quantity \(\infty\), Stolz Theorem does not hold.
    \end{enumerate}
\end{note}

\begin{theorem}{Silverman-Toeplitz Theorem}\label{thm:Toeplitz Theorem}
    Let
    \[
        \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_n \\ \vdots
        \end{bmatrix}
        =
        \begin{bmatrix}
        a_{11} & 0 & \cdots & 0 \\
        a_{21} & a_{22} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn} \\
        \vdots & \vdots &        & \vdots \\
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\  x_2 \\  \vdots \\   x_n \\ \vdots
        \end{bmatrix},
    \]
    where the infinite triangular matrix satisfies:
    \begin{enumerate}
        \item \(\forall j, \lim_{n \to \infty} a_{nj} = 0.\)(Every column sequence converges to \(0\).)
        \item \(\sup_{i\in \mathbb{N}} \sum_{j=1}^{i} \left| a_{ij} \right| < \infty.\)(The absolute row sums are bounded.)
    \end{enumerate}
    And \(\lim_{n \to \infty} x_n = l\).
    We denote \(y_{n}\) as the weighted sum sequence: \(y_{n} = \sum_{j=1}^n a_{nj} x_j\).
    Then the following results hold:
    \begin{enumerate}
        \item If \(l=0\), then  \(\lim_{n \to \infty} y_n = 0\).
        \item If \(l \neq 0\) and \(\lim_{n \to \infty}\sum_{j=1}^{n} a_{ij}=1 \), then \(\lim_{n \to \infty} y_n = l\).
    \end{enumerate}
\end{theorem}




\section{Subsequences}
\begin{leftbarTitle}{Subsequences}\end{leftbarTitle}

\begin{leftbarTitle}{Upper Limits and Lower Limits}\end{leftbarTitle}


\section{Completeness of The Real Numbers}
\begin{leftbarTitle}{Dedkind Completeness}\end{leftbarTitle}

\begin{leftbarTitle}{Least Upper Bound Property}\end{leftbarTitle}

\begin{leftbarTitle}{Monotone Convergence Theorem}\end{leftbarTitle}

\begin{leftbarTitle}{Bolzano-Weierstrass Theorem}\end{leftbarTitle}

\begin{leftbarTitle}{Nested Interval Theorem}\end{leftbarTitle}

\begin{leftbarTitle}{Cauchy Completeness}\end{leftbarTitle}
\begin{definition}{Cauchy Sequence}
    A sequence \(\{x_n\}\) is called a \textbf{Cauchy sequence} if for any \(\varepsilon > 0\), 
    there exists a positive integer \(N\) such that when \(m,n > N\), 
    \[
        \left|x_n - x_m\right| < \varepsilon.
    \]
\end{definition}

\begin{theorem}{Cauchy Convergence Criterion for Sequences}\label{thm:Cauchy Convergence Criterion for Sequences}
    A sequence \(\{x_n\}\) converges if and only if it is a Cauchy sequence.
\end{theorem}


\begin{leftbarTitle}{Heine-Borel Theorem}\end{leftbarTitle}

\section{Iterative Sequences}
Formally, \(x_{0}\) is a \textbf{fixed point} of the function \(f\) if \(f(x_{0}) = x_{0}\).

\begin{theorem}{Banach Fixed-Point Theorem (Contraction Mapping Theorem)}\label{thm:Banach Fixed-Point Theorem}
    There exists a contraction mapping (in~\ref{def:Lipschitz Continuity}) \(f\) on an interval \(I\),
    which admits a unique fixed point \(x^{*}\in I\).
    Furthermore, \(x^{*}\) can be found as follows:
    start with an arbitrary point \(x_{0}\in I\) and define the iterative sequence
    \(x_{n+1}=f(x_n)\) for \(n=0,1,2,\cdots\).
    Then \(\lim_{n \to \infty} x_n = x^{*}\).
\end{theorem}

\begin{remark}
    The following inequalities are equivalent and describe the speed of convergence:  
    \begin{gather*}
        \left| x_{n} - x^{*} \right|  \leqslant \frac{L^{n}}{1-L} \left| x_{1} - x_{0} \right|, \\
        \left| x_{n+1} - x^{*} \right| \leqslant \frac{L}{1-L} \left| x_{n+1} - x_{n} \right|, \\
        \left| x_{n+1} - x^{*} \right| \leqslant L \left| x_{n} - x^{*} \right|.
    \end{gather*}
    Any such value of \(L<1\) is the Lipschitz constant for \(f\), 
    and the smallest one is sometimes called \textbf{the best Lipschitz constant} of \(L\).
\end{remark}



\chapter{Limits and Continuity of Functions}
\section{Limits of Functions}
\begin{leftbarTitle}{Definition of Limit}\end{leftbarTitle}

\begin{leftbarTitle}{Limits of Functions and Sequences}\end{leftbarTitle}
\begin{theorem}{Heine Theorem}\label{thm:Heine Theorem}
    Let \(f\) be a function defined on a deleted neighborhood \(\mathring{U}(x_{0})\) of \(x_{0}\).
    The following two statements are equivalent:
    \begin{enumerate}
        \item \(\lim_{x \to x_{0}} f(x) = A\).
        \item For any sequence \(\{x_n\}\subset \mathring{U}(x_{0})\) with \(\lim_{n \to \infty} x_n = x_0\),
            we have \(\lim_{n \to \infty} f(x_n) = A\) for the sequence \(\{f(x_n)\}\).
    \end{enumerate}
\end{theorem}

\section{Continuous Functions}

\section{Infinitesimal and Infinite Quantities}

\section{Continuous Functions on Closed Intervals}
\begin{leftbarTitle}{Concerning Theorems}\end{leftbarTitle}


\begin{theorem}{The Bolzano-Cauchy Intermediate-Value Theorem}\label{thm:Indeterminate Value Theorem}

\end{theorem}

\begin{theorem}{Zero Point Existence Theorem}\label{thm:Zero Point Existence Theorem}

\end{theorem}

\begin{leftbarTitle}{Uniform Continuity and Lipschitz Continuity}\end{leftbarTitle}

\begin{definition}{Uniform Continuity}
    
\end{definition}

\begin{theorem}{Uniform Continuity Theorem}
    
\end{theorem}


\begin{theorem}{Cantor's Theorem}
    
\end{theorem}


\begin{definition}{Lipschitz Continuity}\label{def:Lipschitz Continuity}
    If there exists a constant \(L > 0\) such that for any \(x_1, x_2 \in I\), 
    \[
        \left| f(x_{1}) - f(x_{2}) \right| \leq L \left| x_{1} - x_{2} \right|,
    \]
    then \(f\) is called \textbf{Lipschitz continuous} on \(I\).

    Specially, if \(L < 1\), then \(f\) is called a \textbf{contraction mapping} on \(I\).
\end{definition}

\begin{remark}
    \begin{itemize}
        \item If \(f\) is Lipschitz continuous on \(I\), then \(f\) is uniformly continuous on \(I\).
            (\(\forall \varepsilon > 0\), just let \(\delta = \frac{\varepsilon}{L}\))
        \item If \(f\) is uniformly continuous on \(I\), then \(f\) is continuous on \(I\).
        \item The converse of the above two statements does not hold.
    \end{itemize}
\end{remark}




\section{Period Three Implies Chaos}



\section{Functional Equations}

\chapter{Differential}
\section{Differential and Derivative}

\section{Higher-Order Derivatives}

\section{Differential Mean Value Theorems}
\begin{definition}{Extremum}
    Let \(f(x)\) is defined on \((a,b)\), \(x_{0}\in (a,b)\).
    If there exists \(U(x_{0}, \delta)\subset (a,b)\) such that \(f(x)\leqslant f(x_{0})\) on it,
    then \(x_{0}\) is called a local maximum point of \(f\),
    and \(f(x_{0})\) is referred to as the corresponding local maximum value.

    The definition of the minimum value is analogous.
\end{definition}

\begin{lemma}{Fermat's Lemma}
    If \(f\) is differentiable at \(x_{0}\) which is a local extremum, then \(f'(x_{0}) = 0\).
\end{lemma}

\begin{theorem}{Rolle's Theorem}
    If \(f\in C[a,b], f\in D(a,b)\) and \(f(a) = f(b)\), then there exists \(\xi\in (a,b)\) such that \(f'(\xi) = 0\).

    \underline{\textbf{Enhanced Version:}}If \(f\in D(a,b)\)(finite or infinite interval), 
    and \(\lim_{x \to a^{+}} f(x) = \lim_{x \to b^{-}} f(x) \) , 
    then there exists \(\xi\in (a,b)\) such that \(f'(\xi) = 0\).
\end{theorem}

\begin{theorem}{Lagrange's Mean Value Theorem}
    If \(f\in C[a,b], f\in D(a,b)\), then there exists \(\xi\in (a,b)\) such that
    \[
        f'(\xi) = \frac{f(b) - f(a)}{b - a}.
    \]
\end{theorem}

\begin{theorem}{Cauchy's Mean Value Theorem}
    If \(f,g\in C[a,b], f,g\in D(a,b)\) and \(g'(x) \neq 0\) for all \(x\in (a,b)\), 
    then there exists \(\xi\in (a,b)\) such that
    \[
        \frac{f'(\xi)}{g'(\xi)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
    \]
\end{theorem}



\section{Theorems and Applications concerning Derivatives}

\begin{theorem}{Darboux's Intermediate Value Theorem for Derivatives}
    If \(f(x)\in D[a,b]\), and \(f'_{+}(a)\cdot f'_{-}(b)<0\),
    then there at least exists \(\xi\in (a,b)\) such that \(f'(\xi) = 0\).
\end{theorem}

\begin{theorem}{Theorem on the Limit of Derivatives}
    If \(f(x)\in C(U(x_{0})),\mathring{D}(U(x_{0}))\), and \(\lim_{x \to x_{0}} f'(x) = A\),
    then \(f\) is differentiable at \(x_{0}\) and \(f'(x_{0}) = A\).
\end{theorem}
\begin{remark}
    In fact, \(\lim_{x \to x_{0}} f'(x) = A\) has already been shown to imply that \(f\in \mathring{D}(U(x_{0}))\).
\end{remark}

\section{Taylor Theorem}

\section{Applications of Taylor Theorem}

\chapter{Integral}

\chapter{Numerical Series}
\section{Convergence of Numerical Series}

\section{Positive Term Series}

\section{General Term Series}
\begin{leftbarTitle}{Cauchy Convergence Criterion for Series}\end{leftbarTitle}

\begin{leftbarTitle}{Alternative Series}\end{leftbarTitle}

\begin{leftbarTitle}{Abel-Dirichlet Test}\end{leftbarTitle}

\begin{theorem}{Abel Transform (Discrete Integration by Parts/Summation by Parts)}\label{thm:Abel Transform}
    Let \(\{a_n\}, \{b_n\}\) be two sequences, then for any \(n\in \mathbb{N}^{+}\),
    \[
        \sum_{k=1}^{n} a_k b_k = a_n B_n + \sum_{k=1}^{n-1} (a_{k+1} - a_{k})B_k,
    \]
    where \(B_n = \sum_{k=1}^{n} b_k\).
\end{theorem}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, angle=180]{img/AbelTransform.jpg}
\end{figure}


\chapter{Series of Functions}

\chapter{Power Series}

\chapter{Limits and Continuity in Euclidean Spaces}

\section{Continuous Mappings}
\begin{leftbarTitle}{Continuous Mappings on Compact Sets}\end{leftbarTitle}

\begin{leftbarTitle}{Continuous Mappings on Connected Sets}\end{leftbarTitle}
\begin{definition}{Connected Set}
    Let \(S\) be a set of points in \(\mathbb{R}^n\). 
    If a continuous mapping 
    \[
        \gamma: [0, 1] \to \mathbb{R}^n
    \]
    satisfies that the range of \(\gamma([0, 1])\) lies entirely within \(S\), 
    we call \(\gamma\) a \textbf{path} in \(S\), 
    where \(\gamma(0)\) and \(\gamma(1)\) are referred to as the starting point and ending point of the path, respectively.  
    
    If for any two points \(\mathbf{x}, \mathbf{y} \in S\), 
    there exists a path in \(S\) with \(\mathbf{x}\) as the starting point and \(\mathbf{y}\) as the ending point, 
    \(S\) is called path-connected, or equivalently, \(S\) is called a \textbf{connected set}.  
    
    A connected open set is called an \textbf{(open) region}. The closure of an (open) region is referred to as a closed region.
\end{definition}

\begin{remark}
    Intuitively, this means that any two points in \(S\) can be connected 
    by a curve lying entirely within \(S\). 
    Clearly, a connected subset of \(\mathbb{R}\) is an interval, 
    and a connected subset of \(\mathbb{R}\) is compact if and only if it is a closed interval.
\end{remark}



\chapter{Multi-variable Differential Calculus}
\section{Directional Derivatives and Total Differential}
\begin{leftbarTitle}{Directional Derivative}\end{leftbarTitle}
\begin{definition}{Directional Derivative}
    Let \(U\subset \mathbb{R}^n\) be an open set, \(f: U\to \mathbb{R}^{1}\),
    \(\mathbf{e}\) is a unit vector in \(\mathbb{R}^{n}\), \(\mathbf{x}^{0}\in U\). Define
    \[
    u(t) = f(\mathbf{x}^{0} + t\mathbf{e}).
    \]
    If the derivative of \(u\) at \(t=0\) 
    \[ 
        u'(0) = \lim_{t \to 0} \frac{u(t) - u(0)}{t} = 
        \lim_{t \to 0} \frac{f(\mathbf{x}^{0} + t\mathbf{e}) - f(\mathbf{x}^{0})}{t} 
    \] 
    exists and is finite, 
    it is called the \textbf{directional derivative} of \(f\) at \(\mathbf{x}^{0}\) in the direction \(\mathbf{e}\), 
    denoted by \(\frac{\partial f}{\partial \mathbf{e}}(\mathbf{x}^{0})\). 
    It is the rate of change of \(f\) at \(\mathbf{x}^{0}\) in the direction \(\mathbf{e}\).
\end{definition}

Consider the following set of unit coordinate vectors: \(\mathbf{e}_{1},\mathbf{e}_{2},\cdots,\mathbf{e}_{n}\).
Let \(\mathbf{e}_{i}=\left( 0, 0, \cdots, 0, 1, 0, \cdots, 0 \right)  \) denote the standard orthonormal basis 
in \(\mathbb{R}^{n}\), where the 1 appears in the \(i\)-th position. That is,
\[
    \langle \mathbf{e}_{i}, \mathbf{e}_{j} \rangle = \delta_{i j} = \begin{cases}
    1, & i = j, \\
    0, & i \neq j.
    \end{cases}
\]
For a function \( f \), the directional derivative of \( f \) at the point \( \mathbf{x}_{0} \) 
in the direction of \( \mathbf{e}_{i} \) 
is called the \( i \)th first-order \textbf{partial derivative} of \( f \) at \(\mathbf{x}^{0}\), denoted by
\[
\frac{\partial f}{\partial x_i}(\mathbf{x}^{0}) \quad \text{or} 
\quad \mathrm{D}_i f(\mathbf{x}^{0})  \quad \text{or} 
\quad f_{x_i}(\mathbf{x}^{0}) \quad (i = 1, 2, \cdots, n).
\]
\( \mathrm{D}_i = \frac{\partial}{\partial x_i} \) is called the \( i \)th partial differential operator (\( i = 1, 2, \cdots, n \)).

Let \(\mathbf{e}_{i}=\sum_{i=0}^{n} \mathbf{e}_{i}\cos\alpha\) be a unit vector, 
where \(\sum_{i=0}^{n} \cos^2\alpha = 1\).
If \(\frac{\partial f}{\partial x_{i}}\) is continuous at \(\mathbf{x}^0\), 
then the directional derivative of \(f\) at \(\mathbf{x}^0\) along the direction \(\mathbf{e}\) is given by:
\[
\frac{\partial f}{\partial \mathbf{e}}(\mathbf{x}^0) = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x}^0) \cos \alpha_i.
\]

This is the formula for \textbf{expressing a directional derivative using partial derivatives}.


\begin{note}
    Let \(\mathbf{e}\) be a direction, then \(\|-\mathbf{e}\| = \|\mathbf{e}\| = 1\), 
    which implies that \(-\mathbf{e}\) is also a direction. At this point, we have:
    \[
    \frac{\partial f}{\partial (-\mathbf{e})}(\mathbf{x}^{0}) = -\frac{\partial f}{\partial \mathbf{e}}(\mathbf{x}^{0}).
    \]
\end{note}



\begin{definition}{Jacobian Matrix (Gradient)}
    Let
    \[
    Jf(\mathbf{x}) = (\mathrm{D}_1 f(\mathbf{x}), \mathrm{D}_2 f(\mathbf{x}), \dots, \mathrm{D}_n f(\mathbf{x})),
    \]
    which is called the \textbf{Jacobian matrix} of the function \( f \) at the point \( \mathbf{x} \), 
    (a \( 1 \times n \) matrix) whose counterpart is the first-order derivative of a single-variable function.

    Henceforth, we represent the point \(\mathbf{x}\) in \( \mathbb{R}^n \) 
    and its increments \(\mathbf{h}\) as column vectors.
    In this way, the differential of the function can be expressed using matrix multiplication as follows:
    \[
    \mathrm{d}f(\mathbf{x}^{0})(\mathbf{\Delta x}) = Jf(\mathbf{x}^{0}) \mathbf{\Delta x}.
    \]
    The Jacobian matrix of the function \( f \) is also frequently denoted as 
    \(\mathrm{grad}\,f\) (or \(\nabla f\)), that is,
    \[
    \mathrm{grad}\,f(\mathbf{x}) = Jf(\mathbf{x}),
    \]
    which is called the \textbf{gradient} of the scalar function \( f \).
\end{definition}

\begin{note}
    Let \(U \subset \mathbb{R}^n\) be an open set, and \(\mathbf{f}: U \to \mathbb{R}^m\) be a \(C^k\) mapping:  
    \begin{itemize}
        \item \(k = 0\), \(\mathbf{f}\) is a continuous mapping;
        \item \(0 < k < +\infty\), \(f_i\) has continuous partial derivatives up to order \(k\), \(i = 1, 2, \dots, m\);
        \item \(k = +\infty\), \(f_i\) has continuous partial derivatives of all orders, \(i = 1, 2, \dots, m\);
        \item \(k = \omega\), \(f_i\) is really analytic, i.e., 
            in the neighborhood of any point \(\mathbf{x}^0 = (x_1^0, x_2^0, \dots, x_n^0) \in U\), 
            \(f_i\) can be expanded into a convergent (\(n\)-dimensional) power series, \(i = 1, 2, \dots, m\).
    \end{itemize}
    Let \(C^k(U, \mathbb{R}^m)\) denote the totality of \(C^k\) mappings from \(U\) to \(\mathbb{R}^m\).
\end{note}


\begin{leftbarTitle}{Total Differential}\end{leftbarTitle}
\begin{definition}{Total Differential}
    Let \(U\subset \mathbb{R}^n\) be an open set, \(f: U\to \mathbb{R}^{1}\), \(\mathbf{x}^{0}\in U\),
    \(\mathbf{\Delta x}=\left( \Delta x_{1},\Delta x_{2},\cdots,\Delta x_{n} \right) \in \mathbb{R}^{n}\). If
    \[
    f(\mathbf{x}^{0} + \mathbf{\Delta x}) - f(\mathbf{x}^{0}) = 
    \sum_{i=1}^n A_{i} \Delta x_{i} + o(\|\mathbf{\Delta x}\|) \qquad (\|\mathbf{\Delta x}\| \to 0),
    \]
    where \(A_{1}, A_{2}, \dots, A_{n}\) are constants independent of \(\mathbf{\Delta x}\), 
    then the function \(f\) is said to be \textbf{differentiable} at the point \(\mathbf{x}^{0}\), 
    and the linear main part \(\sum_{i=1}^n A_{i} \Delta x_{i}\) is called the \textbf{total differential} 
    of \(f\) at \(\mathbf{x}^{0}\), 
    denoted as
    \[
    df(\mathbf{x}^{0})(\mathbf{\Delta x}) = \sum_{i=1}^n A_{i} \Delta x_{i}.
    \]
    If \(f\) is differentiable at every point in the open set \(U\), 
    then \(f\) is called a differentiable function on \(U\).    
\end{definition}

\begin{theorem}{Conditions of Differentiability}
    \begin{description}
        \item[Necessary Condition] If an \(n\)-variable function \(f\) is differentiable at the point \(\mathbf{x}_{0}\), 
        then \(f\) is continuous at \(\mathbf{x}^{0}\) and 
        possesses first-order partial derivatives \(\frac{\partial f}{\partial x_{i}}(\mathbf{x}^{0})\) 
        at \(\mathbf{x}^{0}\) for \(i = 1, 2, \dots, n\), and
        \[
        \mathbf{A} = \left( A_{1}, A_{2}, \dots, A_{n} \right)  = Jf(\mathbf{x}^{0}) = 
        \left(\mathrm{D}_{1}f(\mathbf{x}^{0}), \mathrm{D}_{2}f(\mathbf{x}^{0}), \dots, \mathrm{D}_{n}f(\mathbf{x}^{0}) \right).
        \]\footnote{
            It is referred to as the total differential formula, and the more common form is
            \[
                \mathrm{d}f(x_{0},y_{0})=
                \frac{\partial f}{\partial x}(x_{0},y_{0})\,\mathrm{d}x+\frac{\partial f}{\partial y}(x_{0},y_{0})\,\mathrm{d}y.
            \]
        }
        However, the converse is not true.
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item[Sufficient Condition] Let \(U \subset \mathbb{R}^n\) be an open set, 
        and let \(f: U \to \mathbb{R}^1\) be an \(n\)-variable function. 
        If \(Jf = \left( \mathrm{D}_{1}f, \mathrm{D}_{2}f, \dots, \mathrm{D}_{n}f \right)\) 
        is continuous at \(\mathbf{x}^{0}\) 
        (i.e., \(\frac{\partial f}{\partial x_{i}}\) is continuous at \(\mathbf{x}^{0}\) for \(i = 1, 2, \dots, n\)), 
        then \(f\) is differentiable at \(\mathbf{x}^{0}\).
        However, the converse is not necessarily true.
    \end{description}    
\end{theorem}

\begin{note}
    \begin{itemize}
        \item The continuity of the derivative function at \(\mathbf{x}^{0}\) implies that 
            the original function \(f\) is differentiable in some neighborhood of \(\mathbf{x}^{0}\).
        \item In fact, this condition can be relaxed to require that one partial derivative exists at the point, 
            while the remaining \(n-1\) partial derivative functions are continuous at that point.
    \end{itemize}
\end{note}

\begin{proof}
    Taking a function of three variables as an example.

    Assume the \(3\)-ary function \(f: \mathbb{R}^3 \to \mathbb{R}\) meets:
    \begin{enumerate}
        \item There exists \(f_{z}(x_{0},y_{0},z_{0})\). 
        \item The partial derivative functions \(f_{x}(x,y,z)\) and \(f_{y}(x,y,z)\) are continuous at \((x_{0},y_{0},z_{0})\),
            i.e. there are partial derivatives in some neighborhood of \((x_{0},y_{0},z_{0})\).
    \end{enumerate}
    Consider the total increment of \(f\) at the point \((x_{0},y_{0},z_{0})\):
    \begin{align*}
        \Delta f 
        &= \underbrace{\left[ f(x_0 + \Delta x, y_0 + \Delta y, z_0 + \Delta z) - f(x_0, y_0 + \Delta y, z_0 + \Delta z) \right]}_{I_1}\\
        &+ \underbrace{\left[ f(x_0, y_0 + \Delta y, z_0 + \Delta z) - f(x_0, y_0, z_0 + \Delta z) \right]}_{I_2}\\
        &+ \underbrace{\left[ f(x_0, y_0, z_0 + \Delta z) - f(x_0, y_0, z_0) \right]}_{I_3}.
    \end{align*}
    For \(I_{1}, I_{2}\), by the Lagrange's Mean Value Theorem of unary functions, 
    there exist \(\theta_{1}, \theta_{2} \in (0,1)\) such that
    \begin{gather*}
        I_{1}=f_{x}(x_{0}+\theta_{1}\Delta x,y_{0}+\Delta y,z_{0}+\Delta z)\Delta x,\\
        I_{2}=f_{y}(x_{0},y_{0}+\theta_{2}\Delta y,z_{0}+\Delta z)\Delta y.
    \end{gather*}
    Then by the continuity of the their partial derivatives at \((x_{0},y_{0},z_{0})\), we have
    \[
        \lim_{\Delta x, \Delta y, \Delta z \to 0} I_{1} = f_{x}(x_{0},y_{0},z_{0})\Delta x, \quad
        \lim_{\Delta x, \Delta y, \Delta z \to 0} I_{2} = f_{y}(x_{0},y_{0},z_{0})\Delta y.
    \]
    They can be expressed in terms of infinitesimals(\(\rho = \sqrt{\Delta x^2 + \Delta y^2 + \Delta z^2}\)):
    \begin{align*}
        I_{1}=f_{x}(x_{0},y_{0},z_{0})\Delta x + \alpha_{1}\Delta x, \quad \alpha_{1}\to 0(\rho\to 0),\\
        I_{2}=f_{y}(x_{0},y_{0},z_{0})\Delta y + \alpha_{2}\Delta y, \quad \alpha_{2}\to 0(\rho\to 0).
    \end{align*}
    For \(I_{3}\), by the definition of the partial derivative \(f_{z}(x,y,z)\) at \((x_{0},y_{0},z_{0})\), we have
    \[
        I_{3}=f_{z}(x_{0},y_{0},z_{0})\Delta z + \alpha_{3}\Delta z, \quad \alpha_{3}\to 0(\rho\to 0).
    \]
    Accordingly, 
    \begin{align*}
        \Delta f &= I_{1} + I_{2} + I_{3} \\
        &= \left[ f_{x}(x_{0},y_{0},z_{0})\Delta x + \alpha_{1}\Delta x \right] + \left[ f_{y}(x_{0},y_{0},z_{0})\Delta y + \alpha_{2}\Delta y \right] + \left[ f_{z}(x_{0},y_{0},z_{0})\Delta z + \alpha_{3}\Delta z \right] \\
        &= f_{x}(x_{0},y_{0},z_{0})\Delta x + f_{y}(x_{0},y_{0},z_{0})\Delta y + f_{z}(x_{0},y_{0},z_{0})\Delta z + \left[ \alpha_{1}\Delta x + \alpha_{2}\Delta y + \alpha_{3}\Delta z \right].
    \end{align*}
    Apparently, 
    \[
        \lim_{\rho \to 0} \frac{\alpha_{1}\Delta x + \alpha_{2}\Delta y + \alpha_{3}\Delta z}{\rho} = 0,
    \]
    i.e. \(\alpha_{1}\Delta x + \alpha_{2}\Delta y + \alpha_{3}\Delta z = o(\rho)\). 
    Therefore, \(f(x,y,z)\) is differentiable at \((x_{0},y_{0},z_{0})\), which completes the proof.
\end{proof}

\begin{note}{(At some point)}
    \begin{enumerate}
        \item  Differentiable
            \begin{itemize}
                \item \(\implies\) Continuous
                \item \(\implies\) Partial derivatives exist: \(\mathrm{D}_{\vec{u}}=\nabla f\cdot\vec{u}\)
            \end{itemize}
        \item  Directional Derivative
            \begin{itemize}
                \item All directional derivatives exist \(\not\implies\) differentiable or continuous.
                \item All directional derivatives exist and are equal \(\not\implies\) differentiable.
            \end{itemize}
        \item  Partial Derivative
        \begin{itemize}
            \item  The continuity and existence of directional/partial derivatives are mutually exclusive.
        \end{itemize}
    \end{enumerate}
\end{note}


\begin{leftbarTitle}{Higher-Order Partial Derivatives and Differential}\end{leftbarTitle}
If the first-order partial derivative of \(f\), \(\frac{\partial f}{\partial x_i}\), 
itself possesses partial derivatives, then the second-order partial derivative of \(f\) is defined, 
and is denoted as follows(the first is also called the mixed partial derivative):
\[
f_{x_i x_j} = \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial}{\partial x_j} \left( \frac{\partial f}{\partial x_i} \right), 
\quad f_{x_i x_i} = \frac{\partial^2 f}{\partial x_i^2} = \frac{\partial}{\partial x_i} \left( \frac{\partial f}{\partial x_i} \right), 
\quad i, j = 1, 2, \dots, n.
\]

Similarly, higher-order partial derivatives of order \(3,4,\cdots m,\cdots\) can be defined.

The following theorem provides the conditions under which mixed partial derivatives are equal.

\begin{theorem}{Conditions for Equality of Mixed Partial Derivatives}
    \begin{enumerate}\label{thm:condition_equality}
        \item Let $U \subset \mathbb{R}^2$ be an open set, and $f: U \to \mathbb{R}$ be a function of two variables. 
            If $f_{xy}$ and $f_{yx}$ are continuous at $(x_0, y_0) \in U$, then
            \[
            f_{xy}(x_0, y_0) = f_{yx}(x_0, y_0).
            \]
        \item Let \(U \subset \mathbb{R}^n\) be an open set, and \(f: U \to \mathbb{R}\) be a function of \(n\) variables. 
            If \(f\) has partial derivatives up to order \(k\) in \(D\), 
            and all of them are continuous at \(\mathbf{x}^{0} = (x_1^0, x_2^0, \ldots, x_n^0) \in U\), then
                \[
                \frac{\partial^l f}{\partial x_{i_1} \partial x_{i_2} \cdots \partial x_{i_l}}(\mathbf{x}^{0}) 
                = \frac{\partial^l f}{\partial x_{i_2} \partial x_{i_1} \cdots \partial x_{i_l}}(\mathbf{x}^{0}) 
                = \cdots 
                = \frac{\partial^l f}{\partial x_{i_l} \partial x_{i_l - 1} \cdots \partial x_{i_1}}(\mathbf{x}^{0}),
                \]
            that is, the order of taking partial derivatives \(l ( \leq k )\) does not affect the result.\footnote{
                If the condition "\(f_{xy}\) and \(f_{yx}\) are continuous at \((x_0, y_0)\)",
                is not satisfied, then the conclusion "\(f_{xy}(x_0, y_0) = f_{yx}(x_0, y_0)\)" does not necessarily hold.
            }
    \end{enumerate}
\end{theorem}


\begin{proof}
    When $k \neq 0, h \neq 0$, define
    \[
        \varphi(y) = f(x_0 + h, y) - f(x_0, y),
    \]
    and
    \[
        \psi(x) = f(x, y_0 + k) - f(x, y_0).
    \]
    Applying the Lagrange Mean Value Theorem, we have
    \[
        \begin{aligned}
            &[f(x_0 + h, y_0 + k) - f(x_0, y_0 + k)] - [f(x_0 + h, y_0) - f(x_0, y_0)] \\
            =& \varphi(y_0 + k) - \varphi(y_0) \\
            =& \varphi'(y_0 + \theta_1 k) k \\
            =& [f_y(x_0 + h, y_0 + \theta_1 k) - f_y(x_0, y_0 + \theta_1 k)] k \\
            =& f_{yx}(x_0 + \theta_2 h, y_0 + \theta_1 k) h k, \quad 0 < \theta_1, \theta_2 < 1.
        \end{aligned}
    \]
    On the other hand,
    \[
        \begin{aligned}
            &[f(x_0 + h, y_0 + k) - f(x_0, y_0 + k)] - [f(x_0 + h, y_0) - f(x_0, y_0)] \\
            =& [f(x_0 + h, y_0 + k) - f(x_0 + h, y_0)] - [f(x_0, y_0 + k) - f(x_0, y_0)] \\
            =& \psi(x_0 + h) - \psi(x_0) \\
            =& \psi'(x_0 + \theta_3 h) h \\
            =& [f_x(x_0 + \theta_{3}h, y_0 + k) - f_x(x_0 + \theta_{3}h, y_0)] h \\
            =& f_{xy}(x_0 + \theta_3 h, y_0 + \theta_4 k) h k, \quad 0 < \theta_3, \theta_4 < 1.
        \end{aligned}
    \]
    Therefore,
    \[
    f_{xy}(x_0 + \theta_3 h, y_0 + \theta_4 k) = f_{yx}(x_0 + \theta_2 h, y_0 + \theta_1 k).
    \]
    Since $f_{xy}$ and $f_{yx}$ are continuous at $(x_0, y_0)$, letting $h \to 0, k \to 0$, we obtain
    \[
    f_{xy}(x_0, y_0) = f_{yx}(x_0, y_0).
    \]
    By applying ~\ref{thm:condition_equality} and the principle of mathematical induction, one can immediately derive the following result.
\end{proof}

Suppose \(z=f(x,y)\) has continuous partial derivatives in the domain \(U\subset \mathbb{R}^2\). 
Then \(z\) is differentiable, and
\[
    \mathrm{d}z = \frac{\partial z}{\partial x} \mathrm{d}x + \frac{\partial z}{\partial y} \mathrm{d}y.
\]
If \(z\) also has continuous second-order partial derivatives, 
then \(\frac{\partial z}{\partial x}\) and \(\frac{\partial z}{\partial y}\) are also differentiable, 
and thus \(\mathrm{d}z\) is differentiable. 
We call the differential of \(\mathrm{d}z\) the second-order differential of \(z\), denoted as
\[
    \mathrm{d}^2z = \mathrm{d}(\mathrm{d}z).
\]
In general, based on the \(k\)-th order differential \(\mathrm{d}^kz\) of \(z\), 
its \((k+1)\)-th order differential (if it exists) is defined as
\[
    \mathrm{d}^{k+1}z = \mathrm{d}(\mathrm{d}^kz), \quad k = 1, 2, \cdots .
\]
Due to the fact that for the independent variables \( x \) and \( y \), we always have
\[
    \mathrm{d}^2 x = \mathrm{d}(\mathrm{d}x) = 0, \qquad \mathrm{d}^2 y = \mathrm{d}(\mathrm{d}y) = 0,
\]
the second-order differential of \( z = f(x, y) \) is given by
\begin{align*}
    \mathrm{d}^2 z &= \mathrm{d}(\mathrm{d}z) 
        = \mathrm{d}\left( \frac{\partial z}{\partial x} \mathrm{d}x + \frac{\partial z}{\partial y} \mathrm{d}y \right) \\
    &= \mathrm{d}\left( \frac{\partial z}{\partial x} \right) \mathrm{d}x + \frac{\partial z}{\partial x} \mathrm{d}^2 x 
        + \mathrm{d}\left( \frac{\partial z}{\partial y} \right) \mathrm{d}y + \frac{\partial z}{\partial y} \mathrm{d}^2 y\\
    &= \left( \frac{\partial^2 z}{\partial x^2} \mathrm{d}x + \frac{\partial^2 z}{\partial x \partial y} \mathrm{d}y \right) \mathrm{d}x
        + \left( \frac{\partial^2 z}{\partial y \partial x} \mathrm{d}x + \frac{\partial^2 z}{\partial y^2} \mathrm{d}y \right) \mathrm{d}y\\
    &= \frac{\partial^2 z}{\partial x^2} (\mathrm{d}x)^2 + 2 \frac{\partial^2 z}{\partial x \partial y} \mathrm{d}x \mathrm{d}y + \frac{\partial^2 z}{\partial y^2} (\mathrm{d}y)^2,
\end{align*}
where \( (\mathrm{d}x)^2 \) and \( (\mathrm{d}y)^2 \) denote \( \mathrm{d}^2 x \) and \( \mathrm{d}^2 y \) respectively.
If we treat \( \frac{\partial}{\partial x} \), \( \frac{\partial}{\partial y} \) as operators for partial differentiation 
and define
\[
    \left( \frac{\partial}{\partial x} \right)^2 = \frac{\partial^2}{\partial x^2}, \quad
    \left( \frac{\partial}{\partial y} \right)^2 = \frac{\partial^2}{\partial y^2}, \quad
    \left( \frac{\partial}{\partial x} \frac{\partial}{\partial y} \right) = \frac{\partial^2}{\partial x \partial y},
\]
then the formulas for the first and second differentials can be written as
\[
    \mathrm{d}z = \left( \mathrm{d}x \frac{\partial}{\partial x} + \mathrm{d}y \frac{\partial}{\partial y} \right) z,
\]
\[
    \mathrm{d}^2 z = \left( \mathrm{d}x \frac{\partial}{\partial x} + \mathrm{d}y \frac{\partial}{\partial y} \right)^2 z.
\]
Similarly, we define
\[
    \left( \frac{\partial}{\partial x} \right)^p
    \left( \frac{\partial}{\partial y} \right)^q
    = \frac{\partial^{p+q}}{\partial x^p \partial y^q}
    = \frac{\partial^q}{\partial y^q}
    \left( \frac{\partial}{\partial x} \right)^p,
    \quad (p, q = 1, 2, \dots)
\]
It is easy to use mathematical induction to prove the formula for higher-order differentials:
\[
    \mathrm{d}^k z = \left( \mathrm{d}x \frac{\partial}{\partial x} + \mathrm{d}y \frac{\partial}{\partial y} \right)^k z, 
    \quad k = 1, 2, \cdots.
\]
For an \( n \)-variable function \( u = f(x_1, x_2, \dots, x_n) \), higher-order differentials can be similarly defined, and the following holds:
\[
    \mathrm{d}^k u 
    = \left( \mathrm{d}x_1 \frac{\partial}{\partial x_1} + \mathrm{d}x_2 \frac{\partial}{\partial x_2} 
    + \cdots + \mathrm{d}x_n \frac{\partial}{\partial x_n} \right)^k u, \quad k = 1, 2, \dots.
\]



\section{Differential of Vector-Valued Functions}
Consider an $n$-dimensional vector-valued function defined on a domain $U \subset \mathbb{R}^n$:
\begin{gather*}
    f: U \to \mathbb{R}^m, \\ 
    \mathbf{x} \mapsto \mathbf{y} = f(\mathbf{x})
\end{gather*}
Expressed in coordinate vector form:
\[
    \mathbf{y} =
    \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_m
    \end{pmatrix}
    =
    \begin{pmatrix}
    f_1(x_1, x_2, \dots, x_n) \\
    f_2(x_1, x_2, \dots, x_n) \\
    \vdots \\
    f_m(x_1, x_2, \dots, x_n)
    \end{pmatrix},
    \qquad \mathbf{x} = \begin{pmatrix} 
    x_1 \\ x_2 \\ \vdots \\ x_n 
    \end{pmatrix}  \in U
\]

\begin{enumerate}
    \item  If each component function $f_i(x_1, x_2, \dots, x_n)$ ($i=1,2,\dots,m$) is partially differentiable at $\mathbf{x}^0$, 
        then the vector-valued function $\mathbf{f}$ is differentiable at $\mathbf{x}^0$, and we define the matrix
        \[
            \left( \frac{\partial f}{\partial x_j} (\mathbf{x}^0) \right)_{m \times n}
            =
            \begin{pmatrix}
            \frac{\partial f_1}{\partial x_1}(\mathbf{x}^0) & \frac{\partial f_1}{\partial x_2}(\mathbf{x}^0) & \cdots & \frac{\partial f_1}{\partial x_n}(\mathbf{x}^0) \\
            \frac{\partial f_2}{\partial x_1}(\mathbf{x}^0) & \frac{\partial f_2}{\partial x_2}(\mathbf{x}^0) & \cdots & \frac{\partial f_2}{\partial x_n}(\mathbf{x}^0) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial f_m}{\partial x_1}(\mathbf{x}^0) & \frac{\partial f_m}{\partial x_2}(\mathbf{x}^0) & \cdots & \frac{\partial f_m}{\partial x_n}(\mathbf{x}^0)
            \end{pmatrix}
        \]

        This matrix is called the Jacobian matrix of $\mathbf{f}$ at $\mathbf{x}^0$, 
        denoted by $f'(\mathbf{x}^0)$ (or $\mathrm{D}f(\mathbf{x}^0)$, $J_f(\mathbf{x}^0)$).

        For the special case $m=1$, i.e., $n$-variable scalar function $z=f(x_1,x_2,\dots,x_n)$, 
        the derivative at $\mathbf{x}^0$ is
        \[
            f'(\mathbf{x}^0) = 
            \left( 
                \frac{\partial f}{\partial x_1}(\mathbf{x}^0), \frac{\partial f}{\partial x_2}(\mathbf{x}^0), 
                \cdots, \frac{\partial f}{\partial x_n}(\mathbf{x}^0) 
            \right)
        \]
        If the vector-valued function $\mathbf{f}$ is differentiable at every point in $U$, 
        then $\mathbf{f}$ is said to be differentiable on $U$, and the corresponding relationship is
        \[
        \mathbf{x} \in U \mapsto f'(\mathbf{x}) = J_f(\mathbf{x})
        \]
        where $f'(\mathbf{x})$ (or $\mathrm{D}f(\mathbf{x})$, $J_f(\mathbf{x})$) 
        denotes the derivative of $\mathbf{f}$ at $\mathbf{x}$ in $U$.
    \item  If every component function $f_i(x_1, x_2, \dots, x_n)$ $(i=1,2,\dots,m)$ of $\mathbf{f}$ 
        has continuous partial derivatives at $\mathbf{x}^0$, 
        then every element of the Jacobian matrix of $\mathbf{f}$ is continuous at $\mathbf{x}^0$. 
        In this case, $\mathbf{f}$ is said to have a continuous derivative at $\mathbf{x}^0$ as a vector-valued function.
        
        If the derivative of a vector-valued function $\mathbf{f}$ is continuous at every point in $U$, 
        then $\mathbf{f}$ is said to have a continuous derivative on $U$.
    \item  If there exists an $m \times n$ matrix $A$ that depends only on $\mathbf{x}^0$ (and not on $\Delta \mathbf{x}$), 
        such that in the neighborhood of $\mathbf{x}^0$,
        \[
        \Delta \mathbf{y} = f(\mathbf{x}^0 + \Delta \mathbf{x}) - f(\mathbf{x}^0) = A \Delta \mathbf{x} + o(\|\Delta \mathbf{x}\|)
        \]
        (where $\Delta \mathbf{x} = (\Delta x_1, \Delta x_2, \dots, \Delta x_n)^T$ is a column vector and 
        $\|\Delta \mathbf{x}\|$ denotes its norm), 
        then $f$ is said to be differentiable at $\mathbf{x}^0$ as a vector-valued function, 
        and $A\Delta \mathbf{x}$ is called the differential of $f$ at $\mathbf{x}^0$, denoted as $\mathrm{d}\mathbf{y}$. 
        If we denote $\Delta \mathbf{x}$ by 
        $\mathrm{d}\mathbf{x}$ ($\mathrm{d}\mathbf{x} = (\mathrm{d}x_1, \mathrm{d}x_2, \dots, \mathrm{d}x_n)^T$), then
        \[
            \mathrm{d}\mathbf{y} = A\,\mathrm{d}\mathbf{x}.
        \]

        If the vector-valued function $\mathbf{f}$ is differentiable at every point in $U$, 
        then $\mathbf{f}$ is said to be differentiable on $U$.
\end{enumerate}

Combining the above three points, we obtain the following unified statement:

A vector-valued function \(\mathbf{f}\) is continuous, differentiable, 
and has derivatives if and only if each of its coordinate component functions 
\(f_i(x_1, x_2, \dots, x_n)\) (\(i = 1, 2, \dots, m\)) is continuous, differentiable, and has derivatives.


\section{Derivatives of Composite Mappings (Chain Rule)}
Let \( U \subset \mathbb{R}^l \) and \( V \subset \mathbb{R}^n \) be open sets, and let 
\[
\mathbf{g}: U \to V \quad \text{and} \quad \mathbf{f}: V \to \mathbb{R}^m
\]
be mappings. If \( \mathbf{g} \) is derivative at \( \mathbf{u}^{0} \in U \) 
and \( \mathbf{f} \) is differentiable at \( \mathbf{x}^{0} = \mathbf{g}(\mathbf{u}^{0}) \), 
then the composite mapping \( \mathbf{f} \circ \mathbf{g} \) is differentiable at \( \mathbf{u}^{0} \), and:
\[
J(\mathbf{f} \circ \mathbf{g})(\mathbf{u}^{0}) = 
J\mathbf{f}(\mathbf{x}^{0}) J\mathbf{g}(\mathbf{u}^{0}).
\]

\begin{note}
    \begin{enumerate}
        \item  outer differentiable + inner derivative = total derivative
        \item  outer differentiable + inner differentiable = total differentiable
        \item  
    \end{enumerate}
\end{note}

Specially, define \( z = f(x, y), (x,y)\subset D_{f}\subset \mathbb{R}^{2} \), 
\(\mathbf{g}:D_{g}\to \mathbb{R}^{2}, (u,v)\mapsto (x(u,v), y(u,v))\), 
and \(g(D_{g})\subset D_{f}\), 
then we have composite function
\[
z = f \circ \mathbf{g} = f\left[x(u,v), y(u,v)\right],\quad (u,v)\in D_{g}.
\]
\[
\mathbb{R}^{2}\xrightarrow{\mathbf{g}:\text{derivative}}\mathbb{R}^{2}\xrightarrow{f:\text{differentiable}}\mathbb{R}
\]
If \(\mathbf{g}\) is derivative at \((u_{0}, v_{0})\in D_{g}\), 
and \(f\) is differentiable at \((x_{0}, y_{0}) = \mathbf{g}(u_{0}, v_{0})\), 
then \(z = f \circ \mathbf{g}\) is differentiable at \((u_{0}, v_{0})\), and at the point,
\[
    \begin{bmatrix} 
        \frac{\partial z}{\partial u}   & \frac{\partial z}{\partial v} 
    \end{bmatrix} 
    =
    \begin{bmatrix}
        \frac{\partial z}{\partial x} & \frac{\partial z}{\partial y}
    \end{bmatrix}
    \begin{bmatrix} 
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{bmatrix} 
\]

\begin{proof}
    
\end{proof}

\section{Mean Value Theorem and Taylor's Formula}

\begin{definition}{Convex Region}
    Let \(D \subseteq \mathbb{R}^n\) be a region. 
    If every line segment connecting any two points \(\mathbf{x}_0, \mathbf{x}_1 \in D\) 
    (denoted by \(\overline{\mathbf{x}_0 \mathbf{x}_1}\))
    is entirely contained in \(D\), i.e., for any \(\lambda \in [0, 1]\), we have  
    \[
    \mathbf{x}_0 + \lambda (\mathbf{x}_1 - \mathbf{x}_0) \in D,
    \]
    then \(D\) is called a convex region.
\end{definition}

\begin{theorem}{Lagrange's Mean Value Theorem}\label{thm:Multi_Lagrange}
    Let \(f\) be \underline{differentiable} on \underline{a convex region} \(D \subseteq \mathbb{R}^n\). 
    For any two points \(\mathbf{a}, \mathbf{b} \in D\), 
    there exists a point \(\mathbf{\xi}\in \overline{\mathbf{a} \mathbf{b}}\)
    such that:  
    \[
    f(\mathbf{b}) - f(\mathbf{a}) = Jf(\mathbf{\xi})(\mathbf{b} - \mathbf{a}).
    \]
\end{theorem}

\begin{proof}
    
\end{proof}

\begin{theorem}
    Let \(D\) be a region in \(\mathbb{R}^n\). If for any \(\mathbf{x} \in D\), we have  
    \[
    Jf(\mathbf{x}) = 0,
    \]
    then \(f\) is constant on \(D\).
\end{theorem}

\begin{proof}
    
\end{proof}

\begin{theorem}{Taylor's Formula}
    \begin{description}
        \item[Lagrange's Remainder]  Let \(D \subseteq \mathbb{R}^n\) be a convex region, 
            and let \(f: D \to \mathbb{R}\) have \(m+1\) continuous partial derivatives. 
            For \(\mathbf{x}^0 = (x_1^0, x_2^0, \dots, x_n^0) \in D\) and \(\mathbf{x} = (x_1, x_2, \dots, x_n) \in D\), 
            there exists \(\mathbf{\xi} \in \overline{\mathbf{x}^0 \mathbf{x}}\) such that:  
            \[
            f(\mathbf{x}) = f(\mathbf{x}^0) 
            + \sum_{k=1}^m \frac{1}{k!} \left( \sum_{i=1}^n (x_i - x_i^0) \frac{\partial}{\partial x_i} \right)^k f(\mathbf{x}^0) 
            + \frac{1}{(m+1)!} \left( \sum_{i=1}^n (x_i - x_i^0) \frac{\partial}{\partial x_i} \right)^{m+1} f(\mathbf{\xi}).
            \]
        \item[Peano's Remainder] Let \(D \subseteq \mathbb{R}^n\) be a convex region, 
            and let \(f: D \to \mathbb{R}\) have \(m\) continuous partial derivatives. 
            Then:
            \[
            f(\mathbf{x}) = f(\mathbf{x}^0) 
            + \sum_{k=1}^m \frac{1}{k!} \sum_{i_1, i_2, \dots, i_k=1}^n 
            \frac{\partial^k f}{\partial x_{i_1} \partial x_{i_2} \dots \partial x_{i_k}}(\mathbf{x}^0) 
            \prod_{j=1}^{k} (x_{i_j} - x_{i_j}^0) 
            + R_m(\mathbf{x} - \mathbf{x}^0),
            \]
            where \(R_m(\mathbf{x} - \mathbf{x}^0) = O(\|\mathbf{x} - \mathbf{x}^0\|^{m+1})\) or \(o(\|\mathbf{x} - \mathbf{x}^0\|^{m})\), as \(\|\mathbf{x} - \mathbf{x}^0\| \to 0\).

    \end{description}
\end{theorem}

In applications, particularly important is the expression of the first three terms in Taylor's formula, which is given as
(let \(x_1 - x_1^0\) be denoted by \(\Delta x_1\), and similarly for other variables;
\(\Delta \mathbf{x} = (\Delta x_1, \Delta x_2, \dots, \Delta x_n)\)):
\[
    f(\mathbf{x}) = f(\mathbf{x}^0) + Jf(\mathbf{x}^0)(\Delta\mathbf{x})
    + \frac{1}{2!}(\Delta\mathbf{x})Hf(\mathbf{x}^0)(\Delta \mathbf{x})^{\mathrm{T}}+ \cdots,
\]
where the matrix
\[
Hf(\mathbf{x}^0) =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} 
    & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} 
    & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} 
    & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}_{\mathbf{x}^0}
\]
is called the \textbf{Hessian matrix} of the function \(f\). It is an \(n \times n\) symmetric matrix.


\section{Implicit Function Theorem}

\begin{theorem}{Implicit Function Theorem}
    Let \(U \subset \mathbb{R}^{n+1}\) be an open set, and \(F: U \to \mathbb{R}\) be an \(n+1\)-variable function. If:  
    \begin{enumerate}
        \item \(F \in C^k(U, \mathbb{R})\), where \(1 \leq k \leq +\infty\);
        \item \(F(\mathbf{x}^0, y^0) = 0\), 
            where \(\mathbf{x}^0 = (x_1^0, x_2^0, \dots, x_n^0) \in \mathbb{R}^n\), \(y^0 \in \mathbb{R}\), 
            and \((\mathbf{x}^0, y^0) \in U\) 
            (i.e., the equation \(F(\mathbf{x}, y) = 0\) has a solution \((\mathbf{x}^0, y^0)\));
        \item \(F'_y(\mathbf{x}^0, y^0) \neq 0\).
    \end{enumerate}

    Then there exists an open interval \(I \times J\) containing \((\mathbf{x}^0, y^0)\) 
    (\(I\) being an open interval in \(\mathbb{R}^n\) containing \(\mathbf{x}^0\), 
    and \(J\) being an open interval in \(\mathbb{R}\) containing \(y^0\)), 
    as shown in Fig.~\ref{fig:ImplicitFunction}, such that:  
    \begin{enumerate}
        \item \(\forall x \in I\), the equation \(F(\mathbf{x}, y) = 0\) has a unique solution \(y = f(\mathbf{x})\), 
            where \(f: I \to J\) is an \(n\)-variable function 
            (called the \textbf{implicit function} \(f\), hidden within the equation \(F(\mathbf{x}, f(\mathbf{x})) = 0\), 
            though not necessarily explicitly expressed);
        \item \(y^0 = f(\mathbf{x}^0)\);
        \item \(f \in C^k(I, \mathbb{R})\);
        \item When \(x \in I\), 
            \(\frac{\partial f}{\partial x_i} = \frac{\partial y}{\partial x_i} = -\frac{F_x(\mathbf{x}, y)}{F_y(\mathbf{x}, y)}\), 
            \(i = 1, 2, \dots, n\), where \(y = f(x)\).
    \end{enumerate}
\end{theorem}  
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/ImplicitFunction.png}
    \caption{Implicit Function}
    \label{fig:ImplicitFunction}
\end{figure}

\begin{proof}
    Only the single-variable implicit function theorem is proved; 
    the multi-variable case can be derived using mathematical induction.
    
    Without loss of generality, assume \(F_y(x^0, y^0) > 0\).

    First, prove the \underline{existence of the implicit function}.  
    From the continuity of \(F_y(x^{0}, y^{0}) > 0\) and \(F_y(x, y)\), 
    it is known that there exist closed rectangle:
    \[
    D^* = \{(x, y) \mid |x - x_0| \leq \alpha, |y - y_0| \leq \beta\} \subset U,
    \]
    where the following holds:
    \[
    F_y(x, y) > 0.
    \]
    Thus, for fixed \(x_0\), the function \(F(x^{0}, y)\) is strictly monotonically increasing 
    within \([y^{0} - \beta, y^{0} + \beta]\). Furthermore, since:
    \[
    F(x^{0}, y^{0}) = 0,
    \]
    it follows that:
    \[
    F(x^{0}, y^{0} - \beta) < 0, \quad F(x^{0}, y^{0} + \beta) > 0.
    \]
    Due to the continuity of \(F(x, y)\) within \(D^*\), there exists \(\rho > 0\) such that along the line segment:
    \[
    x = x^{0} + \rho, \, y = y^{0} + \beta,
    \]
    we have \(F(x, y) > 0\), and along the line segment:
    \[
    x = x^{0} + \rho, \, y = y^{0} - \beta,
    \]
    we have \(F(x, y) < 0\).
    Therefore, for any point \(\bar{x} \in (x^{0} - \rho, x^{0} + \rho)\), treat \(F(x, y)\) as a single-variable function of \(y\). 
    Within \([y^{0} - \beta, y^{0} + \beta]\), this function is continuous. From the previous discussion, we know:
    \[
    F(\bar{x}, y^{0} - \beta) < 0, \quad F(\bar{x}, y^{0} + \beta) > 0.
    \]
    According to the zero point existence theorem~\ref{thm:Zero Point Existence Theorem}, 
    there must exist a unique \(\bar{y} \in [y^{0} - \beta, y^{0} + \beta]\) 
    such that \(F(\bar{x}, \bar{y}) = 0\). 
    Furthermore, because \(F_y(x, y) > 0\) within \(D^*\), this \(\bar{y}\) is unique.
    Denote the corresponding relationship as \(\bar{y} = f(\bar{x})\), 
    then the function \(y = f(x)\) is defined within \((x^{0} - \rho, x^{0} + \rho)\), 
    satisfying \(F(x, f(x)) = 0\), and clearly:
    \[
    y^{0} = f(x^{0}).
    \]

    Further proving \underline{the continuity of the implicit function} \(y = f(x)\) on \((x^{0} - \rho, x^{0} + \rho)\):  
    Let \(\bar{x} \in (x^{0} - \rho, x^{0} + \rho)\) be any point. 
    For any given \(\varepsilon > 0\) (\(\varepsilon\) being sufficiently small), 
    since \(F(\bar{x}, \bar{y}) = 0\) (\(\bar{y} = f(\bar{x})\)), from the previous discussion we know:  
    \[
    F(\bar{x}, \bar{y} - \varepsilon) < 0, \quad F(\bar{x}, \bar{y} + \varepsilon) > 0.
    \]
    Furthermore, due to the continuity of \(F(x, y)\) on \(D^*\), there exists \(\delta > 0\) such that:  
    \[
    F(x, \bar{y} - \varepsilon) < 0, \quad F(x, \bar{y} + \varepsilon) > 0, \quad \text{when} \quad x \in O(x^{0}, \delta).
    \]
    By reasoning similar to the previous discussion, 
    it can be obtained that when \(x \in O(x^{0}, \delta)\), 
    the corresponding implicit function value must satisfy \(f(x) \in (\bar{y} - \varepsilon, \bar{y} + \varepsilon)\), i.e.,  
    \[
    \left|f(x) - f(x^{0})\right| < \varepsilon.
    \]
    This implies that \(y = f(x)\) is continuous on \((x^{0} - \rho, x^{0} + \rho)\).  

    Finally, prove the \underline{differentiability} of \(y = f(x)\) on \((x^{0} - \rho, x^{0} + \rho)\):  
    Let \(\bar{x} \in (x^{0} - \rho, x^{0} + \rho)\) be any point. 
    Take \(\Delta x\) sufficiently small such that \(\bar{x} = x + \Delta x \in (x^{0} - \rho, x^{0} + \rho)\). 
    Denote \(\bar{y} = f(\bar{x})\) and \(\bar{y} + \Delta y = f(\bar{x})\). Clearly,  
    \[
    F(\bar{x}, \bar{y}) = 0 \quad \text{and} \quad F(\bar{x}, \bar{y} + \Delta y) = 0.
    \]
    Using the multi-variable function's mean value theorem~\ref{thm:Multi_Lagrange}, we obtain:  
    \begin{align*}
        0   &= F(\bar{x}, \bar{y} + \Delta y) - F(\bar{x}, \bar{y}) \\
            &= F_x(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y) \Delta x + F_y(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y) \Delta y,
    \end{align*}
    where \(0 < \theta < 1\).  
    Note that \(F_y \neq 0\) on \(D^*\), hence:  
    \[
    \frac{\Delta y}{\Delta x} = 
        -\frac{F_x(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y)}{F_y(\bar{x} + \theta \Delta x, \bar{y} + \theta \Delta y)}.
    \]
    Let \(\Delta x \to 0\). Considering the continuity of \(F_x\) and \(F_y\), we obtain:  
    \[
    \frac{dy}{dx} \Big|_{x = \bar{x}} = -\frac{F_x(\bar{x}, \bar{y})}{F_y(\bar{x}, \bar{y})}.
    \]
    Thus:  
    \[
    f'(\bar{x}) = -\frac{F_x(\bar{x}, \bar{y})}{F_y(\bar{x}, \bar{y})}.
    \]

    The proof is complete.
\end{proof}


\chapter{Multiple Integrals}




\begin{thebibliography}{99} 
\bibitem{1} 徐森林, 薛春华. \emph{数学分析 (1st edition) }. 清华大学出版社, 2005.
\bibitem{2} 陈纪修, 於崇华. \emph{数学分析 (3rd edition) }. 高等教育出版社, 2019.
\bibitem{3} 常庚哲, 史济怀. \emph{数学分析教程 (3rd edition) }. 中国科学技术大学出版社, 2012.
\bibitem{4} 裴礼文. \emph{数学分析中的典型问题与方法 (3rd edition) }. 高等教育出版社, 2021.
\bibitem{5} 汪林. \emph{数学分析中的问题与反例 (1st edition) }. 高等教育出版社, 2015.
\bibitem{6} 谢惠民, 恽自求, 易法槐, 钱定边. \emph{数学分析习题课讲义 (2nd edition) }. 高等教育出版社, 2019.
\bibitem{7} Walter Rudin. \emph{Principles of Mathematical Analysis (3rd edition) }. McGraw-Hill, 1976.
\bibitem{8} 菲赫金哥尔茨. \emph{微积分学教程 (8th edition) }. 高等教育出版社, 2006.
\bibitem{9} Wikipedia. \url{https://en.wikipedia.org/wiki/}.
\end{thebibliography}

\end{document}