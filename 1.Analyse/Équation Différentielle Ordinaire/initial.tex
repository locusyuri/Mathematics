\documentclass[11pt]{../../TexTemplate/elegantbook}

\title{Équation Différentielle Ordinaire} % 这里放置书名
% \subtitle{Subtitle} % 这里放置副标题

\author{MonoCat} % 这里放置作者名
\date{July, 2025} % 这里放置日期
\version{0.1} % 这里放置版本号
% \institute{Elegant\LaTeX{} Program} % 这里放置机构名
% \bioinfo{Custom Key}{Custom Value} % 这里放置自定义信息

% \extrainfo{extra information} % 这里放置额外信息，将显示在最下方中央

\setcounter{tocdepth}{2} % 设置目录深度
\setcounter{secnumdepth}{2} % 设置章节编号深度


% \logo{logo-blue.png} % 这里放置封面logo，默认从figure目录下寻找
% \cover{LogiqueMathematique.png} % 这里放置封面图片，默认从figure目录下寻找

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170} % 自定义颜色
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect} % 保护命令参数不被 LaTeX 解析器过早处理，允许在某些特殊环境中使用脆弱命令（fragile commands）。
\usepackage{xeCJK} % 使用 xeCJK 包支持中文


% ===== 开始文档 =====
\begin{document}

\maketitle %生成文档的标题页，根据之前定义的标题信息（如标题、作者、日期等）自动创建一个格式化的标题页

% === 前言部分 ===
\frontmatter        % 开始前言，页码为 i, ii, iii...
\tableofcontents    % 目录 (页码: i, ii)
% \listoffigures      % 图表目录 (页码: iii)
% \listoftables       % 表格目录 (页码: iv)

\chapter{Preface}   % 前言章节（无编号，页码: v, vi...）
% \chapter{Acknowledgments}  % 致谢（无编号）
% I would like to thank...
% === 正文部分 ===
\mainmatter         % 开始正文，页码从 1 重新开始

\chapter{Introduction} % 这里放置章节标题
\section{Classification of Differential Equations} % 这里放置小节标题
An equation involving one dependent variable and its derivatives with respect to one or more independent variables 
is called a \textbf{differential equation}.
Differential equations can be classified according to the following criteria:
%% 
\begin{leftbarTitle}{Number of Independent Variables}\end{leftbarTitle}
An \textbf{ordinary differential equation(ODE)} is defined as an equation of the following form:
\begin{equation}\label{eq:plain ODE}
F\left( x, y, \frac{\mathrm{d}y}{\mathrm{d}x}, \frac{\mathrm{d}^2y}{\mathrm{d}x^2}, \ldots, \frac{\mathrm{d}^ny}{\mathrm{d}x^n} \right) = 0,\quad n \in \mathbb{N},
\end{equation}
or, using the prime notation for derivatives,
\begin{equation*}
F\left( x, y, y', y'', \ldots, y^{(n)} \right) = 0,\quad n \in \mathbb{N}.
\end{equation*}
If there are two or more independent variables, the equation is called a \textbf{partial differential equation(PDE)}.
%% 
\begin{leftbarTitle}{Order}\end{leftbarTitle}
The order of a differential equation is the order of the highest derivative present in the equation.
\begin{itemize}
    \item A first-order equation has the form $ F(x, y, y') = 0 $.
    \item A second-order equation has the form $ F(x, y, y', y'') = 0 $.
    \item Higher-order equations involve derivatives of order three or more.
\end{itemize}
%%
\begin{note}
    Crucially, the order tells you how many initial conditions are needed to find a unique solution.
\end{note}
%%
\begin{leftbarTitle}{Linearity}\end{leftbarTitle}
An $n$-th order differential equation is linear if it can be written in the form: 
\begin{equation*}
    a_n(x)y^{(n)} + a_{n-1}(x)y^{(n-1)} + \dots + a_1(x)y' + a_0(x)y = g(x)
\end{equation*}
where the coefficients $a_{i}(x)$ and the term $g(x)$ depend only on the independent variable $x$.
Otherwise, it is nonlinear.
%%
\begin{note}
    Specially, for the aforementioned equation, if $g(x) = 0$, it is called \textbf{homogeneous},
    and \textbf{non-homogeneous} otherwise.
\end{note}

\section{Solution to a Ordinary Differential Equation}
\begin{leftbarTitle}{Particular and General Solutions}\end{leftbarTitle}
Let $J$ be an interval in $\mathbb{R}$. 
A function $y=\phi(x)$ defined on the interval $J$ is called a solution to equation~\eqref{eq:plain ODE} if it satisfies: 
\[
F(x, \phi(x), \phi'(x), \phi''(x), \dots, \phi^{(n)}(x)) = 0 \quad x \in J. 
\]
The interval $J$ is then called the interval of existence of the solution $y = \phi(x)$.

Generally speaking, 
the solution to equation~\eqref{eq:plain ODE} contains one or more arbitrary constants, 
the determination of which depends on other conditions that the solution must satisfy. 
If a solution to a differential equation does not contain any arbitrary constants, 
it is called a \textbf{particular solution} of the differential equation.

Suppose $y = \phi(x; c_{1}, c_{2}, \cdots, c_{n})$ is a solution to equation~\eqref{eq:plain ODE}, 
where $c_{1}, c_{2}, \ldots, c_{n}$ are arbitrary constants. 
If $c_{1}, c_{2}, \ldots, c_{n}$ are mutually independent, 
then $y = \phi(x; c_{1}, c_{2}, \ldots, c_{n})$ is called the \textbf{general solution} to equation~\eqref{eq:plain ODE}. 
Here, "mutually independent" means that the Jacobian determinant is non-zero: 
\[
\det \frac{\partial(\phi, \phi', \dots, \phi^{(n-1)})}{\partial(c_1, c_2, \dots, c_n)} \neq 0, \quad x \in J.
\]
When all the arbitrary constants in the general solution are determined, 
one obtains a particular solution to the differential equation.

\begin{leftbarTitle}{Initial Conditions, Explicit and Implicit Solutions}\end{leftbarTitle}
Let $y = \phi(x)$ be a solution to equation~\eqref{eq:plain ODE} that also satisfies 
\begin{equation}\label{eq:initial conditions}
    \phi(x_0) = y_0, \quad \phi'(x_0) = y_0', \dots, \quad \phi^{(n-1)}(x_0) = y_0^{(n-1)}.
\end{equation}
The conditions~\eqref{eq:initial conditions} are called the \textbf{initial conditions} for equation~\eqref{eq:plain ODE}, 
and $y = \phi(x)$ is called the solution to equation~\eqref{eq:plain ODE} 
satisfying the initial conditions~\eqref{eq:initial conditions}.
Such initial value problems are often referred to as \textbf{Cauchy problems}.

A function $y = \phi(x)$ that turns the differential equation~\eqref{eq:plain ODE} into an identity is called an 
\textbf{(explicit) solution} to the equation. 
If a solution $y = \phi(x)$ to the differential equation~\eqref{eq:plain ODE} is determined 
by the relation $\Phi(x, y) = 0$, then $\Phi(x, y) = 0$ is called an \textbf{implicit solution} to the differential equation~\eqref{eq:plain ODE}. 
An implicit solution is also called an "integral". 

\begin{leftbarTitle}{Integral Curve and Direction Field}\end{leftbarTitle}
Consider the first-order differential equation:
\begin{equation}\label{eq:first order ODE}
    \frac{dy}{dx} = f(x,y),
\end{equation}
where $f$ is continuous in a planar region $G$. 
Suppose 
\[
y = \phi(x), \quad x \in J
\] 
is a solution to this equation, where $J \subset \mathbb{R}$ is an interval. 
Then the set of points in the plane 
\[
\Gamma = { (x,y) | y = \phi(x), x \in J }
\]
is a differentiable curve in the plane. 
This curve is called a solution curve or an \textbf{integral curve}.

Let $(x_0, y_0) \in \Gamma$. 
The slope of the tangent line to the curve $\Gamma$ at this point is 
\[
\phi'(x_{0}) = f(x_{0}, y_{0}).
\]
Therefore, the equation of the tangent line is 
\[
    y - y_0 = f(x_0, y_0)(x - x_0).
\]
This implies that even without knowing the explicit expression for $\phi$, 
we can obtain the slope and equation of the tangent line to the solution curve 
at a given point from equation~\eqref{eq:first order ODE}. 

\begin{remark}
    Note that in a small neighborhood of a point on a differentiable curve, 
    the tangent line can be seen as a first-order approximation of the curve. 
    Utilizing this viewpoint, one can obtain an approximate solution to the differential equation. 
    In fact, this is the fundamental idea behind Euler's method.
\end{remark}

At each point $P$ in the region $G$, 
we can draw a short line segment $l(P)$ with slope $f(P)$. 
We call $l(P)$ the line element of equation~\eqref{eq:first order ODE} at point $P$. 
The region $G$ together with the entire collection of these line elements is called 
the lineal \textbf{linear element field} or \textbf{direction field} for equation~\eqref{eq:first order ODE}.

\begin{theorem}
    A necessary and sufficient condition for a continuously differentiable curve 
    $\Gamma = \{ (x,y) | y = \psi(x), x \in J \}$ in the plane 
    to be an integral curve of equation~\eqref{eq:first order ODE} is that 
    for every point $(x, y)$ on the curve $\Gamma$, 
    its tangent line at that point coincides with the line element determined 
    by equation~\eqref{eq:first order ODE} at that point.
\end{theorem}

\begin{proof}
    The necessity follows from the preceding discussion. 
    We now prove the sufficiency. 
    For any point $(x, y) = (x, \psi(x))$ on the curve $\Gamma$, 
    the slope of the tangent line to $\Gamma$ at this point is $\psi'(x)$. 
    By the condition of the theorem, we have $\psi'(x) = f(x, y)$. 
    Since $(x, y)$ is an arbitrary point on the curve, it follows that $y = \psi(x)$ is a solution to equation~\eqref{eq:first order ODE}. 
\end{proof}

\chapter{First Order Equations}
\section{Exact Equations}
\begin{definition}{Exact Equations}
An equation of the form
\begin{equation}\label{eq:symmetric form}
    M(x,y) \, \mathrm{d}x + N(x,y) \, \mathrm{d}y = 0
\end{equation}
is called the symmetric form of a first-order differential equation.

If there exists a continuously differentiable function $u(x,y)$ such that
\[
\mathrm{d}U(x,y) = M(x,y) \, \mathrm{d}x + N(x,y) \, \mathrm{d}y,
\]
then equation~\eqref{eq:symmetric form} is said to be an \textbf{exact equation} or a \textbf{total differential equation}.

It follows that, when equation~\eqref{eq:symmetric form} is exact, it can be rewritten as
\[
\mathrm{d}(U(x,y)) = 0,
\]
which implies
\begin{equation}\label{general integral}
    U(x,y) = c,
\end{equation}
where $c$ is an arbitrary constant. Equation~\eqref{general integral} is called the \textbf{general integral} of equation~\eqref{eq:symmetric form}.
\end{definition}
\begin{remark}
    It should be noted that, strictly speaking, 
    equation~\eqref{eq:symmetric form} is not a differential equation. 
    However, expressing a first-order differential equation in the form of \eqref{eq:symmetric form} is extremely convenient for analysis.
    This formulation does not necessarily require $y$ to be expressed as a function of $x$. 
    For the sake of simplicity in description, we often refer to the symmetric form~\eqref{eq:symmetric form} as a differential equation, too.
\end{remark}

\begin{theorem}
    Let the functions \(M(x, y)\) and \(N(x, y)\) be continuous in a simply connected domain \(D \subset \mathbb{R}^2\), 
    and suppose their first-order partial derivatives \(\frac{\partial M}{\partial y}\) and \(\frac{\partial N}{\partial x}\) are also continuous. 
    Then a necessary and sufficient condition for equation~\eqref{eq:symmetric form} to be exact is
    \[
    \frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}
    \]
    in the domain \(D\). 
    When this condition holds, for any \((x_0, y_0), (x, y) \in D\), a general integral of equation~\eqref{eq:symmetric form} is given by
    \[
    \int_{\gamma} M(x, y)dx + N(x, y)dy = c,
    \]
    where \(\gamma\) is any curve composed of finitely many smooth segments within \(D\) 
    connecting \((x_0, y_0)\) and \((x, y)\), and \(c\) is an arbitrary constant.
\end{theorem}
\begin{proof}
    
\end{proof}

The aforementioned proof also serves as a method for determining the bivariate function $U(x, y)$ 
that satisfies specific conditions. In addition to this approach, 
there exist two simpler methods for solving $U(x, y)$.

\begin{description}
    \item [Utilizing Curve Integrals to Solve $U(x, y)$] 


    \item [Term Combination Method]
        Utilizing the properties of bivariate differential functions, 
        we combine the terms of the differential equation into a full differential form. 
        This method requires familiarity with some simple bivariate differential functions, such as:
        \begin{gather*}
            y\mathrm{d}x + x\mathrm{d}y = \mathrm{d}(xy), \\
            \frac{y\mathrm{d}x-x\mathrm{d}y}{y^{2}} = \mathrm{d}\left(\frac{x}{y}\right), \\
            \frac{-y\mathrm{d}x+x\mathrm{d}y}{x^{2}} = \mathrm{d}\left(\frac{y}{x}\right), \\
            \frac{1}{x}\mathrm{d}x + \frac{1}{y}\mathrm{d}y = \frac{y\mathrm{d}x+x\mathrm{d}y}{xy} = \mathrm{d}(\ln |xy|), \\
            \frac{1}{x}\mathrm{d}x - \frac{1}{y}\mathrm{d}y = \frac{y\mathrm{d}x-x\mathrm{d}y}{xy} = \mathrm{d}(\ln |\frac{x}{y}|), \\
            \frac{y\mathrm{d}x-x\mathrm{d}y}{x^{2}-y^{2}} = \frac{1}{2}\mathrm{d}\left(\ln \left|\frac{x-y}{x+y}\right|\right), \\
            \frac{y\mathrm{d}x+x\mathrm{d}y}{x^{2}+y^{2}} = \mathrm{d}\left(\arctan \frac{y}{x}\right), \\
            \frac{y\mathrm{d}x-x\mathrm{d}y}{x^{2}+y^{2}} = \mathrm{d}\left( \operatorname{arccot} \frac{y}{x} \right).
        \end{gather*}
\end{description}


\vspace{0.7cm}
The theory above can also be rewritten in differential form:

Let:
\[
\omega^1 = M(x, y) \, dx + N(x, y) \, dy.
\]
The differential form \( \omega^1 \) is said to be \textbf{closed} if \( \mathrm{d}\omega^1 = 0 \). 
It is called \textbf{exact} if there exists a function \( U(x, y) \) such that \( \omega^1 = \mathrm{d}U(x, y) \). 
By the Poincaré theorem, it can be concluded that on \( \mathbb{R}^2 \), 
a first-order differential form is exact if and only if it is closed.
Note that:
\[
\mathrm{d}\omega^1 = \left( \frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} \right) \mathrm{d}x \wedge \mathrm{d}y.
\]
Clearly, \( \mathrm{d}\omega^1 = 0 \) holds if and only if:
\[
\frac{\partial N}{\partial x} = \frac{\partial M}{\partial y}.
\]
Under this condition, the expression for the function \( U(x, y) \) is:
\[
U(x, y) = \int \omega^1.
\]


\section{Separable Equations}
\begin{definition}{Separable Equations}
    If the functions \(M(x, y)\) and \(N(x, y)\) in Equation~\eqref{eq:symmetric form} can both be written as 
    the product of a function of \(x\) and a function of \(y\), that is,
    \[
    M(x, y) = M_1(x) M_2(y), \quad N(x, y) = N_1(x) N_2(y),
    \]
    then equation~\eqref{eq:symmetric form} is called a separable equation.

    When equation~\eqref{eq:symmetric form} is a separable equation, it can be written as
    \begin{equation}\label{eq:separable form}
        M_1(x) M_2(y) \, \mathrm{d}x + N_1(x) N_2(y) \, \mathrm{d}y = 0,
    \end{equation}
    or more conveniently as
    \begin{equation}\label{eq:separable form 2}
        \frac{\mathrm{d}y}{\mathrm{d}x} = f(x)g(y) \left(= -\frac{M_1(x)}{N_1(x)} \cdot \frac{N_2(y)}{M_2(y)}\right).
    \end{equation}
\end{definition}

\begin{theorem}{Solutions to Separable Equations}
    All the solutions to the separable equation~\eqref{eq:separable form} are given by:
    \[
    \int_{x_{0}}^{x}  \frac{M_{1}(t)}{N_{1}(t)}\mathrm{d}t + \int_{y_{0}}^{y} \frac{N_{2}(s)}{M_{2}(s)}\mathrm{d}s = c,
    \]
    and 
    \begin{gather*}
        y \equiv b_{i}, \quad i = 1, 2, \ldots, m, 
        x \equiv a_{j}, \quad j = 1, 2, \ldots, n,
    \end{gather*}
    where \(M_{2}(b_{i})=0\quad(i=1,2,\ldots,m)\) and \(N_{1}(a_{j})=0\quad(j=1,2,\ldots,n)\), \(c\) is arbitrary constant.
\end{theorem}

\section{Homogeneous Equations}
\begin{definition}\label{def:homogeneous equations}
    A first-order differential equation 
    \begin{equation*}
        M(x, y) \, \mathrm{d}x + N(x, y) \, \mathrm{d}y = 0
    \end{equation*}
    is called a \textbf{homogeneous equation} if both \(M\) and \(N\) are homogeneous functions
    \footnote{
        A function \(f(x, y)\) is called a homogeneous function of degree \(n\) 
        if it satisfies the condition \(f(tx, ty) = t^n f(x, y)\) for all \(t > 0\).
        }
    of the same degree \(n\).    
    In other words, for the equation
    \[
        \frac{\mathrm{d}y}{\mathrm{d}x} = f(x, y), 
    \]
    \(f(x,y)\) can be rewritten as \(g\left(\frac{y}{x}\right)\).
\end{definition}


The equation  
\begin{equation}\label{eq:Second kind separable equation}
    \frac{dy}{dx} = f\left(\frac{a_1x + b_1y + c_1}{a_2x + b_2y + c_2}\right) 
\end{equation}
can be transformed into a separable equation via variable change, 
where \(a_1, a_2, b_1, b_2, c_1, c_2\) are constants. 

\begin{itemize}
    \item  When \(c_1 = c_2 = 0\), the equation becomes:  
        \[
        \frac{dy}{dx} = f\left(\frac{a_1 + b_1 \frac{y}{x}}{a_2 + b_2 \frac{y}{x}}\right) = g\left(\frac{y}{x}\right).
        \]
        Let
        \[
        u = \frac{y}{x}, \text{ namely } y = ux. 
        \]
        Differentiating both sides with respect to \(x\), we get:
        \[
        \frac{dy}{dx} = x \frac{du}{dx} + u.
        \]
        Substituting the results into original equation and simplifying, we obtain:
        \[
        \frac{du}{dx} = \frac{g(u) - u}{x},
        \]
        which is a separable equation. 
        It can be solved easily. 
        Then, substituting \(u = \frac{y}{x}\) back, the solution is derived.
    \item When \(c_1, c_2\) are not entirely zero, the right-hand side of ~\eqref{eq:Second kind separable equation} 
        consists of linear polynomials of \(x\) and \(y\). Therefore:
        \[
        \begin{cases}\label{eq gp:two lines}
        a_1x + b_1y + c_1 = 0, \\
        a_2x + b_2y + c_2 = 0,
        \end{cases}
        \]
        represents two intersecting straight lines on the \(Oxy\) plane. For the coefficient determinant of the system:
        \[
        \begin{vmatrix}
        a_1 & b_1 \\
        a_2 & b_2
        \end{vmatrix},
        \]
        two cases are analyzed:
        \begin{enumerate}
            \item  If \(\begin{vmatrix} a_1 & b_1 \\ a_2 & b_2 \end{vmatrix} \neq 0\), 
                then \(\frac{a_1}{a_2} \neq \frac{b_1}{b_2}\), 
                indicating that the two lines intersect at a unique point \((\alpha, \beta)\) on the \(Oxy\) plane. 
                Let:
                \[
                \begin{cases}
                X = x - \alpha, \\
                Y = y - \beta,
                \end{cases}
                \]
                then~\eqref{eq gp:two lines} becomes:
                \[
                \begin{cases}
                a_1X + b_1Y = 0, \\
                a_2X + b_2Y = 0.
                \end{cases}
                \]
                Substituting into~\ref{eq:Second kind separable equation}, it simplifies to:
                \[
                \frac{dY}{dX} 
                = f\left(\frac{a_1 + b_1 \frac{Y}{X}}{a_2 + b_2 \frac{Y}{X}}\right) 
                = g\left(\frac{Y}{X}\right).
                \]
                This is a homogeneous differential equation. 
                Solving it by substitution and reverting back to the original variables 
                yields the solution to equation~\ref{eq:Second kind separable equation}.
            \item When \(\begin{vmatrix} a_1 & b_1 \\ a_2 & b_2 \end{vmatrix} = 0\). 
                To ensure this system holds, there are three possible scenarios:
                \begin{enumerate}
                    \item  If \(a_1 = b_1 = 0\),~\ref{eq:Second kind separable equation} becomes:
                        \[
                        \frac{dy}{dx} = f\left(\frac{c_1}{a_2x + b_2y + c_2}\right),
                        \]
                        and when \(a_2 = b_2 = 0\), it becomes:
                        \[
                        \frac{dy}{dx} = f\left(\frac{a_1x + b_1y + c_1}{c_2}\right).
                        \]
                        In this case, 
                        let
                        \[
                        u = \frac{a_1x + b_1y + c_1}{c_2}.
                        \]
                        Then it can be transformed into a separable equation.
                    \item  If \(b_1 = b_2 = 0\), ~\ref{eq:Second kind separable equation} transforms into:
                        \[
                        \frac{dy}{dx} = f\left(\frac{a_1x + c_1}{a_2x + c_2}\right),
                        \]
                        and
                        \[
                        \frac{dy}{dx} = f\left(\frac{b_1y + c_1}{b_2y + c_2}\right),
                        \]
                        when \(a_1 = a_2 = 0\). 
                    \item If \(\frac{a_1}{a_2} = \frac{b_1}{b_2} = k\), let \(u = a_2x + b_2y\). In this case:
                        \begin{gather*}
                            \frac{du}{dx} = a_2 + b_2 \frac{\mathrm{d}y}{\mathrm{d}x} \\
                            f\left( \frac{k(a_2x + b_2y) + c_1}{(a_2x + b_2y) + c_2} \right) 
                                = f\left( \frac{ku + c_1}{u + c_2} \right) = g(u)
                        \end{gather*}
                        which simplifies to:
                        \[
                        \frac{du}{dx} = a_2 + b_2 g(u).
                        \]
                \end{enumerate}
        \end{enumerate}
\end{itemize}






\begin{example}
    A function \(f(x, y)\) is called a quasihomogeneous function of degree \(d\) with generalized weights if
    \[
    f(t^\alpha s x, t^\beta s y) = t^{ds} f(x, y),
    \]
    where \(t > 0\), \(\alpha\) and \(\beta\) are positive constants with \(\alpha + \beta = 1\), and \(s \in \mathbb{R}\). 
    Here, \(\alpha\) and \(\beta\) are called the weights of \(x\) and \(y\), respectively.
    Consider the differential equation
    \[
    M(x, y) \, \mathrm{d}x + N(x, y) \, \mathrm{d}y = 0,
    \]
    where \(M(x, y)\) and \(N(x, y)\) are quasihomogeneous functions of degree \(d_0\) and \(d_1\) 
    with weights \(\alpha\) and \(\beta\) for \(x\) and \(y\), respectively.
    Proposition: When \(d_0 = d_1 + \beta - \alpha\) the equation can be solved by elementary integration method.
\end{example}
\section{Linear Equations}
\begin{definition}{First-Order Linear Equations}
    A \textbf{first-order linear equation} is an equation of the form
    \begin{equation}\label{eq:first order linear equation}
        \frac{\mathrm{d}y}{\mathrm{d}x} + p(x) y = q(x),
    \end{equation}
    where \(p(x)\) and \(q(x)\) are continuous functions on the interval \((a, b)\).
    In Equation~\eqref{eq:first order linear equation}, when \(q(x) \equiv  0\), we obtain
    \begin{equation}\label{eq:first order homogeneous linear equation}
        \frac{\mathrm{d}y}{\mathrm{d}x} + p(x) y = 0,
    \end{equation}
    which is called a \textbf{first-order homogeneous linear equation} corresponding to Equation~\eqref{eq:first order linear equation}.
    Otherwise, it is called a first-order non-homogeneous linear equation.
\end{definition}
\begin{note}
    It should be noted that the definition of a homogeneous equation here \emph{differs} from that in the previous section.
\end{note}

Firstly, we solve the first-order homogeneous linear equation.
Equation~\ref{eq:first order homogeneous linear equation} is separable, 
thus its general solution is given by:
\[
y = ce^{-\int p(x) \, \mathrm{d}x},
\]
where \(c\) is an arbitrary constant.

Since ~\ref{eq:first order homogeneous linear equation} is a special case of ~\ref{eq:first order linear equation},
the general solution of ~\ref{eq:first order linear equation} can be expressed as:
\[
y = c(x) e^{-\int p(x) \, \mathrm{d}x},
\]
substituting it into ~\ref{eq:first order linear equation} yields:
\[
y = e^{-\int p(x) \, \mathrm{d}x} \left( c + \int q(x) e^{\int p(x) \, \mathrm{d}x} \, \mathrm{d}x \right).
\]

This method of solving first-order linear equations is known as the \textbf{method of variation of constants}.


\begin{definition}{Bernoulli's Equation}
    A first-order differential equation of the form
    \begin{equation*}
        \frac{\mathrm{d}y}{\mathrm{d}x} + p(x) y = q(x) y^n,\quad n \neq 0, 1,
    \end{equation*}
    where \(n\) is a real number and \(p(x)\) and \(q(x)\) are continuous functions on the interval \((a, b)\), 
    is called a \textbf{Bernoulli's equation}.
\end{definition}
Bernoulli's equation can be transformed into a first-order linear equation by the substitution:
\[z = y^{1-n}.\]
Differentiating both sides with respect to \(x\) gives:
\[\frac{\mathrm{d}z}{\mathrm{d}x} = (1-n) y^{-n} \frac{\mathrm{d}y}{\mathrm{d}x}.\]
Substituting \(\frac{\mathrm{d}y}{\mathrm{d}x}\) from Bernoulli's equation into the above expression yields:
\[\frac{\mathrm{d}z}{\mathrm{d}x} = (1-n) \left( -p(x) z + q(x) \right).\]
This is a first-order linear equation in \(z\), which can be solved using the method for first-order linear equations.

\section{Integrating Factors}
\begin{definition}{Integrating Factors}
    An \textbf{integrating factor} for a first-order differential equation of the form
    \begin{equation}\label{eq:integrating factor equation}
        M(x, y) \, \mathrm{d}x + N(x, y) \, \mathrm{d}y = 0
    \end{equation}
    is a differentiable function \(\mu(x, y)\) such that when multiplied by the equation:
    \[
    \mu(x, y) M(x, y) \, \mathrm{d}x + \mu(x, y) N(x, y) \, \mathrm{d}y = 0,
    \]
    it becomes an exact equation.
    Id est, there exists a function \(\Phi(x, y)\) such that
    \[
    \mu(x, y) M(x, y) \, \mathrm{d}x + \mu(x, y) N(x, y) \, \mathrm{d}y = \mathrm{d}U(x, y).
    \]
    If such functions \(\mu(x, y)\) and \(U(x, y)\) exist, and \(U(x, y)\) is smooth, then
    \begin{equation}\label{eq:integrating factor condition}
        \frac{\partial(\mu M)}{\partial y} = \frac{\partial(\mu N)}{\partial x}
        \left( = \frac{\partial^2 U}{\partial x \partial y} \right).
    \end{equation}
    In this case, \(\mu(x, y)\) is called an integrating factor for equation~\eqref{eq:integrating factor equation}.
\end{definition}
According to Equation~\eqref{eq:integrating factor condition}, 
finding an integrating factor \(\mu(x, y)\) for equation~\eqref{eq:integrating factor equation}
is equivalent to solving the partial differential equation:
\begin{equation}\label{eq:integrating factor PDE}
    \frac{\partial \mu}{\partial x} N - \frac{\partial \mu}{\partial y} M 
    = \left( \frac{\partial M}{\partial y} - \frac{\partial N}{\partial x} \right) \mu.
\end{equation}

\begin{theorem}
\begin{enumerate}
    \item For the partial differential equation~\ref{eq:integrating factor PDE} 
        to have a solution $\mu(x)$ that depends only on $x$, the necessary and sufficient condition is:

        The function $G$ defined below must depend only on $x$:
        \[
        G = -\frac{1}{N(x, y)}\left( \frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} \right) .
        \]

        In this case, we have:
        \[
        \mu(x) = e^{\int_{x_0}^{x} G(t) \, dt}.
        \]
    
    \item 
        For the partial differential equation~\ref{eq:integrating factor PDE} 
        to have a solution $\mu(y)$ that depends only on $y$, 
        the necessary and sufficient condition is:

        The function $H$ defined below must depend only on $y$:
        \[
        H = \frac{1}{M(x, y)}\left( \frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} \right) .
        \]

        In this case, we have:
        \[
        \mu(y) = e^{\int_{y_0}^{y} H(s) \, ds}.
        \]

    \item 
        For equation~\ref{eq:integrating factor equation} to have an integrating factor of the form 
        $\mu = \mu(\phi(x, y))$, the necessary condition is:
        \[
        \frac{1}{\frac{\partial \phi}{\partial x} N - \frac{\partial \phi}{\partial y} M} 
        \left( \frac{\partial M}{\partial y} - \frac{\partial N}{\partial x} \right) 
        = f(\phi(x, y)),
        \]
        where \(f\) is a certain univariate function.
\end{enumerate}
\end{theorem}


\begin{theorem}
    Let the functions \(P(x, y)\), \(Q(x, y)\), \(\mu_1(x, y)\), and \(\mu_2(x, y)\) be continuously differentiable. 
    Suppose \(\mu_1(x, y)\) and \(\mu_2(x, y)\) are integrating factors for equation~\eqref{eq:integrating factor equation}, 
    and the ratio \(\frac{\mu_1(x, y)}{\mu_2(x, y)}\) is not a constant. 
    Then: 
    \[
    \frac{\mu_1(x, y)}{\mu_2(x, y)} = c
    \]
    is a general solution to the equation, where \(c\) is an arbitrary constant.
\end{theorem}


\section{Implicit Equations}
This section discusses the problem of solving the first-order implicit differential equations,
\begin{equation}\label{eq:implicit ODE}
F(x, y, y') = 0
\end{equation}
where \(F\) is a continuously differentiable function. 
A so-called implicit differential equation is one in which \(y'\) does not have an explicit solution, 
that is, the equation cannot be written in the form \(y' = f(x, y)\).

\begin{leftbarTitle}{Differentiation Method}\end{leftbarTitle}
Suppose that Equation~\eqref{eq:implicit ODE} can be solved for \(y\), that is,
\begin{equation}\label{eq:differentiation method}
    y = f(x, p),\quad p = \frac{dy}{dx},
\end{equation}
where \(f(x, p)\) is a continuously differentiable function.

Differentiating both sides of \(y = f(x, p)\) with respect to \(x\), we obtain
\[
p = \frac{dy}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial p} \frac{dp}{dx},
\]
that is,
\begin{equation*}
\frac{\partial f}{\partial p} \frac{dp}{dx} = p - \frac{\partial f}{\partial x}.
\end{equation*}

This is a first-order differential equation in the variables \(x\), \(p\), \(\frac{dp}{dx}\). 
If a solution \(p = p(x)\) can be found, then Equation~\eqref{eq:differentiation method} yields a solution
\[
y = f(x, p(x)).
\]
\begin{leftbarTitle}{Parametric Method}\end{leftbarTitle}
In general, Equation~\eqref{eq:implicit ODE} represents a surface in the \((x, y, p)\)-space. 
Therefore, the solution can be obtained using a parametric representation of the surface. 
Suppose the parametric form of the surface described by Equation~\eqref{eq:implicit ODE} is
\[
x = x(u, v),\quad y = y(u, v),\quad p = p(u, v) = y'.
\]
Note that
\[
dy = p \; \mathrm{d}x,
\]
thus we obtain
\[
y_u' \mathrm{d}u + y_v' \mathrm{d}v = p(u, v)(x_u' \mathrm{d}u + x_v' \mathrm{d}v).
\]
This is an explicit differential equation in the variables \(u\) and \(v\). Suppose it admits a solution
\[
v = v(u, c),
\]
where \(c\) is a constant, then Equation~\eqref{eq:implicit ODE} has a solution
\[
x = x(u, v(u, c)),\quad y = y(u, v(u, c)).
\]

\chapter{Existence and Uniqueness Theorem}
\section{Picard-Lindelöf Theorem}
\begin{theorem}{Bellman-Gronwall Inequality}
    Let \(f(x), g(x)\) be continuous functions on the interval \([a, b]\),
    \(g(x) \geqslant  0\), and \(c\) be a non-negative constant.
    If
    \[
    f(x) \leqslant  c + \int_{a}^{x} f(t) g(t) \, dt,
    \]
    then
    \[
    f(x) \leqslant  c \exp\left(\int_{a}^{x} g(t) \, dt\right).
    \]
\end{theorem}

For a Cauchy problem:
\begin{equation}\label{eq:Cauchy problem}
\begin{cases}
    \frac{\mathrm{d}y}{\mathrm{d}x} = f(x, y), \\
    y(x_0) = y_0,
\end{cases}
\end{equation}
give the existence and uniqueness theorem.

\begin{leftbarTitle}{Picard-Lindelöf Theorem}\end{leftbarTitle}
\begin{theorem}{Picard-Lindelöf Theorem}
    In the Cauchy problem~\eqref{eq:Cauchy problem},
    let \(D\) be a closed rectangle in the \(xy\)-plane: 
    \[
    D = [x_{0}-a, x_{0}+a] \times [y_{0}-b, y_{0}+b].
    \]
    If the function \(f(x, y)\) satisfies the following two conditions:
    \begin{enumerate}
        \item \(f(x, y)\) is continuous in \(D\).
        \item \(f(x, y)\) satisfies the Lipschitz condition with respect to \(y\) in \(D\), 
            i.e., there exists a constant \(L > 0\) such that for any \((x, y_1), (x, y_2) \in D\),
            \[
            |f(x, y_1) - f(x, y_2)| \leqslant  L |y_1 - y_2|.
            \]
    \end{enumerate}
    Then there exists a unique solution \(y = \varphi(x)\) (\(\varphi(x_0) = y_0\)) to the Cauchy problem~\eqref{eq:Cauchy problem}
    in the interval \([x_{0}-h, x_{0}+h]\), where
    \[
    h = \min\left\{a, \frac{b}{M}\right\}, M = \max_{(x, y) \in D} |f(x, y)|.
    \]
\end{theorem}

\begin{proposition}
    
\end{proposition}

\begin{leftbarTitle}{Peano Theorem and Osgood Theorem}\end{leftbarTitle}
In regard to the solutions for the Cauchy problem~\eqref{eq:Cauchy problem},
we have the following two theorems, which are weaker than the Picard-Lindelöf theorem:

\begin{definition}{Osgood Condition}
    Let \(f(x, y)\) be a continuous function in the region \(D\).
    If for any \((x, y_1), (x, y_2) \in D\), 
    \[
    |f(x, y_1) - f(x, y_2)| \leqslant  F(|y_1 - y_2|),
    \]
    where \(F(t) > 0\) (\(t > 0\)) is a continuous function, and
    \[
    \int_{0}^{\varepsilon} \frac{1}{F(t)} \, dt = +\infty, \quad \forall \varepsilon > 0,
    \]
    then \(f(x, y)\) is said to satisfy the \textbf{Osgood condition} with respect to \(y\) in \(D\).
\end{definition}

\begin{remark}
    If \(f(x, y)\) satisfies Lipschitz condition, then it also satisfies the Osgood condition.
    In fact, in this case, we can take \(F(t) = Lt\).
\end{remark}

\begin{theorem}{Peano Theorem}
    In the Cauchy problem~\eqref{eq:Cauchy problem},
    let \(D\) be a closed rectangle in the \(xy\)-plane: 
    \[
    D = [x_{0}-a, x_{0}+a] \times [y_{0}-b, y_{0}+b]
    \].
    If the function \(f(x, y)\) is continuous in \(D\),
    then there \underline{exists} at least one solution \(y = \varphi(x)\) (\(\varphi(x_0) = y_0\)) 
    to the Cauchy problem~\eqref{eq:Cauchy problem}
    in the interval \([x_{0}-h, x_{0}+h]\), where
    \[
    h = \min\left\{a, \frac{b}{M}\right\}, M = \max_{(x, y) \in D} |f(x, y)|.
    \]
\end{theorem}

\begin{theorem}{Osgood Theorem}
    In the Cauchy problem~\eqref{eq:Cauchy problem},
    let \(D\) be a closed rectangle in the \(xy\)-plane: 
    \[
    D = [x_{0}-a, x_{0}+a] \times [y_{0}-b, y_{0}+b]
    \].
    If the function \(f(x, y)\) satisfies the Osgood condition with respect to \(y\) in \(D\),
    then there exists a unique solution for any \((x_{0}, y_{0}) \in D\)
    to the Cauchy problem~\eqref{eq:Cauchy problem}
    in the interval \([x_{0}-h, x_{0}+h]\), where
    \[
    h = \min\left\{a, \frac{b}{M}\right\}, M = \max_{(x, y) \in D} |f(x, y)|.
    \]
\end{theorem}


\section{Continuation of the Solution}
\begin{leftbarTitle}{Uncontinuable Solutions}\end{leftbarTitle}
\begin{definition}{Uncontinuable Solutions}
    Let \(y = \varphi(x)\) be a solution to the Cauchy problem~\eqref{eq:Cauchy problem} in the interval \(I_1\subset \mathbb{R}\).
    If there exists an another solution \(y = \varphi_{2}(x)\) to the Cauchy problem~\eqref{eq:Cauchy problem} 
    in any interval \(I_{2} \supsetneq I_{1}\) such that
    \[
    \varphi_{2}(x) \equiv \varphi(x), \quad x \in I_{1},
    \]
    then \(y = \varphi_{1}(x)\) is called \textbf{continuable},
    and \(y = \varphi_{2}(x)\) is called a \textbf{continuation} of \(y = \varphi_{1}(x)\).
    If there does not exist such a solution \(y = \varphi_{2}(x)\), 
    then \(y = \varphi_{1}(x)\) is called \textbf{uncontinuable}, or \textbf{saturated}.
\end{definition}

\begin{theorem}
    In the Cauchy problem~\eqref{eq:Cauchy problem},
    let \(D\) be a \underline{bounded} closed rectangle in the \(xy\)-plane.
    If the function \(f(x, y)\) is continuous in \(D\),
    and satisfies the local Lipschitz condition with respect to \(y\) in \(D\),
    then any solution \(y = \varphi(x)\) passing through \((x_{0}, y_{0}) \in D\) 
    to the Cauchy problem~\eqref{eq:Cauchy problem} can be continued
    until it arbitrarily approaches the boundary of \(D\).
\end{theorem}

\begin{leftbarTitle}{Comparison Theorem}\end{leftbarTitle}

\section{Singular Solutions and Envelopes}

\section{Dependency of Solutions on Initial Data}

\chapter{System of First-Order Linear Equations}
\section{System of First-Order Linear Equations}
\begin{leftbarTitle}{Common Forms}\end{leftbarTitle}
System of first-order equations with \(n\) variables is of the form:
\begin{equation}\label{eq:system of first-order linear equations}
    \begin{cases}
        \frac{\mathrm{d}y_1}{\mathrm{d}x} = f_1(x, y_1, y_2, \ldots, y_n), \\
        \frac{\mathrm{d}y_2}{\mathrm{d}x} = f_2(x, y_1, y_2, \ldots, y_n), \\
        \quad\vdots \\
        \frac{\mathrm{d}y_n}{\mathrm{d}x} = f_n(x, y_1, y_2, \ldots, y_n).
    \end{cases}
\end{equation}
If the right-hand side of each equation in system~\eqref{eq:system of first-order linear equations}
does not include explicitly \(x\), then the system is called \textbf{autonomous}.

The solution to system~\eqref{eq:system of first-order linear equations} is an \(n\)-tuple of functions
\[y_1 = \varphi_1(x), y_2 = \varphi_2(x), \ldots, y_n = \varphi_n(x),\]
which satisfy all equations in system~\eqref{eq:system of first-order linear equations} simultaneously.

Solution containing arbitrary constants \(C_{1}, C_{2}, \ldots, C_{n}\)
\[
\begin{cases} 
    y_{1}=\varphi_{1}(x, C_{1}, C_{2}, \ldots, C_{n}),\\ 
    y_{2}=\varphi_{2}(x, C_{1}, C_{2}, \ldots, C_{n}), \\ 
    \quad\vdots \\ 
    y_{n}=\varphi_{n}(x, C_{1}, C_{2}, \ldots, C_{n}) 
\end{cases}
\]
is called the \textbf{general solution} of system~\eqref{eq:system of first-order linear equations}.
If general solution satisfies 
\[
\begin{cases} 
    \Phi_{1}(x, y_{1}, \cdots, y_{n}, C_{1}, \cdots, C_{n})=0, \\ 
    \Phi_{2}(x, y_{1}, \cdots, y_{n}, C_{1}, \cdots, C_{n})=0, \\ 
    \quad\vdots \\ 
    \Phi_{n}(x, y_{1}, \cdots, y_{n}, C_{1}, \cdots, C_{n})=0,
\end{cases}
\]
then it is called the \textbf{general integral} of system~\eqref{eq:system of first-order linear equations}.

\vspace{0.7cm}
For convenience, we rewrite system~\eqref{eq:system of first-order linear equations} in matrix form:
\begin{equation}\label{eq:matrix form of system of first-order linear equations}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{F}(x, \mathbf{Y}),
\end{equation}
and autonomous system as:
\begin{equation}\label{eq:matrix form of autonomous system of first-order linear equations}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{F}(\mathbf{Y}),
\end{equation}
and Cauchy problem for system~\eqref{eq:matrix form of system of first-order linear equations} as:
\begin{equation}\label{eq:Cauchy problem for system of first-order linear equations}
\begin{cases}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{F}(x, \mathbf{Y}), \\
    \mathbf{Y}(x_{0}) = \mathbf{Y}_{0},
\end{cases}
\end{equation}
where 
\[
\mathbf{Y} = \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix}, \quad
\mathbf{F}(x, \mathbf{Y}) = 
\begin{bmatrix} f_{1}(x, y_{1}, y_{2}, \ldots, y_{n}) \\ f_{2}(x, y_{1}, y_{2}, \ldots, y_{n}) \\ 
    \vdots \\ f_{n}(x, y_{1}, y_{2}, \ldots, y_{n}) 
\end{bmatrix},
\quad
\frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = 
\begin{bmatrix} 
    \frac{\mathrm{d}y_{1}}{\mathrm{d}x} \\ \frac{\mathrm{d}y_{2}}{\mathrm{d}x} \\ \vdots \\ \frac{\mathrm{d}y_{n}}{\mathrm{d}x} 
\end{bmatrix}.
\]
With these notations, from the perspective of form, 
the system of first-order linear equations is similar to first-order equations.

\vspace{0.7cm}
In system~\eqref{eq:system of first-order linear equations},
if \(f_{i}(x, y_{1}, y_{2}, \ldots, y_{n})\) is a linear function of \(y_{1}, y_{2}, \ldots, y_{n}\), 
i.e., it can be rewritten as:
\[
\begin{cases} 
    \frac{\mathrm{d}y_{1}}{\mathrm{d}x} = a_{11}(x)y_{1} + a_{12}(x)y_{2} + \cdots + a_{1n}(x)y_{n} + f_{1}(x), \\ 
    \frac{\mathrm{d}y_{2}}{\mathrm{d}x} = a_{21}(x)y_{1} + a_{22}(x)y_{2} + \cdots + a_{2n}(x)y_{n} + f_{2}(x), \\ 
    \quad\vdots \\ 
    \frac{\mathrm{d}y_{n}}{\mathrm{d}x} = a_{n1}(x)y_{1} + a_{n2}(x)y_{2} + \cdots + a_{nn}(x)y_{n} + f_{n}(x).
\end{cases}
\]
It is called a \textbf{system of first-order linear equations}.
\(a_{ij}(x)\) and \(f_{i}(x)\) are always assumed to be continuous on some interval \(I \subset \mathbb{R}\), 
where \(i, j = 1, 2, \ldots, n\).

For convenience, we rewrite the system of first-order linear equations in matrix form:
\begin{equation}\label{eq:matrix form of system of first-order linear equations - linear}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A}(x) \mathbf{Y} + \mathbf{F}(x),
\end{equation}
where
\[
\mathbf{A}(x) = 
\begin{bmatrix} 
    a_{11}(x) & a_{12}(x) & \cdots & a_{1n}(x) \\ 
    a_{21}(x) & a_{22}(x) & \cdots & a_{2n}(x) \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    a_{n1}(x) & a_{n2}(x) & \cdots & a_{nn}(x)
\end{bmatrix}, \quad
\mathbf{F}(x) =
\begin{bmatrix}
    f_{1}(x) \\ f_{2}(x) \\ \vdots \\ f_{n}(x)
\end{bmatrix}.
\]
On the interval \(I\), if \(\mathbf{F}(x) \equiv \mathbf{0}\), that is,
\begin{equation}\label{eq:matrix form of system of first-order linear equations - linear and homogeneous}
\frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A}(x) \mathbf{Y}
\end{equation}
then system~\eqref{eq:matrix form of system of first-order linear equations - linear} is called a \textbf{homogeneous system},
otherwise, it is called a \textbf{non-homogeneous system}.

\begin{leftbarTitle}{General Theory}\end{leftbarTitle}

\begin{theorem}{Existence and Uniqueness Theorem for System of First-Order Linear Equations}
    \label{thm:existence and uniqueness theorem for system of first-order linear equations}
    In the Cauchy problem~\eqref{eq:Cauchy problem for system of first-order linear equations},
    let \(D\) be a closed region in the \(\mathbb{R}^{n+1}\):
    \[
    D = [x_{0}-a, x_{0}+a] \times [\mathbf{Y}_{0}-b, \mathbf{Y}_{0}+b].
    \]
    If the function \(\mathbf{F}(x, \mathbf{Y})\) satisfies the following two conditions:
    \begin{enumerate}
        \item \(\mathbf{F}(x, \mathbf{Y})\) is continuous in \(D\).
        \item \(\mathbf{F}(x, \mathbf{Y})\) satisfies the Lipschitz condition with respect to \(\mathbf{Y}\) in \(D\), 
            i.e., there exists a constant \(L > 0\) such that for any \((x, \mathbf{Y}_{1}), (x, \mathbf{Y}_{2}) \in D\),
            \[
            \|\mathbf{F}(x, \mathbf{Y}_{1}) - \mathbf{F}(x, \mathbf{Y}_{2})\| \leqslant  L \|\mathbf{Y}_{1} - \mathbf{Y}_{2}\|.
            \]
    \end{enumerate}
    Then there exists a unique solution \(\mathbf{Y} = \mathbf{\Phi}(x)\) (\(\mathbf{\Phi}(x_0) = \mathbf{Y}_0\))
    to the Cauchy problem~\eqref{eq:Cauchy problem for system of first-order linear equations}
    in the interval \([x_{0}-h, x_{0}+h]\), where
    \[
    h = \min\{a, \frac{b}{M}\}, \quad M = \max_{(x, \mathbf{Y}) \in D} \|\mathbf{F}(x, \mathbf{Y})\|.
    \]
\end{theorem}

\section{General Theory of Homogeneous Linear Systems}
Similar to linear homogeneous systems of algebraic equations,
the linear combination of solutions to homogeneous linear systems of differential equations
is still a solution to the system.
\begin{proposition}
    If \(\mathbf{Y}_{1}(x)\) and \(\mathbf{Y}_{2}(x)\) are two solutions to the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then any linear combination of them
    \[
    \mathbf{Y}(x) = C_{1} \mathbf{Y}_{1}(x) + C_{2} \mathbf{Y}_{2}(x),
    \]
    where \(C_{1}\) and \(C_{2}\) are arbitrary constants, is also a solution to the system.

    Three or more solutions also have this property.
\end{proposition}
With this proposition, it is easy to verify that the set of all solutions to 
the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}
forms a linear space. And similarly, linear independence of solutions can be defined.
Then we can introduce the concept of fundamental solution matrix.

\begin{definition}{Fundamental Solution Matrix}
    Let \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) be \(n\) linearly independent solutions to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
    Then the matrix
    \[
    \mathbf{\Phi}(x) = 
    \begin{pmatrix}
        \mathbf{Y}_{1}(x) & \mathbf{Y}_{2}(x) & \cdots & \mathbf{Y}_{n}(x) \\
    \end{pmatrix}
    = 
    \begin{pmatrix}
        y_{11}(x) & y_{12}(x) & \cdots & y_{1n}(x) \\
        y_{21}(x) & y_{22}(x) & \cdots & y_{2n}(x) \\
        \vdots & \vdots & \ddots & \vdots \\
        y_{n1}(x) & y_{n2}(x) & \cdots & y_{nn}(x)
    \end{pmatrix}
    \]
    is called a \textbf{fundamental solution matrix} of the system.

    Simultaneously, such a set of solutions is called a \textbf{fundamental solution system}.
\end{definition}

\begin{leftbarTitle}{Criteria for Linear Dependence}\end{leftbarTitle}
Given \(n\) vector functions with \(n\) components each:

\begin{equation}\label{VectorFunctionGroup}
    \mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x),
\end{equation}
criteria for their linear independence on the definition interval \(I\) is provided by the following theorem.
\begin{theorem}{Wronskian Determinant Theorem}
    For vector functions~\eqref{VectorFunctionGroup},
    let
    \[
    W(x) = \det
    \begin{pmatrix}
        y_{11}(x) & y_{12}(x) & \cdots & y_{1n}(x) \\
        y_{21}(x) & y_{22}(x) & \cdots & y_{2n}(x) \\
        \vdots & \vdots & \ddots & \vdots \\
        y_{n1}(x) & y_{n2}(x) & \cdots & y_{nn}(x)
    \end{pmatrix}
    \]
    be their Wronskian determinant.
    Then, if~\eqref{VectorFunctionGroup} are linearly dependent on \(I\), then \(W(x) \equiv 0\) for all \(x \in I\);
\end{theorem}
Furthermore, if~\eqref{VectorFunctionGroup} are solutions to 
the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
then the following conclusion holds:
\begin{theorem}
    If ~\eqref{VectorFunctionGroup} are linearly independent solutions to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then \(W(x) \not\equiv 0\) for all \(x \in I\).
\end{theorem}
Combine the above two theorems, we have the following corollary:
\begin{corollary}{Criterion for Linear Independence}\label{corollary:criterion for linear independence}
    For vector functions~\eqref{VectorFunctionGroup},
    if their Wronskian determinant \(W(x_{0}) \neq  0\) for some \(x_{0} \in I\),
    then they are linearly independent on \(I\).
\end{corollary}


As for the relation between the solutions and the coefficient,
we have the following theorem.
\begin{theorem}{Liouville's Formula}
    Let \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) be \(n\) solutions to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    and \(W(x)\) be their Wronskian determinant.
    Then
    \[
    W(x) = W(x_{0}) \exp\left(\int_{x_{0}}^{x} \mathrm{tr}(\mathbf{A}(t)) \, dt\right),
    \]
    where \(\mathrm{tr}(\mathbf{A}(t))\) is the trace of matrix \(\mathbf{A}(t)\).
\end{theorem}

\begin{leftbarTitle}{Solution Space}\end{leftbarTitle}
With the above conclusions, we can give the existence of fundamental solution systems.
\begin{theorem}
    The fundamental solution system to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}
    does exist.
\end{theorem}

\begin{proof}
    Due to the existence and uniqueness theorem for system of first-order linear equations
    (Theorem~\ref{thm:existence and uniqueness theorem for system of first-order linear equations}),
    for initial conditions
    \begin{equation}\label{eq:initial conditions for fundamental solution system}
        \mathbf{Y}_{1}(x_{0}) = \begin{pmatrix} 1 \\ 0  \\ \vdots \\ 0 \end{pmatrix} ,
        \mathbf{Y}_{2}(x_{0}) = \begin{pmatrix} 0 \\ 1  \\ \vdots \\ 0 \end{pmatrix} ,
        \cdots,
        \mathbf{Y}_{n}(x_{0}) = \begin{pmatrix} 0 \\ 0  \\ \vdots \\ 1 \end{pmatrix} ,
        \quad x_{0} \in I,
    \end{equation}
    there exist \(n\) solutions \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
    Their Wronskian determinant at \(x = x_{0}\) is
    \[
    W(x_{0}) = \begin{vmatrix}1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{vmatrix} = 1 \neq 0.
    \]
    Therefore, by the criterion for linear independence (Corollary~\ref{corollary:criterion for linear independence}),
    \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) are linearly independent on \(I\),
    i.e., they form a fundamental solution system to
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
\end{proof}

Fundamental solution systems satisfying~\eqref{eq:initial conditions for fundamental solution system}
are called \textbf{standard fundamental systems}, 
and their fundamental solution matrices are called \textbf{standard fundamental solution matrices}.
Obviously, standard fundamental solution matrices is identity matrix at \(x = x_{0}\).

Then, general solution can be also derived.
\begin{theorem}{General Solution to Homogeneous Linear Systems}\label{theorem:general solution to homogeneous linear systems}
    If \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) is a fundamental solution system to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then the \textbf{general solution} to the system is given by:
    \[
    \mathbf{Y}(x) = C_{1} \mathbf{Y}_{1}(x) + C_{2} \mathbf{Y}_{2}(x) + \cdots + C_{n} \mathbf{Y}_{n}(x) 
    = \mathbf{\Phi}(x) \mathbf{C},
    \]
    where \(C_{1}, C_{2}, \ldots, C_{n}\) are arbitrary constants, and \(\mathbf{\Phi}(x)\) is the fundamental matrix solution.
\end{theorem}
\begin{proof}
    
\end{proof}

Therefore, the number of linearly independent solutions to
the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}
can be not exceed \(n\),
and the solution space of the system is an \(n\)-dimensional linear space.

\section{General Theory of Non-Homogeneous Linear Systems}
For the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear},
similar to linear systems of algebraic equations,
we have the following conclusion:
\begin{itemize}
    \item The difference between any two solutions to
    the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear}
    is a solution to the corresponding homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}. 
    \item If \(\tilde{\mathbf{Y}}(x)\) is a particular solution to 
    the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear}, 
    then
    \[
    \mathbf{Y}(x) = \mathbf{Y}_{0}(x) + \tilde{\mathbf{Y}}(x),
    \]
    is still a solution to the system,
    where \(\mathbf{Y}_{0}(x)\) is the general solution to the
    corresponding homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
\end{itemize}

Then we can give the general solution to
the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear}.
\begin{theorem}{General Solution to Non-Homogeneous Linear Systems}\label{theorem:general solution to non-homogeneous linear systems}
    If \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) is a fundamental solution system to 
    the corresponding homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then the general solution to the non-homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear} is given by:
    \[
    \mathbf{Y}(x) = C_{1} \mathbf{Y}_{1}(x) + C_{2} \mathbf{Y}_{2}(x) + \cdots + C_{n} \mathbf{Y}_{n}(x) + \tilde{\mathbf{Y}}(x),
    \]
    where \(\tilde{\mathbf{Y}}(x)\) is a particular solution to the non-homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear}.
\end{theorem}

For non-homogeneous linear systems, method of variation of constants can also be used to find particular solutions.
According to Theorem~\ref{theorem:general solution to homogeneous linear systems},
the general solution to the corresponding homogeneous linear system is given by:
\[
\mathbf{Y}(x) = \mathbf{\Phi}(x) \mathbf{C},
\]
where \(\mathbf{\Phi}(x)\) is the fundamental matrix solution,
and \(\mathbf{C}=\begin{pmatrix} C_{1} & C_{2} & \cdots & C_{n} \end{pmatrix}^{\mathrm{T}}\) is a constant vector.
Now find a particular solution to the non-homogeneous linear system in the form:
\[
\mathbf{Y}(x) = \mathbf{\Phi}(x) \mathbf{C}(x),
\]
where \(\mathbf{C}(x)=\begin{pmatrix} C_{1}(x) & C_{2}(x) & \cdots & C_{n}(x) \end{pmatrix}^{\mathrm{T}}\) 
is a vector function to be determined.
Substituting it into the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear},
we have:
\begin{equation}\label{eq:variation of constants - step 1}
    \mathbf{\Phi}(x) \frac{\mathrm{d}\mathbf{C}}{\mathrm{d}x} = \mathbf{F}(x).
\end{equation}
Since \(\mathbf{\Phi}(x)\) is invertible, we obtain:
\begin{equation}\label{eq:variation of constants - step 2}
    \frac{\mathrm{d}\mathbf{C}}{\mathrm{d}x} = \mathbf{\Phi}^{-1}(x) \mathbf{F}(x).
\end{equation}
Integrating both sides of Equation~\eqref{eq:variation of constants - step 2}, we have:
\begin{equation}\label{eq:variation of constants - step 3}
    \mathbf{C}(x) = \int_{x_{0}}^{x} \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \, \mathrm{d}t,
\end{equation}
where \(x_{0}\) is an arbitrary constant.
Then substituting Equation~\eqref{eq:variation of constants - step 3} into
\(\mathbf{Y}(x) = \mathbf{\Phi}(x) \mathbf{C}(x)\), 
we obtain a particular solution to the non-homogeneous linear 
system~\eqref{eq:matrix form of system of first-order linear equations - linear}:
\begin{equation}\label{eq:particular solution by variation of constants}
    \tilde{\mathbf{Y}}(x) = \mathbf{\Phi}(x) \int_{x_{0}}^{x} \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \, \mathrm{d}t.
\end{equation}

\begin{remark}
    If \(\mathbf{\Phi}(x)^{-1}\) is difficult to compute,
    we can use~\eqref{eq:variation of constants - step 1} directly to find \(\frac{\mathrm{d}\mathbf{C}}{\mathrm{d}x}\).
\end{remark}


\section{Solution to Constant Coefficient Homogeneous Linear Systems}
For autonomous linear systems with constant coefficients:
\begin{equation}\label{eq:constant coefficient linear system}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A} \mathbf{Y},
\end{equation}
we have the following conclusion:
\begin{theorem}
    Matrix exponential function \(\Phi(x)=e^{\mathbf{A}x}\) is a fundamental solution matrix to
    the homogeneous linear system~\eqref{eq:constant coefficient linear system}.
\end{theorem}
For according non-homogeneous linear systems with constant coefficients:
\begin{equation}\label{eq:constant coefficient non-homogeneous linear system}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A} \mathbf{Y} + \mathbf{F}(x),
\end{equation}
we can also use method of variation of constants to find particular solutions.
\begin{theorem}
    The general solution to the non-homogeneous linear system~\eqref{eq:constant coefficient non-homogeneous linear system} 
    is given by:
    \[
    \mathbf{Y}(x) = e^{\mathbf{A}x}\mathbf{C} + \int_{x_0}^{x} e^{\mathbf{A}(x-s)} \mathbf{F}(s) \, \mathrm{d}s,
    \]
    where $\mathbf{C}$ is a constant vector.
    The solution satisfying the initial condition \(\mathbf{Y}(x_{0}) = \mathbf{Y}_{0}\) is given by:
    \[
    \mathbf{Y}(x) = e^{\mathbf{A}(x - x_{0})} \mathbf{Y}_{0} + \int_{x_0}^{x} e^{\mathbf{A}(x-s)} \mathbf{F}(s) \, \mathrm{d}s.
    \]
\end{theorem}

\vspace{0.7cm}
% 我们面对的问题是: e^Ax 是否可以用初等函数的有限形式表示出来? 如果可以, 如何表示?
The problem we confront is: 
Can \(e^{\mathbf{A}x}\) be expressed in a finite form of elementary functions? If so, how can it be expressed?

In fact, if \(\mathbf{A}\) is an \(n\)-order Jordan block, i.e.,
\begin{align*}
    \mathbf{A} &=
\begin{pmatrix}
    \lambda & 1 & 0 & \cdots & 0 \\
    0 & \lambda & 1 & \cdots & 0 \\
    0 & 0 & \lambda & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda
\end{pmatrix}_{n \times n} = 
\begin{pmatrix} 
    \lambda & 0 & 0 & \cdots & 0 \\
    0 & \lambda & 0 & \cdots & 0 \\
    0 & 0 & \lambda & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda
\end{pmatrix} + 
\begin{pmatrix}
    0 & 1 & 0 & \cdots & 0 \\
    0 & 0 & 1 & \cdots & 0 \\
    0 & 0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & 0
\end{pmatrix} \\
&=:
\operatorname{diag}(\lambda, \lambda, \ldots, \lambda) + \mathbf{Z}_{n},
\end{align*}
then we have:
\begin{equation}\label{eq:matrix exponential of Jordan block}
\begin{aligned}
    e^{\mathbf{A}x} &= e^{\operatorname{diag}(\lambda, \lambda, \ldots, \lambda)x + \mathbf{Z}_{n}x} 
    = e^{\operatorname{diag}(\lambda, \lambda, \ldots, \lambda)x} \cdot e^{\mathbf{Z}_{n}x} \\
    &= e^{\lambda x} \cdot \left( \mathbf{E} + \frac{\mathbf{Z}_{n}x}{1!} + \frac{(\mathbf{Z}_{n}x)^{2}}{2!} + \cdots + \frac{(\mathbf{Z}_{n}x)^{n-1}}{(n-1)!} \right) \\
    &= e^{\lambda x} \cdot
    \begin{pmatrix}
        1 & x & \frac{x^{2}}{2!} & \cdots & \frac{x^{n-1}}{(n-1)!} \\
        0 & 1 & x & \cdots & \frac{x^{n-2}}{(n-2)!} \\
        0 & 0 & 1 & \cdots & \frac{x^{n-3}}{(n-3)!} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 1
    \end{pmatrix}_{n\times n}.
\end{aligned}
\end{equation}
And Jordan canonical form can be deemed as a block diagonal matrix composed of Jordan blocks.
Therefore, if we can compute the Jordan canonical form of matrix \(\mathbf{A}\),
then we can express \(e^{\mathbf{A}x}\) in a finite form of elementary functions.

According to the theory of Jordan canonical form,
for any \(n\)-order square matrix \(\mathbf{A} \in M_{n}(\mathbb{C})\),
there exists an invertible matrix \(\mathbf{P} \in M_{n}(\mathbb{C})\) such that
\[
\mathbf{P}^{-1} \mathbf{A} \mathbf{P} = \mathbf{J},
\]
where \(\mathbf{J}\) is the Jordan canonical form of \(\mathbf{A}\),
\[
\mathbf{J} = \operatorname{diag}(\mathbf{J}_{1}, \mathbf{J}_{2}, \ldots, \mathbf{J}_{k}),
\]
and \(\mathbf{J}_{i}\) is a Jordan block corresponding to eigenvalue \(\lambda_{i}\) of \(\mathbf{A}\).
Then we have:
\begin{equation}\label{eq:matrix exponential by Jordan form}
    e^{\mathbf{A}x} = e^{\mathbf{P} \mathbf{J} \mathbf{P}^{-1} x} = \mathbf{P} e^{\mathbf{J} x} \mathbf{P}^{-1}
    = \mathbf{P} \operatorname{diag} \left( e^{\mathbf{J}_{1} x}, e^{\mathbf{J}_{2} x}, \ldots, 
    e^{\mathbf{J}_{k} x} \right) \mathbf{P}^{-1}.
\end{equation}

However, computing the Jordan canonical form and transition matrix is not always easy.
Note that \(e^{\mathbf{A}x}\) is a fundamental solution matrix to
the homogeneous linear system~\eqref{eq:constant coefficient linear system},
since \(\mathbf{P}\) is invertible,
\(e^{\mathbf{A}x}\mathbf{P}\) 
is also a fundamental solution matrix to~\eqref{eq:constant coefficient linear system}.
Then according to \eqref{eq:matrix exponential by Jordan form},
\(\mathbf{P}e^{\mathbf{J}x}\) is also a fundamental solution matrix to~\eqref{eq:constant coefficient linear system}.

By~\eqref{eq:matrix exponential of Jordan block},
we can utilize method of undetermined coefficients to find
\(n\) linearly independent solution matrix to~\eqref{eq:constant coefficient linear system}.
% 下面按 A 是否有重特征根分类讨论
In the following, we classify the discussion based on whether \(\mathbf{A}\) has repeated eigenvalues.

% A 无重特征根
\begin{leftbarTitle}{Distinct Eigenvalues}\end{leftbarTitle}
\begin{theorem}
    Let \(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\) be \(n\) distinct eigenvalues of matrix \(\mathbf{A}\),
    then the fundamental solution matrix to 
    the homogeneous linear system~\eqref{eq:constant coefficient linear system} is given by:
    \[
    \Phi(x) =
    \begin{pmatrix}
        e^{\lambda_{1} x} \boldsymbol{\xi}_{1} & e^{\lambda_{2} x} \boldsymbol{\xi}_{2} & \cdots & e^{\lambda_{n} x} \boldsymbol{\xi}_{n}
    \end{pmatrix},
    \]
    where \(\boldsymbol{\xi}_{i}\) is the eigenvector corresponding to eigenvalue \(\lambda_{i}\) of 
    matrix \(\mathbf{A}\) (\(i = 1, 2, \ldots, n\)).
\end{theorem}
% 复特征值
If there is a complex eigenvalue \(\lambda=\alpha+ \beta i\) (\(\beta \neq 0\)) of matrix \(\mathbf{A}\),
then its complex conjugate \(\bar{\lambda}=\alpha - \beta i\) is also an eigenvalue of \(\mathbf{A}\).
And their corresponding eigenvectors are also complex conjugates of each other, i.e. \(\boldsymbol{\xi}, \bar{\boldsymbol{\xi}}\).
At this time, the pair of conjugate eigenvalues and eigenvectors will produce a complex solution:
\begin{align*}
    \mathbf{Y} &= e^{\lambda x} \boldsymbol{\xi} = e^{\alpha x} \left[ \cos(\beta x) + i \sin(\beta x) \right] (\boldsymbol{\xi}_{1} + i \boldsymbol{\xi}_{2})  \\
    & = e^{\alpha x} \left[ \left( \cos(\beta x) \boldsymbol{\xi}_{1} - \sin(\beta x) \boldsymbol{\xi}_{2} \right) 
    + i \left( \sin(\beta x) \boldsymbol{\xi}_{1} + \cos(\beta x) \boldsymbol{\xi}_{2} \right) \right].
\end{align*}
Since the equation~\eqref{eq:constant coefficient linear system} is linear, 
the real and imaginary parts of this complex solution are each real solutions to the original equation. 
Thus, we obtain two linearly independent real solutions:
\begin{gather*}
    \mathbf{Y}_{1} = e^{\alpha x} \left( \cos(\beta x) \boldsymbol{\xi}_{1} - \sin(\beta x) \boldsymbol{\xi}_{2} \right), \\
    \mathbf{Y}_{2} = e^{\alpha x} \left( \sin(\beta x) \boldsymbol{\xi}_{1} + \cos(\beta x) \boldsymbol{\xi}_{2} \right).
\end{gather*}

% A 有重特征根
\begin{leftbarTitle}{Repeated Eigenvalues}\end{leftbarTitle}
Assume that \(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{s}\) are all distinct eigenvalues of matrix \(\mathbf{A}\),
with algebraic multiplicities \(n_{1}, n_{2}, \ldots, n_{s}\) respectively,
where \(n_{1} + n_{2} + \cdots + n_{s} = n\).
Note that the fundamental solution matrix \(e^{\mathbf{A}x} \mathbf{P}= \mathbf{P}e^{\mathbf{J}x}\), 
hence in the expression of \(\mathbf{P}e^{\mathbf{J}x}\),
all column vectors corresponding to the eigenvalue \(\lambda_{j}\) have the form:
\begin{equation}\label{eq:solution form for repeated eigenvalue}
    \mathbf{Y} = e^{\lambda_{j} x} 
    \left[ \boldsymbol{\xi}_{0} + \frac{x}{1!}\boldsymbol{\xi}_{1} + \frac{x^2}{2!}\boldsymbol{\xi}_{2} + \cdots + \frac{x^{n_j - 1}}{(n_j - 1)!}\boldsymbol{\xi}_{n_j - 1} \right],
\end{equation}
where \(\xi_{0}, \xi_{1}, \ldots, \xi_{n_j - 1}\) are constant vectors.


\begin{lemma}
    Let \(\lambda_{j}\) be an eigenvalue of matrix \(\mathbf{A}\) with algebraic multiplicity \(n_{j}\),
    then~\eqref{eq:constant coefficient linear system} has non-zero solutions of 
    the form~\eqref{eq:solution form for repeated eigenvalue}
    if and only if
    \(\boldsymbol{\xi}_{0}\) is a non-zero solution to
    \begin{equation}\label{eq:generalized eigenvector equation}
        (\mathbf{A} - \lambda_{j} \mathbf{E})^{n_{j}} \boldsymbol{\xi} = 0,
    \end{equation}
    and \(\boldsymbol{\xi}_{1}, \boldsymbol{\xi}_{2}, \ldots, \boldsymbol{\xi}_{n_j - 1}\) 
    satisfy the following chain of generalized eigenvector equations\footnote{
        In fact, \(\boldsymbol{\xi}_{n_{j}-1}\) is the true eigenvector corresponding to eigenvalue \(\lambda_{j}\),
        and \(\boldsymbol{\xi}_{0}, \boldsymbol{\xi}_{1}, \ldots, \boldsymbol{\xi}_{n_{j}-2}\) 
        are generalized eigenvectors of orders \(n_{j}, n_{j}-1, \ldots, 2\) respectively.
    }:
    \begin{equation}\label{eq:generalized eigenvector chain}    
        \begin{cases}
            \boldsymbol{\xi}_{1} = (\mathbf{A} - \lambda_{j} \mathbf{E}) \boldsymbol{\xi}_{0}, \\
            \boldsymbol{\xi}_{2} = (\mathbf{A} - \lambda_{j} \mathbf{E}) \boldsymbol{\xi}_{1}, \\
            \vdots \\
            \boldsymbol{\xi}_{n_j - 1} = (\mathbf{A} - \lambda_{j} \mathbf{E}) \boldsymbol{\xi}_{n_{j}-2}.
        \end{cases}
    \end{equation}
\end{lemma}

\begin{lemma}
    Under the same conditions as above,
    denote the linear space of all constant vectors of \(n\) degrees as \(V\),
    then
    \begin{enumerate}
        \item The subspace of \(V\) 
            \[
            V_{j} = \{ \xi \in V \mid (\mathbf{A} - \lambda_{j} \mathbf{E})^{n_{j}} \xi = 0 \},\quad j=1, 2, \ldots, s
            \]
            is invariant under \(\mathbf{A}\).
        \item There exists a direct sum decomposition of \(V\):
            \[
            V = V_{1} \oplus V_{2} \oplus \cdots \oplus V_{s}.
            \]
    \end{enumerate}
\end{lemma}

\begin{theorem}
    Under the same conditions as above,
    the fundamental solution matrix to 
    the homogeneous linear system~\eqref{eq:constant coefficient linear system} is given by:
    \[
    \Phi(x) =
    \begin{pmatrix}
        e^{\lambda_{1}x} \mathbf{P}_{1}^{(1)}(x) & 
        e^{\lambda_{2}x} \mathbf{P}_{2}^{(2)}(x) & 
        \cdots & e^{\lambda_{s}x} \mathbf{P}_{s}^{(s)}(x)
    \end{pmatrix},
    \]
    where for each \(j = 1, 2, \ldots, s\),
    \[
    \mathbf{P}_{j}^{(j)}(x) =
        \boldsymbol{\xi}_{j0}^{(i)} + \frac{x}{1!} \boldsymbol{\xi}_{j1}^{(i)} + \frac{x^{2}}{2!} \boldsymbol{\xi}_{j2}^{(i)} + 
        \cdots + \frac{x^{n_j - 1}}{(n_j - 1)!} \boldsymbol{\xi}_{j, n_j - 1}^{(i)},
    \]
    which is the \(j\)-th vector polynomial corresponding to eigenvalue \(\lambda_{i}\) 
    (\(i = 1, 2, \ldots, n_{j};j = 1, 2, \ldots, n_{i}\)).
    \(\boldsymbol{\xi}_{10}^{(i)}, \cdots, \boldsymbol{\xi}_{n_{i}0}^{(i)}\) is \(n_{i}\) linearly independent solutions to
    ~\eqref{eq:generalized eigenvector equation},
    and the other \(\boldsymbol{\xi}_{jl}^{(i)}\) (\(j = 1, 2, \ldots, n_{i}; l = 1, 2, \ldots, n_{j} - 1\))
    is obtained by replacing corresponding \(\boldsymbol{\xi}_{j0}^{(i)}\) with \(\boldsymbol{\xi}_{0}\) in~\eqref{eq:generalized eigenvector chain}.
\end{theorem}



\vspace{0.7cm}
Let us summarize the method from an overall perspective.

When there exist repeated eigenvalues, let \(k\) be the algebraic multiplicity of eigenvalue \(\lambda\).
The core issue is: 
\newline \underline{can we find \(k\) linearly independent eigenvectors 
for this eigenvalue \(\lambda\) that has been repeated \(k\) times?}\footnote{
    As a matter of fact, the method above has already contained the following two cases. 
    But for clarity, we explicitly state them here.
}
\begin{itemize}
    \item If we can, that is \(\text{AM} = \text{GM}\)\footnote{
        \(\text{AM}\) refers to algebraic multiplicity,
        \(\text{GM}\) refers to geometric multiplicity.
    }, and we say the corresponding eigenvalue is complete;
    \item If not, that is \(\text{AM} > \text{GM}\), and we say the corresponding eigenvalue is defective.
        We can only find \(\text{GM}<\text{AM}=k\) linearly independent eigenvectors,
        yet there are still \(k - \text{GM}\) solutions missing.
        In fact, these missing solutions are just the generalized eigenvectors.
\end{itemize}
Now we discuss the two cases separately.
\begin{description}
    \item[Complete] 
        If eigenvalue \(\lambda\) is complete,
        then we can find \(k\) linearly independent eigenvectors 
        \(\boldsymbol{\xi}_{1}, \boldsymbol{\xi}_{2}, \ldots, \boldsymbol{\xi}_{k}\)
        corresponding to eigenvalue \(\lambda\).
        Thus, we can directly write out \(k\) linearly independent solutions:
        \[
        \mathbf{Y}_{1} = e^{\lambda x} \boldsymbol{\xi}_{1}, \quad
        \mathbf{Y}_{2} = e^{\lambda x} \boldsymbol{\xi}_{2}, \quad
        \ldots, \quad
        \mathbf{Y}_{k} = e^{\lambda x} \boldsymbol{\xi}_{k}.
        \]
    \item[Defective] 
        When we cannot find enough eigenvectors, 
        we need to look for so-called generalized eigenvectors to supplement the missing solutions.
        Assume that we have found only \(m < k\) linearly independent eigenvectors
        \(\boldsymbol{\xi}_{1}, \boldsymbol{\xi}_{2}, \ldots, \boldsymbol{\xi}_{m}\),
        i.e., \(\text{GM} = m < k = \text{AM}\).
        Then for each eigenvector \(\boldsymbol{\xi}_{i}\) (\(i = 1, 2, \ldots, m\)),
        we can find a chain of generalized eigenvectors:
        \[
        \boldsymbol{\xi}_{i}, \boldsymbol{\xi}_{i}^{(1)}, \boldsymbol{\xi}_{i}^{(2)}, \ldots, \boldsymbol{\xi}_{i}^{(t_{i})},
        \]
        where \(t_{i}\) is the length of the chain corresponding to eigenvector \(\boldsymbol{\xi}_{i}\),
        and these generalized eigenvectors satisfy:
        \[
        \begin{cases}
            (\mathbf{A} - \lambda \mathbf{E}) \boldsymbol{\xi}_{i}^{(1)} = \boldsymbol{\xi}_{i}, \\
            (\mathbf{A} - \lambda \mathbf{E}) \boldsymbol{\xi}_{i}^{(2)} = \boldsymbol{\xi}_{i}^{(1)}, \\
            \vdots \\
            (\mathbf{A} - \lambda \mathbf{E}) \boldsymbol{\xi}_{i}^{(t_{i})} = \boldsymbol{\xi}_{i}^{(t_{i}-1)},
        \end{cases}
        \]
        they will contribute \(t_{i} + 1\) linearly independent solutions\footnote{
            Here, \(\boldsymbol{\xi}_{i}\) is the true eigenvector,
            and \(\boldsymbol{\xi}_{i}^{(1)}, \boldsymbol{\xi}_{i}^{(2)}, \ldots, \boldsymbol{\xi}_{i}^{(t_{i})}\)
            are generalized eigenvectors of orders \(2, 3, \ldots, t_{i} + 1\) respectively.
            % 不同于之前的编号方式 (自顶向下), 这里采用自底向上的编号方式, 方便理解.
            Different from the previous numbering method (\underline{top-down}), 
            here we use a \underline{bottom-up} numbering method for better understanding.
        }.
        Note that the total number of vectors in all chains is:
        \[
        t_{1} + t_{2} + \cdots + t_{m} + m = k.
        \]
        Then we can write out \(k\) linearly independent solutions:
        \[
        \begin{aligned}
            & \mathbf{Y}_{i} = e^{\lambda x} \boldsymbol{\xi}_{i}, \\
            & \mathbf{Y}_{i}^{(1)} = e^{\lambda x} \left( x \boldsymbol{\xi}_{i} + \boldsymbol{\xi}_{i}^{(1)} \right), \\
            & \mathbf{Y}_{i}^{(2)} = e^{\lambda x} \left( \frac{x^{2}}{2!} \boldsymbol{\xi}_{i} + x \boldsymbol{\xi}_{i}^{(1)} + \boldsymbol{\xi}_{i}^{(2)} \right), \\
            & \vdots \\
            & \mathbf{Y}_{i}^{(t_{i})} = e^{\lambda x} \left( \frac{x^{t_{i}}}{t_{i}!} \boldsymbol{\xi}_{i} + \frac{x^{t_{i}-1}}{(t_{i}-1)!} \boldsymbol{\xi}_{i}^{(1)} + 
            \cdots + x \boldsymbol{\xi}_{i}^{(t_{i}-1)} + \boldsymbol{\xi}_{i}^{(t_{i})} \right),
        \end{aligned}
        \]
        for each \(i = 1, 2, \ldots, m\).
\end{description}





\section{Periodic Coefficient Linear Differential Equation Systems} % 周期系数线性微分方程组

\chapter{System of Higher-Order Linear Equations}
This chapter mainly discusses the theory and solution methods of higher-order linear differential equations,
with the following general forms:
\begin{equation}\label{eq:higher-order linear equation - non-homogeneous}
    \frac{\mathrm{d}^{n}y}{\mathrm{d}x^{n}} + p_{1}(x) \frac{\mathrm{d}^{n-1}y}{\mathrm{d}x^{n-1}} 
    + \cdots + p_{n-1}(x) \frac{\mathrm{d}y}{\mathrm{d}x} + p_{n}(x) y = f(x).
\end{equation}
When \(f(x) \equiv 0\), it reduces to the homogeneous case:
\begin{equation}\label{eq:higher-order linear equation - homogeneous}
    \frac{\mathrm{d}^{n}y}{\mathrm{d}x^{n}} + p_{1}(x) \frac{\mathrm{d}^{n-1}y}{\mathrm{d}x^{n-1}} 
    + \cdots + p_{n-1}(x) \frac{\mathrm{d}y}{\mathrm{d}x} + p_{n}(x) y = 0,
\end{equation}


\section{General Theory of Higher-Order Linear Equations}


\section{Solution to Constant Coefficient Homogeneous Linear Equations}

\section{Solution to Constant Coefficient Non-Homogeneous Linear Equations}

\chapter{Boundary Value Problems} % 边值问题
\section{Sturm-Liouville Problems} % 斯特姆-刘维尔问题

\begin{thebibliography}{99} 
\bibitem{1} 柳彬. \emph{常微分方程 (1th edition) }. 北京大学出版社, 2021.
\bibitem{2} 东北师范大学微分方程教研室. \emph{常微分方程 (3th edition) }. 高等教育出版社, 2022.
\bibitem{3} 王高雄, 周之铭, 朱思铭, 王寿松. \emph{常微分方程 (4th edition) }. 高等教育出版社, 2020.
\bibitem{9} Wikipedia. \url{https://en.wikipedia.org/wiki/}.

\end{thebibliography}

\end{document}
