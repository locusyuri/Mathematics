\chapter{System of First-Order Linear Equations} % 一阶线性方程组
\section{System of First-Order Linear Equations}
\begin{leftbarTitle}{Common Forms}\end{leftbarTitle}
System of first-order equations with \(n\) variables is of the form:
\begin{equation}\label{eq:system of first-order linear equations}
    \begin{cases}
        \frac{\mathrm{d}y_1}{\mathrm{d}x} = f_1(x, y_1, y_2, \ldots, y_n), \\
        \frac{\mathrm{d}y_2}{\mathrm{d}x} = f_2(x, y_1, y_2, \ldots, y_n), \\
        \quad\vdots \\
        \frac{\mathrm{d}y_n}{\mathrm{d}x} = f_n(x, y_1, y_2, \ldots, y_n).
    \end{cases}
\end{equation}
If the right-hand side of each equation in system~\eqref{eq:system of first-order linear equations}
does not include explicitly \(x\), then the system is called \textbf{autonomous}.

The solution to system~\eqref{eq:system of first-order linear equations} is an \(n\)-tuple of functions
\[y_1 = \varphi_1(x), y_2 = \varphi_2(x), \ldots, y_n = \varphi_n(x),\]
which satisfy all equations in system~\eqref{eq:system of first-order linear equations} simultaneously.

Solution containing arbitrary constants \(C_{1}, C_{2}, \ldots, C_{n}\)
\[
\begin{cases} 
    y_{1}=\varphi_{1}(x, C_{1}, C_{2}, \ldots, C_{n}),\\ 
    y_{2}=\varphi_{2}(x, C_{1}, C_{2}, \ldots, C_{n}), \\ 
    \quad\vdots \\ 
    y_{n}=\varphi_{n}(x, C_{1}, C_{2}, \ldots, C_{n}) 
\end{cases}
\]
is called the \textbf{general solution} of system~\eqref{eq:system of first-order linear equations}.
If general solution satisfies 
\[
\begin{cases} 
    \Phi_{1}(x, y_{1}, \cdots, y_{n}, C_{1}, \cdots, C_{n})=0, \\ 
    \Phi_{2}(x, y_{1}, \cdots, y_{n}, C_{1}, \cdots, C_{n})=0, \\ 
    \quad\vdots \\ 
    \Phi_{n}(x, y_{1}, \cdots, y_{n}, C_{1}, \cdots, C_{n})=0,
\end{cases}
\]
then it is called the \textbf{general integral} of system~\eqref{eq:system of first-order linear equations}.

\vspace{0.7cm}
For convenience, we rewrite system~\eqref{eq:system of first-order linear equations} in matrix form:
\begin{equation}\label{eq:matrix form of system of first-order linear equations}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{F}(x, \mathbf{Y}),
\end{equation}
and autonomous system as:
\begin{equation}\label{eq:matrix form of autonomous system of first-order linear equations}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{F}(\mathbf{Y}),
\end{equation}
and Cauchy problem for system~\eqref{eq:matrix form of system of first-order linear equations} as:
\begin{equation}\label{eq:Cauchy problem for system of first-order linear equations}
\begin{cases}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{F}(x, \mathbf{Y}), \\
    \mathbf{Y}(x_{0}) = \mathbf{Y}_{0},
\end{cases}
\end{equation}
where 
\[
\mathbf{Y} = \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{bmatrix}, \quad
\mathbf{F}(x, \mathbf{Y}) = 
\begin{bmatrix} f_{1}(x, y_{1}, y_{2}, \ldots, y_{n}) \\ f_{2}(x, y_{1}, y_{2}, \ldots, y_{n}) \\ 
    \vdots \\ f_{n}(x, y_{1}, y_{2}, \ldots, y_{n}) 
\end{bmatrix},
\quad
\frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = 
\begin{bmatrix} 
    \frac{\mathrm{d}y_{1}}{\mathrm{d}x} \\ \frac{\mathrm{d}y_{2}}{\mathrm{d}x} \\ \vdots \\ \frac{\mathrm{d}y_{n}}{\mathrm{d}x} 
\end{bmatrix}.
\]
With these notations, from the perspective of form, 
the system of first-order linear equations is similar to first-order equations.

\vspace{0.7cm}
In system~\eqref{eq:system of first-order linear equations},
if \(f_{i}(x, y_{1}, y_{2}, \ldots, y_{n})\) is a linear function of \(y_{1}, y_{2}, \ldots, y_{n}\), 
i.e., it can be rewritten as:
\[
\begin{cases} 
    \frac{\mathrm{d}y_{1}}{\mathrm{d}x} = a_{11}(x)y_{1} + a_{12}(x)y_{2} + \cdots + a_{1n}(x)y_{n} + f_{1}(x), \\ 
    \frac{\mathrm{d}y_{2}}{\mathrm{d}x} = a_{21}(x)y_{1} + a_{22}(x)y_{2} + \cdots + a_{2n}(x)y_{n} + f_{2}(x), \\ 
    \quad\vdots \\ 
    \frac{\mathrm{d}y_{n}}{\mathrm{d}x} = a_{n1}(x)y_{1} + a_{n2}(x)y_{2} + \cdots + a_{nn}(x)y_{n} + f_{n}(x).
\end{cases}
\]
It is called a \textbf{system of first-order linear equations}.
\(a_{ij}(x)\) and \(f_{i}(x)\) are always assumed to be continuous on some interval \(I \subset \mathbb{R}\), 
where \(i, j = 1, 2, \ldots, n\).

For convenience, we rewrite the system of first-order linear equations in matrix form:
\begin{equation}\label{eq:matrix form of system of first-order linear equations - linear}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A}(x) \mathbf{Y} + \mathbf{F}(x),
\end{equation}
where
\[
\mathbf{A}(x) = 
\begin{bmatrix} 
    a_{11}(x) & a_{12}(x) & \cdots & a_{1n}(x) \\ 
    a_{21}(x) & a_{22}(x) & \cdots & a_{2n}(x) \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    a_{n1}(x) & a_{n2}(x) & \cdots & a_{nn}(x)
\end{bmatrix}, \quad
\mathbf{F}(x) =
\begin{bmatrix}
    f_{1}(x) \\ f_{2}(x) \\ \vdots \\ f_{n}(x)
\end{bmatrix}.
\]
On the interval \(I\), if \(\mathbf{F}(x) \equiv \mathbf{0}\), that is,
\begin{equation}\label{eq:matrix form of system of first-order linear equations - linear and homogeneous}
\frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A}(x) \mathbf{Y}
\end{equation}
then system~\eqref{eq:matrix form of system of first-order linear equations - linear} is called a \textbf{homogeneous system},
otherwise, it is called a \textbf{non-homogeneous system}.

\begin{leftbarTitle}{General Theory}\end{leftbarTitle}

\begin{theorem}{Existence and Uniqueness Theorem for System of First-Order Linear Equations}
    \label{thm:existence and uniqueness theorem for system of first-order linear equations}
    In the Cauchy problem~\eqref{eq:Cauchy problem for system of first-order linear equations},
    let \(D\) be a closed region in the \(\mathbb{R}^{n+1}\):
    \[
    D = [x_{0}-a, x_{0}+a] \times [\mathbf{Y}_{0}-b, \mathbf{Y}_{0}+b].
    \]
    If the function \(\mathbf{F}(x, \mathbf{Y})\) satisfies the following two conditions:
    \begin{enumerate}
        \item \(\mathbf{F}(x, \mathbf{Y})\) is continuous in \(D\).
        \item \(\mathbf{F}(x, \mathbf{Y})\) satisfies the Lipschitz condition with respect to \(\mathbf{Y}\) in \(D\), 
            i.e., there exists a constant \(L > 0\) such that for any \((x, \mathbf{Y}_{1}), (x, \mathbf{Y}_{2}) \in D\),
            \[
            \|\mathbf{F}(x, \mathbf{Y}_{1}) - \mathbf{F}(x, \mathbf{Y}_{2})\| \leqslant  L \|\mathbf{Y}_{1} - \mathbf{Y}_{2}\|.
            \]
    \end{enumerate}
    Then there exists a unique solution \(\mathbf{Y} = \mathbf{\Phi}(x)\) (\(\mathbf{\Phi}(x_0) = \mathbf{Y}_0\))
    to the Cauchy problem~\eqref{eq:Cauchy problem for system of first-order linear equations}
    in the interval \([x_{0}-h, x_{0}+h]\), where
    \[
    h = \min\{a, \frac{b}{M}\}, \quad M = \max_{(x, \mathbf{Y}) \in D} \|\mathbf{F}(x, \mathbf{Y})\|.
    \]
\end{theorem}

\section{General Theory of Homogeneous Linear Systems}
Similar to linear homogeneous systems of algebraic equations,
the linear combination of solutions to homogeneous linear systems of differential equations
is still a solution to the system.
\begin{proposition}
    If \(\mathbf{Y}_{1}(x)\) and \(\mathbf{Y}_{2}(x)\) are two solutions to the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then any linear combination of them
    \[
    \mathbf{Y}(x) = C_{1} \mathbf{Y}_{1}(x) + C_{2} \mathbf{Y}_{2}(x),
    \]
    where \(C_{1}\) and \(C_{2}\) are arbitrary constants, is also a solution to the system.

    Three or more solutions also have this property.
\end{proposition}
With this proposition, it is easy to verify that the set of all solutions to 
the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}
forms a linear space. And similarly, linear independence of solutions can be defined.
Then we can introduce the concept of fundamental solution matrix.

\begin{definition}{Fundamental Solution Matrix}
    Let \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) be \(n\) linearly independent solutions to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
    Then the matrix
    \[
    \mathbf{\Phi}(x) = 
    \begin{pmatrix}
        \mathbf{Y}_{1}(x) & \mathbf{Y}_{2}(x) & \cdots & \mathbf{Y}_{n}(x) \\
    \end{pmatrix}
    = 
    \begin{pmatrix}
        y_{11}(x) & y_{12}(x) & \cdots & y_{1n}(x) \\
        y_{21}(x) & y_{22}(x) & \cdots & y_{2n}(x) \\
        \vdots & \vdots & \ddots & \vdots \\
        y_{n1}(x) & y_{n2}(x) & \cdots & y_{nn}(x)
    \end{pmatrix}
    \]
    is called a \textbf{fundamental solution matrix} of the system.

    Simultaneously, such a set of solutions is called a \textbf{fundamental solution system}.
\end{definition}

\begin{leftbarTitle}{Criteria for Linear Dependence}\end{leftbarTitle}
Given \(n\) vector functions with \(n\) components each:

\begin{equation}\label{VectorFunctionGroup}
    \mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x),
\end{equation}
criteria for their linear independence on the definition interval \(I\) is provided by the following theorem.
\begin{theorem}{Wronskian Determinant Theorem}
    For vector functions~\eqref{VectorFunctionGroup},
    let
    \[
    W(x) = \det
    \begin{pmatrix}
        y_{11}(x) & y_{12}(x) & \cdots & y_{1n}(x) \\
        y_{21}(x) & y_{22}(x) & \cdots & y_{2n}(x) \\
        \vdots & \vdots & \ddots & \vdots \\
        y_{n1}(x) & y_{n2}(x) & \cdots & y_{nn}(x)
    \end{pmatrix}
    \]
    be their Wronskian determinant.
    Then, if~\eqref{VectorFunctionGroup} are linearly dependent on \(I\), then \(W(x) \equiv 0\) for all \(x \in I\);
\end{theorem}
Furthermore, if~\eqref{VectorFunctionGroup} are solutions to 
the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
then the following conclusion holds:
\begin{theorem}
    If ~\eqref{VectorFunctionGroup} are linearly independent solutions to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then \(W(x) \not\equiv 0\) for all \(x \in I\).
\end{theorem}
Combine the above two theorems, we have the following corollary:
\begin{corollary}{Criterion for Linear Independence}\label{corollary:criterion for linear independence}
    For vector functions~\eqref{VectorFunctionGroup},
    if their Wronskian determinant \(W(x_{0}) \neq  0\) for some \(x_{0} \in I\),
    then they are linearly independent on \(I\).
\end{corollary}


As for the relation between the solutions and the coefficient,
we have the following theorem.
\begin{theorem}{Liouville's Formula}
    Let \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) be \(n\) solutions to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    and \(W(x)\) be their Wronskian determinant.
    Then
    \[
    W(x) = W(x_{0}) \exp\left(\int_{x_{0}}^{x} \mathrm{tr}(\mathbf{A}(t)) \, dt\right),
    \]
    where \(\mathrm{tr}(\mathbf{A}(t))\) is the trace of matrix \(\mathbf{A}(t)\).
\end{theorem}

\begin{leftbarTitle}{Solution Space}\end{leftbarTitle}
With the above conclusions, we can give the existence of fundamental solution systems.
\begin{theorem}
    The fundamental solution system to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}
    does exist.
\end{theorem}

\begin{proof}
    Due to the existence and uniqueness theorem for system of first-order linear equations
    (Theorem~\ref{thm:existence and uniqueness theorem for system of first-order linear equations}),
    for initial conditions
    \begin{equation}\label{eq:initial conditions for fundamental solution system}
        \mathbf{Y}_{1}(x_{0}) = \begin{pmatrix} 1 \\ 0  \\ \vdots \\ 0 \end{pmatrix} ,
        \mathbf{Y}_{2}(x_{0}) = \begin{pmatrix} 0 \\ 1  \\ \vdots \\ 0 \end{pmatrix} ,
        \cdots,
        \mathbf{Y}_{n}(x_{0}) = \begin{pmatrix} 0 \\ 0  \\ \vdots \\ 1 \end{pmatrix} ,
        \quad x_{0} \in I,
    \end{equation}
    there exist \(n\) solutions \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
    Their Wronskian determinant at \(x = x_{0}\) is
    \[
    W(x_{0}) = \begin{vmatrix}1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{vmatrix} = 1 \neq 0.
    \]
    Therefore, by the criterion for linear independence (Corollary~\ref{corollary:criterion for linear independence}),
    \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) are linearly independent on \(I\),
    i.e., they form a fundamental solution system to
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
\end{proof}

Fundamental solution systems satisfying~\eqref{eq:initial conditions for fundamental solution system}
are called \textbf{standard fundamental systems}, 
and their fundamental solution matrices are called \textbf{standard fundamental solution matrices}.
Obviously, standard fundamental solution matrices is identity matrix at \(x = x_{0}\).

Then, general solution can be also derived.
\begin{theorem}{General Solution to Homogeneous Linear Systems}\label{theorem:general solution to homogeneous linear systems}
    If \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) is a fundamental solution system to 
    the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then the \textbf{general solution} to the system is given by:
    \[
    \mathbf{Y}(x) = C_{1} \mathbf{Y}_{1}(x) + C_{2} \mathbf{Y}_{2}(x) + \cdots + C_{n} \mathbf{Y}_{n}(x) 
    = \mathbf{\Phi}(x) \mathbf{C},
    \]
    where \(C_{1}, C_{2}, \ldots, C_{n}\) are arbitrary constants, and \(\mathbf{\Phi}(x)\) is the fundamental matrix solution.
\end{theorem}
\begin{proof}
    
\end{proof}

Therefore, the number of linearly independent solutions to
the homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}
can be not exceed \(n\),
and the solution space of the system is an \(n\)-dimensional linear space.

\section{General Theory of Non-Homogeneous Linear Systems}
For the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear},
similar to linear systems of algebraic equations,
we have the following conclusion:
\begin{itemize}
    \item The difference between any two solutions to
    the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear}
    is a solution to the corresponding homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}. 
    \item If \(\tilde{\mathbf{Y}}(x)\) is a particular solution to 
    the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear}, 
    then
    \[
    \mathbf{Y}(x) = \mathbf{Y}_{0}(x) + \tilde{\mathbf{Y}}(x),
    \]
    is still a solution to the system,
    where \(\mathbf{Y}_{0}(x)\) is the general solution to the
    corresponding homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous}.
\end{itemize}

Then we can give the general solution to
the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear}.
\begin{theorem}{General Solution to Non-Homogeneous Linear Systems}\label{theorem:general solution to non-homogeneous linear systems}
    If \(\mathbf{Y}_{1}(x), \mathbf{Y}_{2}(x), \ldots, \mathbf{Y}_{n}(x)\) is a fundamental solution system to 
    the corresponding homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear and homogeneous},
    then the general solution to the non-homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear} is given by:
    \[
    \mathbf{Y}(x) = C_{1} \mathbf{Y}_{1}(x) + C_{2} \mathbf{Y}_{2}(x) + \cdots + C_{n} \mathbf{Y}_{n}(x) + \tilde{\mathbf{Y}}(x),
    \]
    where \(\tilde{\mathbf{Y}}(x)\) is a particular solution to the non-homogeneous linear 
    system~\eqref{eq:matrix form of system of first-order linear equations - linear}.
\end{theorem}

For non-homogeneous linear systems, method of variation of constants can also be used to find particular solutions.
According to Theorem~\ref{theorem:general solution to homogeneous linear systems},
the general solution to the corresponding homogeneous linear system is given by:
\[
\mathbf{Y}(x) = \mathbf{\Phi}(x) \mathbf{C},
\]
where \(\mathbf{\Phi}(x)\) is the fundamental matrix solution,
and \(\mathbf{C}=\begin{pmatrix} C_{1} & C_{2} & \cdots & C_{n} \end{pmatrix}^{\mathrm{T}}\) is a constant vector.
Now find a particular solution to the non-homogeneous linear system in the form:
\[
\mathbf{Y}(x) = \mathbf{\Phi}(x) \mathbf{C}(x),
\]
where \(\mathbf{C}(x)=\begin{pmatrix} C_{1}(x) & C_{2}(x) & \cdots & C_{n}(x) \end{pmatrix}^{\mathrm{T}}\) 
is a vector function to be determined.
Substituting it into the non-homogeneous linear system~\eqref{eq:matrix form of system of first-order linear equations - linear},
we have:
\begin{equation}\label{eq:variation of constants - step 1}
    \mathbf{\Phi}(x) \frac{\mathrm{d}\mathbf{C}}{\mathrm{d}x} = \mathbf{F}(x).
\end{equation}
Since \(\mathbf{\Phi}(x)\) is invertible, we obtain:
\begin{equation}\label{eq:variation of constants - step 2}
    \frac{\mathrm{d}\mathbf{C}}{\mathrm{d}x} = \mathbf{\Phi}^{-1}(x) \mathbf{F}(x).
\end{equation}
Integrating both sides of Equation~\eqref{eq:variation of constants - step 2}, we have:
\begin{equation}\label{eq:variation of constants - step 3}
    \mathbf{C}(x) = \int_{x_{0}}^{x} \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \, \mathrm{d}t,
\end{equation}
where \(x_{0}\) is an arbitrary constant.
Then substituting Equation~\eqref{eq:variation of constants - step 3} into
\(\mathbf{Y}(x) = \mathbf{\Phi}(x) \mathbf{C}(x)\), 
we obtain a particular solution to the non-homogeneous linear 
system~\eqref{eq:matrix form of system of first-order linear equations - linear}:
\begin{equation}\label{eq:particular solution by variation of constants}
    \tilde{\mathbf{Y}}(x) = \mathbf{\Phi}(x) \int_{x_{0}}^{x} \mathbf{\Phi}^{-1}(t) \mathbf{F}(t) \, \mathrm{d}t.
\end{equation}

\begin{remark}
    If \(\mathbf{\Phi}(x)^{-1}\) is difficult to compute,
    we can use~\eqref{eq:variation of constants - step 1} directly to find \(\frac{\mathrm{d}\mathbf{C}}{\mathrm{d}x}\).
\end{remark}


\section{Solution to Constant Coefficient Homogeneous Linear Systems}
For autonomous linear systems with constant coefficients:
\begin{equation}\label{eq:constant coefficient linear system}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A} \mathbf{Y},
\end{equation}
we have the following conclusion:
\begin{theorem}
    Matrix exponential function \(\Phi(x)=e^{\mathbf{A}x}\) is a fundamental solution matrix to
    the homogeneous linear system~\eqref{eq:constant coefficient linear system}.
\end{theorem}
For according non-homogeneous linear systems with constant coefficients:
\begin{equation}\label{eq:constant coefficient non-homogeneous linear system}
    \frac{\mathrm{d}\mathbf{Y}}{\mathrm{d}x} = \mathbf{A} \mathbf{Y} + \mathbf{F}(x),
\end{equation}
we can also use method of variation of constants to find particular solutions.
\begin{theorem}
    The general solution to the non-homogeneous linear system~\eqref{eq:constant coefficient non-homogeneous linear system} 
    is given by:
    \[
    \mathbf{Y}(x) = e^{\mathbf{A}x}\mathbf{C} + \int_{x_0}^{x} e^{\mathbf{A}(x-s)} \mathbf{F}(s) \, \mathrm{d}s,
    \]
    where $\mathbf{C}$ is a constant vector.
    The solution satisfying the initial condition \(\mathbf{Y}(x_{0}) = \mathbf{Y}_{0}\) is given by:
    \[
    \mathbf{Y}(x) = e^{\mathbf{A}(x - x_{0})} \mathbf{Y}_{0} + \int_{x_0}^{x} e^{\mathbf{A}(x-s)} \mathbf{F}(s) \, \mathrm{d}s.
    \]
\end{theorem}

\vspace{0.7cm}
% 我们面对的问题是: e^Ax 是否可以用初等函数的有限形式表示出来? 如果可以, 如何表示?
The problem we confront is: 
Can \(e^{\mathbf{A}x}\) be expressed in a finite form of elementary functions? If so, how can it be expressed?

In fact, if \(\mathbf{A}\) is an \(n\)-order Jordan block, i.e.,
\begin{align*}
    \mathbf{A} &=
\begin{pmatrix}
    \lambda & 1 & 0 & \cdots & 0 \\
    0 & \lambda & 1 & \cdots & 0 \\
    0 & 0 & \lambda & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda
\end{pmatrix}_{n \times n} = 
\begin{pmatrix} 
    \lambda & 0 & 0 & \cdots & 0 \\
    0 & \lambda & 0 & \cdots & 0 \\
    0 & 0 & \lambda & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda
\end{pmatrix} + 
\begin{pmatrix}
    0 & 1 & 0 & \cdots & 0 \\
    0 & 0 & 1 & \cdots & 0 \\
    0 & 0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & 0
\end{pmatrix} \\
&=:
\operatorname{diag}(\lambda, \lambda, \ldots, \lambda) + \mathbf{Z}_{n},
\end{align*}
then we have:
\begin{equation}\label{eq:matrix exponential of Jordan block}
\begin{aligned}
    e^{\mathbf{A}x} &= e^{\operatorname{diag}(\lambda, \lambda, \ldots, \lambda)x + \mathbf{Z}_{n}x} 
    = e^{\operatorname{diag}(\lambda, \lambda, \ldots, \lambda)x} \cdot e^{\mathbf{Z}_{n}x} \\
    &= e^{\lambda x} \cdot \left( \mathbf{E} + \frac{\mathbf{Z}_{n}x}{1!} + \frac{(\mathbf{Z}_{n}x)^{2}}{2!} + \cdots + \frac{(\mathbf{Z}_{n}x)^{n-1}}{(n-1)!} \right) \\
    &= e^{\lambda x} \cdot
    \begin{pmatrix}
        1 & x & \frac{x^{2}}{2!} & \cdots & \frac{x^{n-1}}{(n-1)!} \\
        0 & 1 & x & \cdots & \frac{x^{n-2}}{(n-2)!} \\
        0 & 0 & 1 & \cdots & \frac{x^{n-3}}{(n-3)!} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 1
    \end{pmatrix}_{n\times n}.
\end{aligned}
\end{equation}
And Jordan canonical form can be deemed as a block diagonal matrix composed of Jordan blocks.
Therefore, if we can compute the Jordan canonical form of matrix \(\mathbf{A}\),
then we can express \(e^{\mathbf{A}x}\) in a finite form of elementary functions.

According to the theory of Jordan canonical form,
for any \(n\)-order square matrix \(\mathbf{A} \in M_{n}(\mathbb{C})\),
there exists an invertible matrix \(\mathbf{P} \in M_{n}(\mathbb{C})\) such that
\[
\mathbf{P}^{-1} \mathbf{A} \mathbf{P} = \mathbf{J},
\]
where \(\mathbf{J}\) is the Jordan canonical form of \(\mathbf{A}\),
\[
\mathbf{J} = \operatorname{diag}(\mathbf{J}_{1}, \mathbf{J}_{2}, \ldots, \mathbf{J}_{k}),
\]
and \(\mathbf{J}_{i}\) is a Jordan block corresponding to eigenvalue \(\lambda_{i}\) of \(\mathbf{A}\).
Then we have:
\begin{equation}\label{eq:matrix exponential by Jordan form}
    e^{\mathbf{A}x} = e^{\mathbf{P} \mathbf{J} \mathbf{P}^{-1} x} = \mathbf{P} e^{\mathbf{J} x} \mathbf{P}^{-1}
    = \mathbf{P} \operatorname{diag} \left( e^{\mathbf{J}_{1} x}, e^{\mathbf{J}_{2} x}, \ldots, 
    e^{\mathbf{J}_{k} x} \right) \mathbf{P}^{-1}.
\end{equation}

However, computing the Jordan canonical form and transition matrix is not always easy.
Note that \(e^{\mathbf{A}x}\) is a fundamental solution matrix to
the homogeneous linear system~\eqref{eq:constant coefficient linear system},
since \(\mathbf{P}\) is invertible,
\(e^{\mathbf{A}x}\mathbf{P}\) 
is also a fundamental solution matrix to~\eqref{eq:constant coefficient linear system}.
Then according to \eqref{eq:matrix exponential by Jordan form},
\(\mathbf{P}e^{\mathbf{J}x}\) is also a fundamental solution matrix to~\eqref{eq:constant coefficient linear system}.

By~\eqref{eq:matrix exponential of Jordan block},
we can utilize method of undetermined coefficients to find
\(n\) linearly independent solution matrix to~\eqref{eq:constant coefficient linear system}.
% 下面按 A 是否有重特征根分类讨论
In the following, we classify the discussion based on whether \(\mathbf{A}\) has repeated eigenvalues.

% A 无重特征根
\begin{leftbarTitle}{Distinct Eigenvalues}\end{leftbarTitle}
\begin{theorem}
    Let \(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\) be \(n\) distinct eigenvalues of matrix \(\mathbf{A}\),
    then the fundamental solution matrix to 
    the homogeneous linear system~\eqref{eq:constant coefficient linear system} is given by:
    \[
    \Phi(x) =
    \begin{pmatrix}
        e^{\lambda_{1} x} \boldsymbol{\xi}_{1} & e^{\lambda_{2} x} \boldsymbol{\xi}_{2} & \cdots & e^{\lambda_{n} x} \boldsymbol{\xi}_{n}
    \end{pmatrix},
    \]
    where \(\boldsymbol{\xi}_{i}\) is the eigenvector corresponding to eigenvalue \(\lambda_{i}\) of 
    matrix \(\mathbf{A}\) (\(i = 1, 2, \ldots, n\)).
\end{theorem}
% 复特征值
If there is a complex eigenvalue \(\lambda=\alpha+ \beta i\) (\(\beta \neq 0\)) of matrix \(\mathbf{A}\),
then its complex conjugate \(\bar{\lambda}=\alpha - \beta i\) is also an eigenvalue of \(\mathbf{A}\).
And their corresponding eigenvectors are also complex conjugates of each other, i.e. \(\boldsymbol{\xi}, \bar{\boldsymbol{\xi}}\).
At this time, the pair of conjugate eigenvalues and eigenvectors will produce a complex solution:
\begin{align*}
    \mathbf{Y} &= e^{\lambda x} \boldsymbol{\xi} = e^{\alpha x} \left[ \cos(\beta x) + i \sin(\beta x) \right] (\boldsymbol{\xi}_{1} + i \boldsymbol{\xi}_{2})  \\
    & = e^{\alpha x} \left[ \left( \cos(\beta x) \boldsymbol{\xi}_{1} - \sin(\beta x) \boldsymbol{\xi}_{2} \right) 
    + i \left( \sin(\beta x) \boldsymbol{\xi}_{1} + \cos(\beta x) \boldsymbol{\xi}_{2} \right) \right].
\end{align*}
Since the equation~\eqref{eq:constant coefficient linear system} is linear, 
the real and imaginary parts of this complex solution are each real solutions to the original equation. 
Thus, we obtain two linearly independent real solutions:
\begin{gather*}
    \mathbf{Y}_{1} = e^{\alpha x} \left( \cos(\beta x) \boldsymbol{\xi}_{1} - \sin(\beta x) \boldsymbol{\xi}_{2} \right), \\
    \mathbf{Y}_{2} = e^{\alpha x} \left( \sin(\beta x) \boldsymbol{\xi}_{1} + \cos(\beta x) \boldsymbol{\xi}_{2} \right).
\end{gather*}

% A 有重特征根
\begin{leftbarTitle}{Repeated Eigenvalues}\end{leftbarTitle}
Assume that \(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{s}\) are all distinct eigenvalues of matrix \(\mathbf{A}\),
with algebraic multiplicities \(n_{1}, n_{2}, \ldots, n_{s}\) respectively,
where \(n_{1} + n_{2} + \cdots + n_{s} = n\).
Note that the fundamental solution matrix \(e^{\mathbf{A}x} \mathbf{P}= \mathbf{P}e^{\mathbf{J}x}\), 
hence in the expression of \(\mathbf{P}e^{\mathbf{J}x}\),
all column vectors corresponding to the eigenvalue \(\lambda_{j}\) have the form:
\begin{equation}\label{eq:solution form for repeated eigenvalue}
    \mathbf{Y} = e^{\lambda_{j} x} 
    \left[ \boldsymbol{\xi}_{0} + \frac{x}{1!}\boldsymbol{\xi}_{1} + \frac{x^2}{2!}\boldsymbol{\xi}_{2} + \cdots + \frac{x^{n_j - 1}}{(n_j - 1)!}\boldsymbol{\xi}_{n_j - 1} \right],
\end{equation}
where \(\xi_{0}, \xi_{1}, \ldots, \xi_{n_j - 1}\) are constant vectors.


\begin{lemma}
    Let \(\lambda_{j}\) be an eigenvalue of matrix \(\mathbf{A}\) with algebraic multiplicity \(n_{j}\),
    then~\eqref{eq:constant coefficient linear system} has non-zero solutions of 
    the form~\eqref{eq:solution form for repeated eigenvalue}
    if and only if
    \(\boldsymbol{\xi}_{0}\) is a non-zero solution to
    \begin{equation}\label{eq:generalized eigenvector equation}
        (\mathbf{A} - \lambda_{j} \mathbf{E})^{n_{j}} \boldsymbol{\xi} = 0,
    \end{equation}
    and \(\boldsymbol{\xi}_{1}, \boldsymbol{\xi}_{2}, \ldots, \boldsymbol{\xi}_{n_j - 1}\) 
    satisfy the following chain of generalized eigenvector equations\footnote{
        In fact, \(\boldsymbol{\xi}_{n_{j}-1}\) is the true eigenvector corresponding to eigenvalue \(\lambda_{j}\),
        and \(\boldsymbol{\xi}_{0}, \boldsymbol{\xi}_{1}, \ldots, \boldsymbol{\xi}_{n_{j}-2}\) 
        are generalized eigenvectors of orders \(n_{j}, n_{j}-1, \ldots, 2\) respectively.
    }:
    \begin{equation}\label{eq:generalized eigenvector chain}    
        \begin{cases}
            \boldsymbol{\xi}_{1} = (\mathbf{A} - \lambda_{j} \mathbf{E}) \boldsymbol{\xi}_{0}, \\
            \boldsymbol{\xi}_{2} = (\mathbf{A} - \lambda_{j} \mathbf{E}) \boldsymbol{\xi}_{1}, \\
            \vdots \\
            \boldsymbol{\xi}_{n_j - 1} = (\mathbf{A} - \lambda_{j} \mathbf{E}) \boldsymbol{\xi}_{n_{j}-2}.
        \end{cases}
    \end{equation}
\end{lemma}

\begin{lemma}
    Under the same conditions as above,
    denote the linear space of all constant vectors of \(n\) degrees as \(V\),
    then
    \begin{enumerate}
        \item The subspace of \(V\) 
            \[
            V_{j} = \{ \xi \in V \mid (\mathbf{A} - \lambda_{j} \mathbf{E})^{n_{j}} \xi = 0 \},\quad j=1, 2, \ldots, s
            \]
            is invariant under \(\mathbf{A}\).
        \item There exists a direct sum decomposition of \(V\):
            \[
            V = V_{1} \oplus V_{2} \oplus \cdots \oplus V_{s}.
            \]
    \end{enumerate}
\end{lemma}

\begin{theorem}
    Under the same conditions as above,
    the fundamental solution matrix to 
    the homogeneous linear system~\eqref{eq:constant coefficient linear system} is given by:
    \[
    \Phi(x) =
    \begin{pmatrix}
        e^{\lambda_{1}x} \mathbf{P}_{1}^{(1)}(x) & 
        e^{\lambda_{2}x} \mathbf{P}_{2}^{(2)}(x) & 
        \cdots & e^{\lambda_{s}x} \mathbf{P}_{s}^{(s)}(x)
    \end{pmatrix},
    \]
    where for each \(j = 1, 2, \ldots, s\),
    \[
    \mathbf{P}_{j}^{(j)}(x) =
        \boldsymbol{\xi}_{j0}^{(i)} + \frac{x}{1!} \boldsymbol{\xi}_{j1}^{(i)} + \frac{x^{2}}{2!} \boldsymbol{\xi}_{j2}^{(i)} + 
        \cdots + \frac{x^{n_j - 1}}{(n_j - 1)!} \boldsymbol{\xi}_{j, n_j - 1}^{(i)},
    \]
    which is the \(j\)-th vector polynomial corresponding to eigenvalue \(\lambda_{i}\) 
    (\(i = 1, 2, \ldots, n_{j};j = 1, 2, \ldots, n_{i}\)).
    \(\boldsymbol{\xi}_{10}^{(i)}, \cdots, \boldsymbol{\xi}_{n_{i}0}^{(i)}\) is \(n_{i}\) linearly independent solutions to
    ~\eqref{eq:generalized eigenvector equation},
    and the other \(\boldsymbol{\xi}_{jl}^{(i)}\) (\(j = 1, 2, \ldots, n_{i}; l = 1, 2, \ldots, n_{j} - 1\))
    is obtained by replacing corresponding \(\boldsymbol{\xi}_{j0}^{(i)}\) with \(\boldsymbol{\xi}_{0}\) in~\eqref{eq:generalized eigenvector chain}.
\end{theorem}



\vspace{0.7cm}
Let us summarize the method from an overall perspective.

When there exist repeated eigenvalues, let \(k\) be the algebraic multiplicity of eigenvalue \(\lambda\).
The core issue is: 
\newline \underline{can we find \(k\) linearly independent eigenvectors 
for this eigenvalue \(\lambda\) that has been repeated \(k\) times?}\footnote{
    As a matter of fact, the method above has already contained the following two cases. 
    But for clarity, we explicitly state them here.
}
\begin{itemize}
    \item If we can, that is \(\text{AM} = \text{GM}\)\footnote{
        \(\text{AM}\) refers to algebraic multiplicity,
        \(\text{GM}\) refers to geometric multiplicity.
    }, and we say the corresponding eigenvalue is complete;
    \item If not, that is \(\text{AM} > \text{GM}\), and we say the corresponding eigenvalue is defective.
        We can only find \(\text{GM}<\text{AM}=k\) linearly independent eigenvectors,
        yet there are still \(k - \text{GM}\) solutions missing.
        In fact, these missing solutions are just the generalized eigenvectors.
\end{itemize}
Now we discuss the two cases separately.
\begin{description}
    \item[Complete] 
        If eigenvalue \(\lambda\) is complete,
        then we can find \(k\) linearly independent eigenvectors 
        \(\boldsymbol{\xi}_{1}, \boldsymbol{\xi}_{2}, \ldots, \boldsymbol{\xi}_{k}\)
        corresponding to eigenvalue \(\lambda\).
        Thus, we can directly write out \(k\) linearly independent solutions:
        \[
        \mathbf{Y}_{1} = e^{\lambda x} \boldsymbol{\xi}_{1}, \quad
        \mathbf{Y}_{2} = e^{\lambda x} \boldsymbol{\xi}_{2}, \quad
        \ldots, \quad
        \mathbf{Y}_{k} = e^{\lambda x} \boldsymbol{\xi}_{k}.
        \]
    \item[Defective] 
        When we cannot find enough eigenvectors, 
        we need to look for so-called generalized eigenvectors to supplement the missing solutions.
        Assume that we have found only \(m < k\) linearly independent eigenvectors
        \(\boldsymbol{\xi}_{1}, \boldsymbol{\xi}_{2}, \ldots, \boldsymbol{\xi}_{m}\),
        i.e., \(\text{GM} = m < k = \text{AM}\).
        Then for each eigenvector \(\boldsymbol{\xi}_{i}\) (\(i = 1, 2, \ldots, m\)),
        we can find a chain of generalized eigenvectors:
        \[
        \boldsymbol{\xi}_{i}, \boldsymbol{\xi}_{i}^{(1)}, \boldsymbol{\xi}_{i}^{(2)}, \ldots, \boldsymbol{\xi}_{i}^{(t_{i})},
        \]
        where \(t_{i}\) is the length of the chain corresponding to eigenvector \(\boldsymbol{\xi}_{i}\),
        and these generalized eigenvectors satisfy:
        \[
        \begin{cases}
            (\mathbf{A} - \lambda \mathbf{E}) \boldsymbol{\xi}_{i}^{(1)} = \boldsymbol{\xi}_{i}, \\
            (\mathbf{A} - \lambda \mathbf{E}) \boldsymbol{\xi}_{i}^{(2)} = \boldsymbol{\xi}_{i}^{(1)}, \\
            \vdots \\
            (\mathbf{A} - \lambda \mathbf{E}) \boldsymbol{\xi}_{i}^{(t_{i})} = \boldsymbol{\xi}_{i}^{(t_{i}-1)},
        \end{cases}
        \]
        they will contribute \(t_{i} + 1\) linearly independent solutions\footnote{
            Here, \(\boldsymbol{\xi}_{i}\) is the true eigenvector,
            and \(\boldsymbol{\xi}_{i}^{(1)}, \boldsymbol{\xi}_{i}^{(2)}, \ldots, \boldsymbol{\xi}_{i}^{(t_{i})}\)
            are generalized eigenvectors of orders \(2, 3, \ldots, t_{i} + 1\) respectively.
            % 不同于之前的编号方式 (自顶向下), 这里采用自底向上的编号方式, 方便理解.
            Different from the previous numbering method (\underline{top-down}), 
            here we use a \underline{bottom-up} numbering method for better understanding.
        }.
        Note that the total number of vectors in all chains is:
        \[
        t_{1} + t_{2} + \cdots + t_{m} + m = k.
        \]
        Then we can write out \(k\) linearly independent solutions:
        \[
        \begin{aligned}
            & \mathbf{Y}_{i} = e^{\lambda x} \boldsymbol{\xi}_{i}, \\
            & \mathbf{Y}_{i}^{(1)} = e^{\lambda x} \left( x \boldsymbol{\xi}_{i} + \boldsymbol{\xi}_{i}^{(1)} \right), \\
            & \mathbf{Y}_{i}^{(2)} = e^{\lambda x} \left( \frac{x^{2}}{2!} \boldsymbol{\xi}_{i} + x \boldsymbol{\xi}_{i}^{(1)} + \boldsymbol{\xi}_{i}^{(2)} \right), \\
            & \vdots \\
            & \mathbf{Y}_{i}^{(t_{i})} = e^{\lambda x} \left( \frac{x^{t_{i}}}{t_{i}!} \boldsymbol{\xi}_{i} + \frac{x^{t_{i}-1}}{(t_{i}-1)!} \boldsymbol{\xi}_{i}^{(1)} + 
            \cdots + x \boldsymbol{\xi}_{i}^{(t_{i}-1)} + \boldsymbol{\xi}_{i}^{(t_{i})} \right),
        \end{aligned}
        \]
        for each \(i = 1, 2, \ldots, m\).
\end{description}





\section{Periodic Coefficient Linear Differential Equation Systems} % 周期系数线性微分方程组
