\documentclass[11pt]{../../TexTemplate/elegantbook} % 这里是文档类，默认使用 elegantbook

\title{Algèbre Linéaire} % 这里放置书名
% \subtitle{Subtitle} % 这里放置副标题

\author{CatMono} % 这里放置作者名
\date{July, 2025} % 这里放置日期
\version{0.1} % 这里放置版本号
% \institute{Elegant\LaTeX{} Program} % 这里放置机构名
% \bioinfo{Custom Key}{Custom Value} % 这里放置自定义信息

% \extrainfo{extra information} % 这里放置额外信息，将显示在最下方中央

\setcounter{tocdepth}{2} % 设置目录深度
\setcounter{secnumdepth}{2} % 设置章节编号深度


% \logo{logo-blue.png} % 这里放置封面logo，默认从figure目录下寻找
% \cover{LogiqueMathematique.png} % 这里放置封面图片，默认从figure目录下寻找

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170} % 自定义颜色
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect} % 保护命令参数不被 LaTeX 解析器过早处理，允许在某些特殊环境中使用脆弱命令（fragile commands）。
\usepackage{xeCJK} % 使用 xeCJK 包支持中文


% ===== 开始文档 =====
\begin{document}

\maketitle %生成文档的标题页，根据之前定义的标题信息（如标题、作者、日期等）自动创建一个格式化的标题页

% === 前言部分 ===
\frontmatter        % 开始前言，页码为 i, ii, iii...
\tableofcontents    % 目录 (页码: i, ii)
% \listoffigures      % 图表目录 (页码: iii)
% \listoftables       % 表格目录 (页码: iv)

\chapter{Preface}   % 前言章节（无编号，页码: v, vi...）
This is the preface of the book...

% \chapter{Acknowledgments}  % 致谢（无编号）
% I would like to thank...
% === 正文部分 ===
\mainmatter         % 开始正文，页码从 1 重新开始

\chapter{Determinants} % 这里放置章节标题
\section{Permutations} % 排列

\section{Determinant and Its Properties} % 行列式及其性质

\section{Expanding by Rows (Columns)} % 按行（列）展开
\begin{leftbarTitle}{Expanding by One Row}\end{leftbarTitle}

\begin{leftbarTitle}{Cramer's Rule}\end{leftbarTitle}

\begin{leftbarTitle}{Expanding by \(k\) Rows}\end{leftbarTitle}

\section{Special Determinants} % 这里放置小节标题
\begin{definition}{Vandermonde Determinant}
    The Vandermonde determinant is defined as
    \[
    V_n = \begin{vmatrix}
    1 & 1 & 1 & \cdots & 1 \\
    x_1 & x_2 & x_3 & \cdots & x_n \\
    x_1^2 & x_2^2 & x_3^2 & \cdots & x_n^2 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_1^{n-1} & x_2^{n-1} & x_3^{n-1} & \cdots & x_n^{n-1}
    \end{vmatrix}
    \]
    where \( x_1, x_2, \ldots, x_n \) are distinct variables.
\end{definition}

The value of the Vandermonde determinant is given by
\[
V_n = \prod_{1 \leq i < j \leq n} (x_j - x_i).
\]  

\begin{definition}{Arrow Determinant}
    The Arrow determinant (\(\nwarrow\)) is defined as
    \[
    A_n = \begin{vmatrix}
    a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
    a_{21} & a_{22} & 0 & \cdots & 0 \\
    a_{31} & 0 & a_{33} & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & 0 & 0 & \cdots & a_{nn}
    \end{vmatrix}.
    \]

    The value of the Arrow determinant is given by
    \[
    A_n = \left( a_{11}- \sum_{k=2}^{n} \frac{a_{1k}a_{k1}}{a_{kk}}  \right)\prod_{k=2}^{n} a_{kk}.
    \]
\end{definition}

From the first column sequentially, subtract \( \frac{a_{21}}{a_{22}} \) times the second column, \(\cdots\), 
\( \frac{a_{n1}}{a_{nn}} \) times the \( n \)-th column, so that the first column becomes:
\[
\begin{bmatrix}
a_{11} - \sum\limits_{k=2}^n \frac{a_{1k}a_{k1}}{a_{kk}} &
0 &
0 &
\vdots &
0
\end{bmatrix}^{\mathrm{T}}.
\]
Then expand along the first column.

\begin{definition}{Two-Triangular Determinant}
    If the determinant satisfies 
    \[ 
    a_{ij} = \begin{cases} 
    a, & i < j, \\ 
    x_{i}, & i = j, \\ 
    b, & i > j,
    \end{cases} 
    \], 
    then \( D_{n} \) is called a two-triangular determinant.
\end{definition}

The value of the two-triangular determinant is given by
\[
\begin{vmatrix}
x_{1} & a & a & \dots & a \\
b & x_{2} & a & \dots & a \\
b & b & x_{3} & \dots & a \\
\vdots & \vdots & \vdots & & \vdots \\
b & b & b & \dots & x_{n}
\end{vmatrix}
=
\begin{cases}
\left[ x_{1} + a \sum\limits^{n}_{k=2} \frac{x_{1}-a}{x_{k}-a} \right] \cdot \prod\limits^{n}_{k=2} (x_{k}-a), & a = b \\
(x_{n}-b) D_{n-1} + \prod\limits^{n-1}_{k=1} (x_{k}-a), & a \neq b 
\end{cases}
\]


\chapter{System of Linear Equations}
\begin{definition}{System of Linear Equations}
    A system of linear equations is a collection of one or more linear equations involving the same set of variables. 
    For example, a system of \( m \) linear equations in \( n \) variables can be written as:
    \[
    \begin{cases}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
    \end{cases}
    \]
    where \( x_1, x_2, \ldots, x_n \) are the variables, \( a_{ij} \) are the coefficients, and \( b_i \) are the constants.

    A solution to the system is an ordered set of values for the variables that satisfies all equations simultaneously.
    If two systems have the same solution set, they are called equivalent systems, 
    whose relationship are equivalence.
\end{definition}

\section{Elimination Method}
\section{Linear Space}
\begin{definition}{Linear Space over Field \(F\)}
    Let \( V \) be a non-empty set, and \( F \) be a field.
    If the following two operations (binary mapping) are defined on \( V \):
    \begin{description}
        \item [Vector addition] For any \( \alpha, \beta \in V \), 
            there exists \( \gamma \in V \) such that \( \gamma = \alpha + \beta \).
        \item [Scalar multiplication] For any \( \alpha \in V \) and \( c \in F \), 
            there exists \( \gamma \in V \) such that \( \gamma = c \cdot \alpha \).
    \end{description}
    and the following axioms are satisfied (for any \( \alpha, \beta, \gamma \in V \) and \( k, l \in F \)):
    \begin{description}
        \item [A1. Commutativity of Addition] \( \alpha + \beta = \beta + \alpha \).
        \item [A2. Associativity of Addition] \( (\alpha + \beta) + \gamma = \alpha + (\beta + \gamma) \).
        \item [A3. Existence of Additive Identity] There exists an element \( 0 \in V \) such that 
            \( \alpha + 0 = 0 + \alpha = \alpha \).
        \item [A4. Existence of Additive Inverse] There exists an element 
            \( -\alpha \in V \) such that \( \alpha + (-\alpha) = (-\alpha) + \alpha = 0 \).
        \item [M1. Compatibility of Scalar Multiplication with Field Multiplication] \( k(l\alpha) = (kl)\alpha\).
        \item [M2. Identity Element of Scalar Multiplication] \( 1\cdot\alpha = \alpha\), where \( 1\) is the multiplicative identity in \( F\).
        \item [D1. Distributivity of Scalar Multiplication with respect to Vector Addition] 
            \( k(\alpha + \beta) = k\alpha + k\beta\).
        \item [D2. Distributivity of Scalar Multiplication with respect to Field Addition] \( (k+l)\alpha = k\alpha + l\alpha\).
    \end{description}
    Then \( V \) is called a linear space (or vector space) over the field \( F \).
\end{definition}

\begin{remark}
    In the definition of linear space, \textbf{A1. Commutativity of Addition} can be derived from the other axioms,
    so it is not strictly necessary to include it as an axiom.
\end{remark}

\vspace{0.7cm}
Give some examples of linear spaces:
\begin{enumerate}
    \item The ring of univariate polynomials \( P[x] \) over the number field \( P \), 
        with the usual polynomial addition and scalar multiplication by elements from \( P \), 
        forms a linear space over the number field \( P \). 
        If we consider only polynomials of degree less than \( n \), along with the zero polynomial, 
        this also forms a linear space over \( P \), denoted as \( P[x]_n \).
        
    \item The set of all positive real numbers \( \mathbb{R}^+ \), with addition and scalar multiplication defined as:
        \[
        a \oplus b = ab, \quad k \circ a = a^k,
        \]
        forms a linear space over the number field \( \mathbb{R} \).
    
    \item The set of all real-valued functions, 
        under function addition and scalar multiplication by elements from \( \mathbb{R} \), 
        forms a linear space over the field \( \mathbb{R} \).
    
    \item The field \( P \) itself, under its own addition and multiplication, forms a linear space over itself.
    
    \item The set of all \(n\)-dimensional vectors (ordered \( n \)-tuples of elements from \( P \))
        over the number field \( P \),
        with the usual vector addition and scalar multiplication, 
        forms a linear space over the number field \( P \).
\end{enumerate}

\vspace{0.7cm}
In this section, we primarily examine linear spaces consisting of n-dimensional vectors over the real number field \( \mathbb{R} \).
As a matter of fact, all elements of a linear space can be called vectors,
and the definitions and properties of vectors in \( \mathbb{R}^n \) can be generalized to any linear space.


\begin{leftbarTitle}{Linear Independence}\end{leftbarTitle}
\begin{definition}{Linear Combination}
    Let \( V \) be a linear space over the field \( F \), 
    and let \( \alpha_1, \alpha_2, \ldots, \alpha_n \in V \).
    For any scalars \( k_1, k_2, \ldots, k_n \in F \), the vector
    \[
    \beta = k_1\alpha_1 + k_2\alpha_2 + \cdots + k_n\alpha_n
    \]
    is called a \textbf{linear combination} of the vectors \( \alpha_1, \alpha_2, \ldots, \alpha_n \)
    (we also say that \( \beta \) is linearly expressed by \( \alpha_1, \alpha_2, \ldots, \alpha_n \)),
    where \( k_i \) are called the coefficients of the linear combination.
\end{definition}

{Linear Representation and Equivalence of Vector Groups}

If every vector \( \alpha_i \) (\( i = 1, 2, \dots, t \)) in the vector group 
\( \alpha_1, \alpha_2, \dots, \alpha_t \) can be linearly expressed by the vector group 
\( \beta_1, \beta_2, \dots, \beta_s \), then the vector group 
\( \alpha_1, \alpha_2, \dots, \alpha_t \) is said to be linearly expressed by the vector group 
\( \beta_1, \beta_2, \dots, \beta_s \).

If two vector groups can be linearly expressed by each other, 
they are said to be \textbf{equivalent}.

\begin{remark}
    Linear representation can be used to identify redundant equations in a system of linear equations. 
    In a system of linear equations, if one equation can be linearly expressed by other equations, 
    then through elementary transformations, the corresponding row in the matrix can be reduced to a row of zeros.

    If two vector groups are equivalent, then their corresponding systems of linear equations are equivalent.
\end{remark}

\begin{definition}{Linear Dependence and Independence of Vector Groups}
    A vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 2) \) is said to be \textbf{linearly dependent} 
    if there exists a vector in the group that can be linearly expressed by the other vectors. 
    
    Another equivalent definition is:
    A vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 1) \) is linearly dependent 
    if there exist \underline{not all zero} scalars \( k_1, k_2, \dots, k_t \) in the field \( F \) such that:
    \[
    k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_t \alpha_t = 0.
    \]

    Conversely, a vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 1) \) is \textbf{linearly independent} if:
    the fact that
    \[
    k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_t \alpha_t = 0
    \]
    implies:
    \[
    k_1 = k_2 = \dots = k_t = 0.
    \]    
\end{definition}

\begin{note}
    If a subset of a vector group is linearly dependent, then the entire vector group is linearly dependent.
    If a vector group is linearly independent, then any non-empty subset of the group is also linearly independent.
    (\textbf{Partial dependence implies overall dependence; overall independence implies partial independence.})
    
    Particularly, since two proportional vectors are linearly dependent, a linearly independent vector group cannot contain two proportional vectors.
\end{note}

A few propositions about linear dependence and independence are given below:
\begin{proposition}
\begin{enumerate}
    \item Generally, for a vector group \( \alpha_i = (a_{i1}, a_{i2}, \dots, a_{in}), \, i = 1, 2, \dots, s \), 
        the necessary and sufficient condition for linear dependence is that the equation:
        \[
        x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_s \alpha_s = 0
        \]
        or the homogeneous system of linear equations:
        \[
        \begin{cases}
        a_{11}x_1 + a_{21}x_2 + \dots + a_{s1}x_s = 0 \\
        a_{12}x_1 + a_{22}x_2 + \dots + a_{s2}x_s = 0 \\
        \vdots \\
        a_{1n}x_1 + a_{2n}x_2 + \dots + a_{sn}x_s = 0
        \end{cases}
        \]
        has a non-zero solution.

    \item  If a vector group \( \alpha_i = (a_{i1}, a_{i2}, \dots, a_{in}), \, i = 1, 2, \dots, s \) 
        is linearly independent, then adding one component to each vector to form an \( n+1 \)-dimensional 
        vector group \( \beta_i = (a_{i1}, a_{i2}, \dots, a_{in}, a_{i,n+1}), \, i = 1, 2, \dots, s \), 
        will also be linearly independent.

    \item For two vector groups \( \alpha_1, \alpha_2, \dots, \alpha_r \) and \( \beta_1, \beta_2, \dots, \beta_s \),
        if the former can be linearly expressed by the latter:
        \begin{enumerate}[label=\roman*.]
            \item and \( r > s \), then the former is linearly dependent.
            \item and \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent, then \( r \leqslant s \).
        \end{enumerate}

    \item Any \( n+1 \) \( n \)-dimensional vectors must be linearly dependent.
    
    \item Two equivalent linearly independent vector groups must contain the same number of vectors.

    \item If a vector group \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent, 
        and adding a vector \( \beta \) to form the vector group \( \alpha_1, \alpha_2, \dots, \alpha_r, \beta \) 
        makes it linearly dependent, then \( \beta \) can be uniquely expressed as a linear combination of 
        \( \alpha_1, \alpha_2, \dots, \alpha_r \).

    \item If a vector \( \beta \) can be linearly expressed by a vector group \( \alpha_1, \alpha_2, \dots, \alpha_r \), 
        then \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent
        if and only if the expression for \( \beta \) is unique.
\end{enumerate}
\end{proposition}

\begin{leftbarTitle}{Dimension, Basis, and Coordinates}\end{leftbarTitle}

\begin{leftbarTitle}{Linear Subspaces}\end{leftbarTitle}


\section{Rank of Vector Groups and Matrices}
\begin{leftbarTitle}{Maximal Linearly Independent Group and Rank of Vector Groups}\end{leftbarTitle}
\begin{definition}{Maximal Linearly Independent Group}
    A subset of a vector group is called a \textbf{maximal linearly independent group} if:
    \begin{enumerate}
        \item The subset itself is linearly independent.
        \item Adding any vector (if available) from the vector group to this subset makes the new subset linearly dependent.
    \end{enumerate}
\end{definition}

\begin{property}
    \begin{enumerate}
    \item The maximal linearly independent group is not unique; 
        any maximal linearly independent group is equivalent to the original vector group.
    \item All maximal linearly independent groups of a vector group contain the same number of vectors.
\end{enumerate}
\end{property}

\begin{definition}{Rank of Vector Groups}
    The number of vectors contained in the maximal linearly independent group of a vector group 
    is called the \textbf{rank} of the vector group.
\end{definition}

\begin{property}
    \begin{enumerate}
    \item A vector group is linearly independent if and only if its rank is equal to the number of vectors it contains.

    \item Equivalent vector groups have the same rank, but vector groups with the same rank are not necessarily equivalent.

    \item If rank of a vector group is \( r \), 
        then any \( r \) linearly independent vectors from the group form a maximal linearly independent group.

    \item A vector group containing non-zero vectors always has a maximal linearly independent group, 
        and any linearly independent subset of vectors can be expanded into a maximal linearly independent group.

    \item If vector group (I) can be linearly expressed by vector group (II), then the rank of (I) does not exceed the rank of (II).
\end{enumerate}
\end{property}




\begin{leftbarTitle}{Rank of Matrices}\end{leftbarTitle}

\section{Solution to Linear Systems}
\begin{leftbarTitle}{Necessary and Sufficient Conditions for the Existence of Solutions}\end{leftbarTitle}

\begin{leftbarTitle}{The Structure of Solutions to Homogeneous Systems of Linear Equations}\end{leftbarTitle}
For a homogeneous system of linear equations, 
it is evident that the linear combination of solutions is still a solution to the system. 
Thus, the concept of a \textbf{fundamental system of solutions} is introduced.

\begin{definition}{Fundamental System of Solutions}
    The homogeneous system of linear equations:
    \[
    \begin{cases}
    a_{11}x_{1} + a_{12}x_{2} + \dots + a_{1n}x_{n} = 0 \\
    a_{21}x_{1} + a_{22}x_{2} + \dots + a_{2n}x_{n} = 0 \\
    \vdots \\
    a_{s1}x_{1} + a_{s2}x_{2} + \dots + a_{sn}x_{n} = 0
    \end{cases}
    \]
    (or in matrix form \( AX = O \))
    has a set of solutions \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \), which is called a \textbf{fundamental system of solutions}, 
    if:
    \begin{enumerate}
        \item Any solution of the system can be expressed as a linear combination of \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \).
        \item \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \) are linearly independent\footnote{
            ensuring there are no redundant solutions in the fundamental system of solutions
        }.
    \end{enumerate}
\end{definition}

\begin{remark}
    This definition is similar to the maximal linearly independent group of a vector group. 
    In fact, the fundamental solution set is a maximal linearly independent group of the solution vector group. 
    Therefore, the number of solutions it contains equals its rank.
\end{remark}


\vspace{0.7cm}
All solutions of a homogeneous system of linear equations (\( AX = O \)) form a linear space (solution space),
whose basis is the fundamental system of solutions.
Dimension of solution space is \( n - r(A) \) (where \( n \) is the number of variables, 
and \( r(A) \) is the rank of the coefficient matrix \( A \)). 
Thus, a fundamental system of solutions contains exactly \( n - r(A) \) solutions.
    



\begin{leftbarTitle}{The Structure of Solutions to Nonhomogeneous Linear Systems}\end{leftbarTitle}
The homogeneous system of linear equations \( AX = O \) is called the \textbf{derived system} of 
the nonhomogeneous system \( AX = B \).

It can be easily proven that:
\begin{itemize}
    \item The difference between two solutions of \( AX = B \) is a solution of its derived system \( AX = 0 \).
    \item The sum of a solution of \( AX = B \) and a solution of \( AX = 0 \) is still a solution of \( AX = B \).
\end{itemize}

\vspace{0.7cm}
If \( \gamma_{0} \) is a particular solution of \( AX = B \), 
then any solution \( \gamma \) of \( AX = B \) can be expressed as:
\[
\gamma = \gamma_{0} + \eta,
\]
where \( \eta \) is a solution of \( AX = 0 \).

Thus, for any particular solution \( \gamma_{0} \) of \( AX = B \), 
when \( \eta \) takes all the solutions of its derived system \( AX = 0 \), 
the above formula provides all the solutions of \( AX = B \).

\chapter{Matrices}
\section{Basic Operations}
\begin{leftbarTitle}{Addition}\end{leftbarTitle}
\begin{leftbarTitle}{Scalar Multiplication}\end{leftbarTitle}
\begin{leftbarTitle}{Transpose}\end{leftbarTitle}
\begin{leftbarTitle}{Matrix Multiplication}\end{leftbarTitle}

\begin{theorem}{Determinant of a Product}
    For square matrices \( A \) and \( B \) of the same order, 
    the determinant of their product is equal to the product of their determinants:
    \[
    |AB| = |A| \cdot |B|.
    \]

    It can be easily extended to multiple matrices:
    \[
    |A_1 A_2 \cdots A_k| = |A_1| \cdot |A_2| \cdots |A_k|.
    \]
\end{theorem}

\begin{theorem}{Cauchy-Binet Formula}
    Let \( A = (a_{ij})_{m \times n} \) and \( B = (b_{ij})_{n \times m} \):  
    \begin{enumerate}
        \item If \( m > n \), then \( |AB| = 0 \);
        \item If \( m \leq n \), then \( |AB| \) is equal to 
        the sum of products of all \( m \)-step minors of \( A \) and 
        the corresponding \( m \)-step minors of \( B \), that is:
        \[
        |AB| = \sum_{1 \leq v_1 < v_2 < \dots < v_m \leq n} 
        \begin{vmatrix}
        A\begin{pmatrix}
        1, 2, \dots, m \\
        v_1, v_2, \dots, v_m
        \end{pmatrix}
        \end{vmatrix}
        \cdot 
        \begin{vmatrix}
        B\begin{pmatrix}
        v_1, v_2, \dots, v_m \\
        1, 2, \dots, m
        \end{pmatrix}
        \end{vmatrix}.
        \]
    \end{enumerate}
\end{theorem}




\begin{leftbarTitle}{Matrix Equivalence}\end{leftbarTitle}
\begin{definition}{Matrix Equivalence}
    Two matrices \( A \) and \( B \) are said to be \textbf{equivalent}, 
    denoted as \( A \cong B \),
    if they can be transformed into one another by a combination of elementary row and column operations.
\end{definition}

\begin{note}
    Obviously, two matrices are equivalent if and only if they have the same rank.
\end{note}

\begin{definition}{Canonical Form of Matrix Equivalence}
    For any matrix \( A \) with rank \( r \), 
    it can be equivalent to the matrix
    \[
    J_r = \begin{pmatrix}
    I_r & O \\
    O & O
    \end{pmatrix}_{m \times n},
    \]
    which is called the \textbf{canonical form} of matrix equivalence.
\end{definition}



\section{Special Matrices}
\begin{leftbarTitle}{Diagonal Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Fundamental Matrix}\end{leftbarTitle}
\begin{definition}{Fundamental Matrix}
    \(E_{ij}\) is called a \textbf{fundamental matrix} if the element in the \(i\)-th row and \(j\)-th column is 1,
    and all other elements are 0.
\end{definition}

\begin{property}
    
\end{property}

\begin{leftbarTitle}{Triangular Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Elementary Matrix}\end{leftbarTitle}
\begin{definition}{Elementary Matrix}
    An \textbf{elementary matrix} is obtained by performing a single elementary row operation on an identity matrix \(E\).
    There are three types of elementary matrices (also hold for column operations):
    \begin{description}
        \item [Row swapping] Interchanges rows \(i\) and \(j\) of \(E\): \(P(i,j)\).
        \item [Row scaling] Multiplies row \(i\) of \(E\) by a non-zero scalar \(c\): \(P(i(c))\).
        \item [Row addition] Adds \(k\) times row \(j\) to row \(i\) of \(E\): \(P(i,j(k))\).
    \end{description}
\end{definition}

\begin{note}
    It can be proven that \(P(i,j)\) can be implemented by a sequence of \(P(i(c))\) and \(P(i,j(k))\) operations.
\end{note}

\begin{leftbarTitle}{Symmetric Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Circulant-Shift Permutation Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Nilpotent Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Row Full Rank Matrix}\end{leftbarTitle}

\section{Inverse Matrix}
\begin{leftbarTitle}{Inverse Matrix and Its Operations}\end{leftbarTitle}

\begin{leftbarTitle}{Equivalent Propositions and Method of Inversion}\end{leftbarTitle}

\begin{leftbarTitle}{Generalized Inverse}\end{leftbarTitle}

\section{Block Matrix}

\begin{theorem}{Determinant Reduction Formula}
    Let \(A_{m\times m}, B_{m\times n}, C_{n\times m}, D_{n\times n}\) be matrices. Then:
    \begin{enumerate}
        \item If \( A \) is invertible, then:
            \[
            \begin{vmatrix}
            A & B \\
            C & D
            \end{vmatrix}
            = |A| \cdot |D - CA^{-1}B|.
            \]
        \item If \( D \) is invertible, then:
            \[
            \begin{vmatrix}
            A & B \\
            C & D
            \end{vmatrix}
            = |D| \cdot |A - BD^{-1}C|.
            \]
        \item If both \( A \) and \( D \) are invertible, then:
            \[
            |D| \cdot |A - BD^{-1}C| = |A| \cdot |D - CA^{-1}B|.
            \]
    \end{enumerate}
\end{theorem}

\begin{remark}
    The mnemonic is: 
    For \( \begin{vmatrix} A & B \\ C & D \end{vmatrix} \), for example, if \( A \) is invertible, 
    one factor is \( |A| \), and the other factor is \( D \) (the diagonal element of \( A \)) minus 
    the product of the other three terms arranged clockwise, where the middle one is the inverse matrix.
\end{remark}




\section{Operations of Rank}
\begin{proposition}
    The matrices \( A \) and \( B \) in the following operations do not need to be square matrices; 
    they only need to be compatible for multiplication or addition.
    \begin{description}
        \item [1. Addition]
        \[
        \operatorname{rank}(A + B) \leqslant \operatorname{rank}(A) + \operatorname{rank}(B).
        \]
        \item [2. Multiplication]
        \[
        \operatorname{rank}(AB) \leqslant \operatorname{rank}(A), \quad \operatorname{rank}(AB) \leqslant \operatorname{rank}(B).
        \]
        \item[2.1. Sylvester's Inequality]
        \[
        \operatorname{rank}(AB) \geqslant \operatorname{rank}(A) + \operatorname{rank}(B) - n \quad (A_{s \times n}, B_{n \times m}).
        \]
        Specially, if \( AB = O \), then:
        \[
        \operatorname{rank}(A) + \operatorname{rank}(B) \leqslant n.
        \]

        \item[2.2. Frobenius Inequality]
        \[
        \operatorname{rank}(ABC) \geqslant \operatorname{rank}(AB) + \operatorname{rank}(BC) - \operatorname{rank}(B).
        \]

        \item [3. Transpose]
        \[
        \operatorname{rank}(AA^T) = \operatorname{rank}(A^TA) = \operatorname{rank}(A) = \operatorname{rank}(A^T).
        \]

        \item [4. Inverse]
        \[
        \operatorname{rank}(A) = \operatorname{rank}(A^{-1}) = n.
        \]

        \item[5. Block Matrix]
        \[
        \operatorname{rank}\begin{pmatrix} A & O \\ O & D \end{pmatrix} = \operatorname{rank}(A) + \operatorname{rank}(D).
        \]
    \end{description}
\end{proposition}

\section{Low-Rank Update}
Due to all the row and column vectors of a rank-\(1\) matrix are linearly dependent,
it can be expressed as the outer product of two non-zero vectors;
in other words, a rank-\(1\) matrix can be expressed as \( \alpha \beta^T \),
where \( \alpha \) and \( \beta \) are non-zero column vectors.

Based on the decomposition \( A = \alpha \beta^T \), the matrix of rank-\(1\) has simplified calculation rules:
\begin{property}
    \begin{description}
        \item [Exponentiation]  
        For any positive integer \( k \geq 1 \),
        \[
        A^k = (\beta^T\alpha)^{k-1} \cdot A,
        \]
        where \( \beta^T\alpha \) is a constant (the inner product of vectors).
        \item [Rank Transmission]  
        If \( B \) is any matrix, then:
        \[
        \operatorname{rank}(AB) \leq 1 \quad \text{and} \quad \operatorname{rank}(BA) \leq 1,
        \]
        (rank 1 matrices multiplied by arbitrary matrices result in ranks not exceeding 1).
    \end{description}
\end{property}



\begin{theorem}{Sherman-Morrison Formula}
    If \( A\in \mathbb{R}^{n\times n} \) is an invertible matrix, 
    and \( \alpha, \beta\in \mathbb{R}^n \) are column vectors, 
    then \( A + \alpha \beta^T \) is invertible if and only if \( 1 + \beta^T A^{-1} \alpha \neq 0 \). 
    In this case, the inverse of \( A + \alpha \beta^T \) is given by:
    \[
    \left(A + \alpha \beta^T\right)^{-1} = A^{-1} - \frac{A^{-1} \alpha \beta^T A^{-1}}{1 + \beta^T A^{-1} \alpha},
    \]
    where \(\alpha\beta^{\mathrm{T}}\) is the outer product of \(\alpha\) and \(\beta\).
\end{theorem}

\begin{note}
    Combining the properties of determinants, 
    we can derive the determinant version of the Sherman-Morrison formula:
    \[
    \left|A + \alpha \beta^T\right| = |A| \cdot \left(1 + \beta^T A^{-1} \alpha\right),
    \]
    which is known as the \textbf{matrix determinant lemma}.

    The theorem can also be stated in terms of the adjugate matrix of \( A \):
    \[
    \det(A + uv^T) = \det(A) + v^T \operatorname{adj}(A) u,
    \]
    in which case it applies whether or not the matrix \( A \) is invertible.
\end{note}



\chapter{Linear Spaces}
\section{Linear Spaces over the Field \(\mathbb{F}\)}

\subsection{Linear Spaces}

\subsection{Dimension, Basis, and Coordinates}

\subsection{Basis Transformation and Coordinate Transformation}

\section{Subspaces}

\subsection{Linear Subspaces}

\subsection{Intersection and Sum of Subspaces}

\subsection{Dimension Formula}

\subsection{Direct Sum of Subspaces}

\section{Isomorphisms}

\section{Quotient Spaces}

\chapter{Linear Mappings}
\section{Linear Mappings and Their Computation}
\subsection{Definition of Linear Mappings}

\subsection{Existence and Uniqueness of Linear Mappings}

\subsection{Operations of Linear Mappings}

\subsection{Special Linear Transformations}

\section{Kernel and Image of Linear Mappings}

\section{Matrix Representation of Linear Mappings}

\section{Linear Functions and Dual Spaces }



\chapter{Diagonalization}
\section{Similarity of Matrices}
\section{Eigenvectors and Diagonalization}
\subsection{Eigenvalues and Eigenvectors}

\subsection{Necessary and Sufficient Conditions for Diagonalization}
\begin{leftbarTitle}{Geometric Multiplicity of Eigenvectors}\end{leftbarTitle}
\begin{leftbarTitle}{Algebraic Multiplicity}\end{leftbarTitle}


\section{Space Decomposition and Diagonalization}

\subsection{Invariant Subspace}

\subsection{Hamilton-Cayley Theorem}

\section{Least Squares and Diagonalization}

\chapter{Jordan Forms}
\section{Polynomial Matrices}

\section{Invariant Factors}

\section{Rational Canonical Form}

\section{Elementary Divisors}

\section{Jordan Canonical Form}

\chapter{Quadratic Forms}
\section{Quadratic Forms and Their Standard Forms}
\section{Canonical Forms}
\section{Definite Quadratic Forms}


\chapter{Inner Product Spaces}
\section{Bilinear Forms}

\section{Real Inner Product Spaces}

\section{Metric Matrices and Standard Orthonormal Bases}

\section{Isomorphism of Real Inner Product Spaces}

\section{Orthogonal Completion and Orthogonal Projection}
\subsection{Orthogonal Completion}

\subsection{Least Squares Method}

\section{Orthogonal Transformations and Symmetric Transformations}
\subsection{Orthogonal Transformations}

\subsection{Symmetric Transformations}

\section{Unitary Spaces and Unitary Transformations}

\section{Symplectic Spaces}

\begin{thebibliography}{99} 
\bibitem{ch1} 丘维声, \emph{ 高等代数 (2nd edition) }, 清华大学出版社, 2019. 
\bibitem{ch2} 谢启鸿, 姚慕生, 吴泉水, \emph{ 高等代数学 (4th edition) }, 复旦大学出版社, 2022.
\bibitem{ch3} 王萼芳, 石生明 \emph{ 高等代数 (5th edition) }, 高等教育出版社, 2019.
\bibitem{ch4} 樊启斌, \emph{ 高等代数典型问题与方法 (1st edition) }, 高等教育出版社, 2021.
\bibitem{9} Wikipedia. \url{https://en.wikipedia.org/wiki/}.
\end{thebibliography}

\end{document}