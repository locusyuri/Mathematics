\documentclass[11pt]{../../TexTemplate/elegantbook} % 这里是文档类，默认使用 elegantbook

\title{Algèbre Linéaire} % 这里放置书名
% \subtitle{Subtitle} % 这里放置副标题

\author{CatMono} % 这里放置作者名
\date{July, 2025} % 这里放置日期
\version{0.1} % 这里放置版本号
% \institute{Elegant\LaTeX{} Program} % 这里放置机构名
% \bioinfo{Custom Key}{Custom Value} % 这里放置自定义信息

% \extrainfo{extra information} % 这里放置额外信息，将显示在最下方中央

\setcounter{tocdepth}{2} % 设置目录深度
\setcounter{secnumdepth}{2} % 设置章节编号深度


% \logo{logo-blue.png} % 这里放置封面logo，默认从figure目录下寻找
% \cover{LogiqueMathematique.png} % 这里放置封面图片，默认从figure目录下寻找

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170} % 自定义颜色
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect} % 保护命令参数不被 LaTeX 解析器过早处理，允许在某些特殊环境中使用脆弱命令（fragile commands）。
\usepackage{xeCJK} % 使用 xeCJK 包支持中文


% ===== 开始文档 =====
\begin{document}

\maketitle %生成文档的标题页，根据之前定义的标题信息（如标题、作者、日期等）自动创建一个格式化的标题页

% === 前言部分 ===
\frontmatter        % 开始前言，页码为 i, ii, iii...
\tableofcontents    % 目录 (页码: i, ii)
% \listoffigures      % 图表目录 (页码: iii)
% \listoftables       % 表格目录 (页码: iv)

\chapter{Preface}   % 前言章节（无编号，页码: v, vi...）
The set of all $m \times n$ matrices over the number field $P$ is denoted by \(M_{m \times n}(P)\) or \(P^{m \times n}\). 
Similarly, the set of all linear mappings from $V$ to $V'$ over the field $F$ 
is represented by \(\mathrm{Hom}(V,V')\) or \(\mathcal{L}(V,V')\). 
Furthermore, when considering the set of all linear transformations, 
it is commonly denoted by \(\mathrm{Hom}(V)\), \(\mathcal{L}(V)\), or \(\mathrm{End}(V)\).


    % \chapter{Acknowledgments}  % 致谢（无编号）
% I would like to thank...
% === 正文部分 ===
\mainmatter         % 开始正文，页码从 1 重新开始

\chapter{Preliminaries} 

\chapter{Determinants} % 这里放置章节标题
\section{Permutations} % 排列

\section{Determinant and Its Properties} % 行列式及其性质

\section{Expanding by Rows (Columns)} % 按行（列）展开
\begin{leftbarTitle}{Expanding by One Row}\end{leftbarTitle}

\begin{leftbarTitle}{Cramer's Rule}\end{leftbarTitle}

\begin{leftbarTitle}{Expanding by \(k\) Rows}\end{leftbarTitle}

\section{Special Determinants} % 这里放置小节标题
\begin{definition}{Vandermonde Determinant}
    The Vandermonde determinant is defined as
    \[
    V_n = \begin{vmatrix}
    1 & 1 & 1 & \cdots & 1 \\
    x_1 & x_2 & x_3 & \cdots & x_n \\
    x_1^2 & x_2^2 & x_3^2 & \cdots & x_n^2 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_1^{n-1} & x_2^{n-1} & x_3^{n-1} & \cdots & x_n^{n-1}
    \end{vmatrix}
    \]
    where \( x_1, x_2, \ldots, x_n \) are distinct variables.
\end{definition}

The value of the Vandermonde determinant is given by
\[
V_n = \prod_{1 \leq i < j \leq n} (x_j - x_i).
\]  

\begin{definition}{Arrow Determinant}
    The Arrow determinant (\(\nwarrow\)) is defined as
    \[
    A_n = \begin{vmatrix}
    a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
    a_{21} & a_{22} & 0 & \cdots & 0 \\
    a_{31} & 0 & a_{33} & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & 0 & 0 & \cdots & a_{nn}
    \end{vmatrix}.
    \]

    The value of the Arrow determinant is given by
    \[
    A_n = \left( a_{11}- \sum_{k=2}^{n} \frac{a_{1k}a_{k1}}{a_{kk}}  \right)\prod_{k=2}^{n} a_{kk}.
    \]
\end{definition}

From the first column sequentially, subtract \( \frac{a_{21}}{a_{22}} \) times the second column, \(\cdots\), 
\( \frac{a_{n1}}{a_{nn}} \) times the \( n \)-th column, so that the first column becomes:
\[
\begin{bmatrix}
a_{11} - \sum\limits_{k=2}^n \frac{a_{1k}a_{k1}}{a_{kk}} &
0 &
0 &
\vdots &
0
\end{bmatrix}^{\mathrm{T}}.
\]
Then expand along the first column.

\begin{definition}{Two-Triangular Determinant}
    If the determinant satisfies 
    \[ 
    a_{ij} = \begin{cases} 
    a, & i < j, \\ 
    x_{i}, & i = j, \\ 
    b, & i > j,
    \end{cases} 
    \], 
    then \( D_{n} \) is called a two-triangular determinant.
\end{definition}

The value of the two-triangular determinant is given by
\[
\begin{vmatrix}
x_{1} & a & a & \dots & a \\
b & x_{2} & a & \dots & a \\
b & b & x_{3} & \dots & a \\
\vdots & \vdots & \vdots & & \vdots \\
b & b & b & \dots & x_{n}
\end{vmatrix}
=
\begin{cases}
\left[ x_{1} + a \sum\limits^{n}_{k=2} \frac{x_{1}-a}{x_{k}-a} \right] \cdot \prod\limits^{n}_{k=2} (x_{k}-a), & a = b \\
(x_{n}-b) D_{n-1} + \prod\limits^{n-1}_{k=1} (x_{k}-a), & a \neq b 
\end{cases}
\]


\chapter{System of Linear Equations}
\begin{definition}{System of Linear Equations}
    A system of linear equations is a collection of one or more linear equations involving the same set of variables. 
    For example, a system of \( m \) linear equations in \( n \) variables can be written as:
    \[
    \begin{cases}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
    \end{cases}
    \]
    where \( x_1, x_2, \ldots, x_n \) are the variables, \( a_{ij} \) are the coefficients, and \( b_i \) are the constants.

    A solution to the system is an ordered set of values for the variables that satisfies all equations simultaneously.
    If two systems have the same solution set, they are called equivalent systems, 
    whose relationship are equivalence.
\end{definition}

\section{Elimination Method}
\section{Linear Space}
\begin{definition}{Linear Space over Field \(F\)}
    Let \( V \) be a non-empty set, and \( F \) be a field.
    If the following two operations (binary mapping) are defined on \( V \):
    \begin{description}
        \item [Vector addition] For any \( \alpha, \beta \in V \), 
            there exists \( \gamma \in V \) such that \( \gamma = \alpha + \beta \).
        \item [Scalar multiplication] For any \( \alpha \in V \) and \( c \in F \), 
            there exists \( \gamma \in V \) such that \( \gamma = c \cdot \alpha \).
    \end{description}
    and the following axioms are satisfied (for any \( \alpha, \beta, \gamma \in V \) and \( k, l \in F \)):
    \begin{description}
        \item [A1. Commutativity of Addition] \( \alpha + \beta = \beta + \alpha \).
        \item [A2. Associativity of Addition] \( (\alpha + \beta) + \gamma = \alpha + (\beta + \gamma) \).
        \item [A3. Existence of Additive Identity] There exists an element \( 0 \in V \) such that 
            \( \alpha + 0 = 0 + \alpha = \alpha \).
        \item [A4. Existence of Additive Inverse] There exists an element 
            \( -\alpha \in V \) such that \( \alpha + (-\alpha) = (-\alpha) + \alpha = 0 \).
        \item [M1. Compatibility of Scalar Multiplication with Field Multiplication] \( k(l\alpha) = (kl)\alpha\).
        \item [M2. Identity Element of Scalar Multiplication] \( 1\cdot\alpha = \alpha\), where \( 1\) is the multiplicative identity in \( F\).
        \item [D1. Distributivity of Scalar Multiplication with respect to Vector Addition] 
            \( k(\alpha + \beta) = k\alpha + k\beta\).
        \item [D2. Distributivity of Scalar Multiplication with respect to Field Addition] \( (k+l)\alpha = k\alpha + l\alpha\).
    \end{description}
    Then \( V \) is called a linear space (or vector space) over the field \( F \).
\end{definition}

\begin{remark}
    In the definition of linear space, \textbf{A1. Commutativity of Addition} can be derived from the other axioms,
    so it is not strictly necessary to include it as an axiom.
\end{remark}

\vspace{0.7cm}
Give some examples of linear spaces:
\begin{enumerate}
    \item The ring of univariate polynomials \( P[x] \) over the number field \( P \), 
        with the usual polynomial addition and scalar multiplication by elements from \( P \), 
        forms a linear space over the number field \( P \). 
        If we consider only polynomials of degree less than \( n \), along with the zero polynomial, 
        this also forms a linear space over \( P \), denoted as \( P[x]_n \).
        
    \item The set of all positive real numbers \( \mathbb{R}^+ \), with addition and scalar multiplication defined as:
        \[
        a \oplus b = ab, \quad k \circ a = a^k,
        \]
        forms a linear space over the number field \( \mathbb{R} \).
    
    \item The set of all real-valued functions, 
        under function addition and scalar multiplication by elements from \( \mathbb{R} \), 
        forms a linear space over the field \( \mathbb{R} \).
    
    \item The field \( P \) itself, under its own addition and multiplication, forms a linear space over itself.
    
    \item The set of all \(n\)-dimensional vectors (ordered \( n \)-tuples of elements from \( P \))
        over the number field \( P \),
        with the usual vector addition and scalar multiplication, 
        forms a linear space over the number field \( P \).
\end{enumerate}

\vspace{0.7cm}
In this section, we primarily examine linear spaces consisting of n-dimensional vectors over the real number field \( \mathbb{R} \).
As a matter of fact, all elements of a linear space can be called vectors,
and the definitions and properties of vectors in \( \mathbb{R}^n \) can be generalized to any linear space.


\begin{leftbarTitle}{Linear Independence}\end{leftbarTitle}
\begin{definition}{Linear Combination}
    Let \( V \) be a linear space over the field \( F \), 
    and let \( \alpha_1, \alpha_2, \ldots, \alpha_n \in V \).
    For any scalars \( k_1, k_2, \ldots, k_n \in F \), the vector
    \[
    \beta = k_1\alpha_1 + k_2\alpha_2 + \cdots + k_n\alpha_n
    \]
    is called a \textbf{linear combination} of the vectors \( \alpha_1, \alpha_2, \ldots, \alpha_n \)
    (we also say that \( \beta \) is linearly expressed by \( \alpha_1, \alpha_2, \ldots, \alpha_n \)),
    where \( k_i \) are called the coefficients of the linear combination.
\end{definition}

\begin{definition}{Linear Representation and Equivalence of Vector Groups}
    If every vector \( \alpha_i \) (\( i = 1, 2, \dots, t \)) in the vector group 
    \( \alpha_1, \alpha_2, \dots, \alpha_t \) can be linearly expressed by the vector group 
    \( \beta_1, \beta_2, \dots, \beta_s \), then the vector group 
    \( \alpha_1, \alpha_2, \dots, \alpha_t \) is said to be linearly expressed by the vector group 
    \( \beta_1, \beta_2, \dots, \beta_s \).

    If two vector groups can be linearly expressed by each other, 
    they are said to be \textbf{equivalent}.
\end{definition}



\begin{remark}
    Linear representation can be used to identify redundant equations in a system of linear equations. 
    In a system of linear equations, if one equation can be linearly expressed by other equations, 
    then through elementary transformations, the corresponding row in the matrix can be reduced to a row of zeros.

    If two vector groups are equivalent, then their corresponding systems of linear equations are equivalent.
\end{remark}

\begin{definition}{Linear Dependence and Independence of Vector Groups}
    A vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 2) \) is said to be \textbf{linearly dependent} 
    if there exists a vector in the group that can be linearly expressed by the other vectors. 
    
    Another equivalent definition is:
    A vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 1) \) is linearly dependent 
    if there exist \underline{not all zero} scalars \( k_1, k_2, \dots, k_t \) in the field \( F \) such that:
    \[
    k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_t \alpha_t = 0.
    \]

    Conversely, a vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 1) \) is \textbf{linearly independent} if:
    the fact that
    \[
    k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_t \alpha_t = 0
    \]
    implies:
    \[
    k_1 = k_2 = \dots = k_t = 0.
    \]    
\end{definition}

\begin{note}
    If a subset of a vector group is linearly dependent, then the entire vector group is linearly dependent.
    If a vector group is linearly independent, then any non-empty subset of the group is also linearly independent.
    (\textbf{Partial dependence implies overall dependence; overall independence implies partial independence.})
    
    Particularly, since two proportional vectors are linearly dependent, a linearly independent vector group cannot contain two proportional vectors.
\end{note}

A few propositions about linear dependence and independence are given below:
\begin{proposition}
\begin{enumerate}
    \item Generally, for a vector group \( \alpha_i = (a_{i1}, a_{i2}, \dots, a_{in}), \, i = 1, 2, \dots, s \), 
        the necessary and sufficient condition for linear dependence is that the equation:
        \[
        x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_s \alpha_s = 0
        \]
        or the homogeneous system of linear equations:
        \[
        \begin{cases}
        a_{11}x_1 + a_{21}x_2 + \dots + a_{s1}x_s = 0 \\
        a_{12}x_1 + a_{22}x_2 + \dots + a_{s2}x_s = 0 \\
        \vdots \\
        a_{1n}x_1 + a_{2n}x_2 + \dots + a_{sn}x_s = 0
        \end{cases}
        \]
        has a non-zero solution.

    \item  If a vector group \( \alpha_i = (a_{i1}, a_{i2}, \dots, a_{in}), \, i = 1, 2, \dots, s \) 
        is linearly independent, then adding one component to each vector to form an \( n+1 \)-dimensional 
        vector group \( \beta_i = (a_{i1}, a_{i2}, \dots, a_{in}, a_{i,n+1}), \, i = 1, 2, \dots, s \), 
        will also be linearly independent.

    \item For two vector groups \( \alpha_1, \alpha_2, \dots, \alpha_r \) and \( \beta_1, \beta_2, \dots, \beta_s \),
        if the former can be linearly expressed by the latter:
        \begin{enumerate}[label=\roman*.]
            \item and \( r > s \), then the former is linearly dependent.
            \item and \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent, then \( r \leqslant s \).
        \end{enumerate}

    \item Any \( n+1 \) \( n \)-dimensional vectors must be linearly dependent.
    
    \item Two equivalent linearly independent vector groups must contain the same number of vectors.

    \item If a vector group \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent, 
        and adding a vector \( \beta \) to form the vector group \( \alpha_1, \alpha_2, \dots, \alpha_r, \beta \) 
        makes it linearly dependent, then \( \beta \) can be uniquely expressed as a linear combination of 
        \( \alpha_1, \alpha_2, \dots, \alpha_r \).

    \item If a vector \( \beta \) can be linearly expressed by a vector group \( \alpha_1, \alpha_2, \dots, \alpha_r \), 
        then \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent
        if and only if the expression for \( \beta \) is unique.
\end{enumerate}
\end{proposition}

\begin{leftbarTitle}{Dimension, Basis, and Coordinates}\end{leftbarTitle}
\begin{definition}{Dimension}
    If a linear space \( V \) over field \(F\) contains \(n\) vectors that are linearly independent,
    but any more than \(n\) vectors are linearly dependent, 
    then \( V \) is called an \( n \)-dimensional linear space,
    noted as \( \dim V_{F} = n \), simplified as \( \dim V = n \) when the field is clear from context.

    If a linear space does contain as many vectors that are linearly independent as you wish, 
    it is called an infinite-dimensional linear space.
\end{definition}

\begin{definition}{Basis and Coordinates}
    In an \( n \)-dimensional linear space \( V \), 
    if \( \alpha_1, \alpha_2, \dots, \alpha_n \) are \( n \) linearly independent vectors in \( V \),
    then they form a \textbf{basis} of \( V \).

    Any vector \( \alpha \in V \) can be uniquely expressed as a linear combination of the basis vectors:
    \[
    \alpha = k_1\alpha_1 + k_2\alpha_2 + \dots + k_n\alpha_n,
    \]
    where \( k_1, k_2, \dots, k_n \in F \) are called the \textbf{coordinates} of \( \alpha \) with respect to the basis 
    \( \alpha_1, \alpha_2, \dots, \alpha_n \).
\end{definition}

The definition leads to a theorem that is easy to see:
If there are \( n \) linearly independent vectors in a linear space \( V \),
and any vector in \( V \) can be expressed as a linear combination of these \( n \) vectors,
then these \( n \) vectors form a basis of \( V \), and \( V \) is an \( n \)-dimensional linear space.


\begin{leftbarTitle}{Linear Subspaces}\end{leftbarTitle}
\begin{definition}{Linear Subspace}
    Let \( V \) be a linear space over field \( F \), 
    and let \( W \subseteq V \) be a non-empty subset of \( V \).
    If \( W \) itself is a linear space under the same addition and scalar multiplication as in \( V \),
    then \( W \) is called a \textbf{linear subspace} of \( V \).
    
    Zero Subspace and whole space \( V \) itself are called trivial subspaces of \( V \).
\end{definition}

It can be easily derived that:
Let \(W\) be a non-empty subset of a linear space \(V\).
Then \(W\) is a linear subspace of \(V\) if and only if:
\begin{enumerate}
    \item For any \( \alpha, \beta \in W \), \( \alpha + \beta \in W \).
    \item For any \( \alpha \in W \) and \( k \in F \), \( k\alpha \in W \).
\end{enumerate}


\section{Rank of Vector Groups and Matrices}
\begin{leftbarTitle}{Maximal Linearly Independent Group and Rank of Vector Groups}\end{leftbarTitle}
\begin{definition}{Maximal Linearly Independent Group}
    A subset of a vector group is called a \textbf{maximal linearly independent group} if:
    \begin{enumerate}
        \item The subset itself is linearly independent.
        \item Adding any vector (if available) from the vector group to this subset makes the new subset linearly dependent.
    \end{enumerate}
\end{definition}

\begin{property}
    \begin{enumerate}
    \item The maximal linearly independent group is not unique; 
        any maximal linearly independent group is equivalent to the original vector group.
    \item All maximal linearly independent groups of a vector group contain the same number of vectors.
\end{enumerate}
\end{property}

\begin{definition}{Rank of Vector Groups}
    The number of vectors contained in the maximal linearly independent group of a vector group 
    is called the \textbf{rank} of the vector group.
\end{definition}

\begin{property}
    \begin{enumerate}
    \item A vector group is linearly independent if and only if its rank is equal to the number of vectors it contains.

    \item Equivalent vector groups have the same rank, but vector groups with the same rank are not necessarily equivalent.

    \item If rank of a vector group is \( r \), 
        then any \( r \) linearly independent vectors from the group form a maximal linearly independent group.

    \item A vector group containing non-zero vectors always has a maximal linearly independent group, 
        and any linearly independent subset of vectors can be expanded into a maximal linearly independent group.

    \item If vector group (I) can be linearly expressed by vector group (II), then the rank of (I) does not exceed the rank of (II).
\end{enumerate}
\end{property}




\begin{leftbarTitle}{Rank of Matrices}\end{leftbarTitle}
\begin{definition}{Rank of Matrices}
    The row rank of a matrix is defined as the rank of its row vectors;
    the column rank of a matrix is defined as the rank of its column vectors. 
    The rank of a matrix is defined as the highest order of its non-zero minors.
\end{definition}

\begin{theorem}
    The row rank and column rank of any matrix are equal.
\end{theorem}

\begin{property}
    \begin{enumerate}
        \item Elementary row and column transformations do not change the rank of a matrix.
        \item The rank of a matrix is equal to the number of non-zero rows in the row (column) echelon form of the matrix 
            obtained by elementary row (column) operations.
        \item Elementary row (column) operations do not alter the linear relationships among column (row) vectors.
        \item For a square matrix \(\left( a_{ij} \right)_{n\times n} \), its row vectors (or column vectors) are 
            linearly independent if and only if its determinant \( \det(a_{ij}) \neq 0 \).
    \end{enumerate}
\end{property}
The third property gives a method to find the maximal linearly independent group of row (or column) vectors of a matrix,
take the column vectors as an example:
\begin{enumerate}
    \item Transform the matrix into its row echelon form (REF) or reduced row echelon form (RREF) using elementary row operations.
        For example, for a matrix \( \left( \alpha_{1}, \alpha_{2}, \alpha_{3}, \alpha_{4}, \alpha_{5} \right) \),
        its REF is shown below:
        \[
        \begin{pmatrix}
        a_{11} & a_{12} & 0 & 0 & 0 \\
        0 & 0 & a_{23} & 0 & 0 \\
        0 & 0 & 0 & 0 & a_{35} \\
        \end{pmatrix}.
        \]
    \item Identify the leading 1s (pivots) in each row. The columns containing these pivots correspond to the maximal linearly independent group of row vectors.
        In this example, \(\alpha_{1}, \alpha_{3}, \alpha_{5}\) form a maximal linearly independent group.
\end{enumerate}


\section{Solution to Linear Systems}
\begin{leftbarTitle}{Necessary and Sufficient Conditions for the Existence of Solutions}\end{leftbarTitle}

\begin{leftbarTitle}{The Structure of Solutions to Homogeneous Systems}\end{leftbarTitle}
For a homogeneous system of linear equations, 
it is evident that the linear combination of solutions is still a solution to the system. 
Thus, the concept of a \textbf{fundamental system of solutions} is introduced.

\begin{definition}{Fundamental System of Solutions}
    The homogeneous system of linear equations:
    \[
    \begin{cases}
    a_{11}x_{1} + a_{12}x_{2} + \dots + a_{1n}x_{n} = 0 \\
    a_{21}x_{1} + a_{22}x_{2} + \dots + a_{2n}x_{n} = 0 \\
    \vdots \\
    a_{s1}x_{1} + a_{s2}x_{2} + \dots + a_{sn}x_{n} = 0
    \end{cases}
    \]
    (or in matrix form \( AX = O \))
    has a set of solutions \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \), which is called a \textbf{fundamental system of solutions}, 
    if:
    \begin{enumerate}
        \item Any solution of the system can be expressed as a linear combination of \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \).
        \item \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \) are linearly independent\footnote{
            ensuring there are no redundant solutions in the fundamental system of solutions
        }.
    \end{enumerate}
\end{definition}

\begin{remark}
    This definition is similar to the maximal linearly independent group of a vector group. 
    In fact, the fundamental solution set is a maximal linearly independent group of the solution vector group. 
    Therefore, the number of solutions it contains equals its rank.
\end{remark}


\vspace{0.7cm}
All solutions of a homogeneous system of linear equations (\( AX = O \)) form a linear space (solution space),
whose basis is the fundamental system of solutions.
Dimension of solution space is \( n - r(A) \) (where \( n \) is the number of variables, 
and \( r(A) \) is the rank of the coefficient matrix \( A \)). 
Thus, a fundamental system of solutions contains exactly \( n - r(A) \) solutions.
    



\begin{leftbarTitle}{The Structure of Solutions to Nonhomogeneous Linear Systems}\end{leftbarTitle}
The homogeneous system of linear equations \( AX = O \) is called the \textbf{derived system} of 
the nonhomogeneous system \( AX = B \).

It can be easily proven that:
\begin{itemize}
    \item The difference between two solutions of \( AX = B \) is a solution of its derived system \( AX = 0 \).
    \item The sum of a solution of \( AX = B \) and a solution of \( AX = 0 \) is still a solution of \( AX = B \).
\end{itemize}

\vspace{0.7cm}
If \( \gamma_{0} \) is a particular solution of \( AX = B \), 
then any solution \( \gamma \) of \( AX = B \) can be expressed as:
\[
\gamma = \gamma_{0} + \eta,
\]
where \( \eta \) is a solution of \( AX = 0 \).

Thus, for any particular solution \( \gamma_{0} \) of \( AX = B \), 
when \( \eta \) takes all the solutions of its derived system \( AX = 0 \), 
the above formula provides all the solutions of \( AX = B \).

\chapter{Matrices}
\section{Basic Operations}
\begin{leftbarTitle}{Addition}\end{leftbarTitle}
\begin{leftbarTitle}{Scalar Multiplication}\end{leftbarTitle}
\begin{leftbarTitle}{Transpose}\end{leftbarTitle}
\begin{leftbarTitle}{Matrix Multiplication}\end{leftbarTitle}

\begin{theorem}{Determinant of a Product}
    For square matrices \( A \) and \( B \) of the same order, 
    the determinant of their product is equal to the product of their determinants:
    \[
    |AB| = |A| \cdot |B|.
    \]

    It can be easily extended to multiple matrices:
    \[
    |A_1 A_2 \cdots A_k| = |A_1| \cdot |A_2| \cdots |A_k|.
    \]
\end{theorem}

\begin{theorem}{Cauchy-Binet Formula}
    Let \( A = (a_{ij})_{m \times n} \) and \( B = (b_{ij})_{n \times m} \):  
    \begin{enumerate}
        \item If \( m > n \), then \( |AB| = 0 \);
        \item If \( m \leq n \), then \( |AB| \) is equal to 
        the sum of products of all \( m \)-step minors of \( A \) and 
        the corresponding \( m \)-step minors of \( B \), that is:
        \[
        |AB| = \sum_{1 \leq v_1 < v_2 < \dots < v_m \leq n} 
        \begin{vmatrix}
        A\begin{pmatrix}
        1, 2, \dots, m \\
        v_1, v_2, \dots, v_m
        \end{pmatrix}
        \end{vmatrix}
        \cdot 
        \begin{vmatrix}
        B\begin{pmatrix}
        v_1, v_2, \dots, v_m \\
        1, 2, \dots, m
        \end{pmatrix}
        \end{vmatrix}.
        \]
    \end{enumerate}
\end{theorem}




\begin{leftbarTitle}{Matrix Equivalence}\end{leftbarTitle}
\begin{definition}{Matrix Equivalence}
    Two matrices \( A \) and \( B \) are said to be \textbf{equivalent}, 
    denoted as \( A \cong B \),
    if they can be transformed into one another by a combination of elementary row and column operations.
\end{definition}

\begin{note}
    Obviously, two matrices are equivalent if and only if they have the same rank.
\end{note}

\begin{definition}{Canonical Form of Matrix Equivalence}
    For any matrix \( A \) with rank \( r \), 
    it can be equivalent to the matrix
    \[
    J_r = \begin{pmatrix}
    I_r & O \\
    O & O
    \end{pmatrix}_{m \times n},
    \]
    which is called the \textbf{canonical form} of matrix equivalence.
\end{definition}



\section{Special Square Matrices}
\begin{leftbarTitle}{Diagonal Matrix}\end{leftbarTitle}
\begin{definition}{Diagonal Matrix}
    A matrix \( A = (a_{ij})_{n \times n} \) is called a \textbf{diagonal matrix} if:
    \[
    a_{ij} = 0, \quad i \neq j,
    \]
    denoted as:
    \[
    A = \mathrm{diag}(a_{11}, a_{22}, \dots, a_{nn}).
    \]

    If 
    \[
    a_{i,n+1-i} = 0, \quad i = 1, 2, \dots, n,
    \]
    then \( A \) is called an \textbf{skew-diagonal matrix},
    denoted as:
    \[
    A = \mathrm{sdiag}(a_{1n}, a_{2,n-1}, \dots, a_{n1}).
    \]
\end{definition}

\begin{property}
    \begin{enumerate}
        \item The sum, difference, and product of two diagonal matrices of the same order are still diagonal matrices.
        \item Multiplying a matrix \(A\) on the left (right) by a diagonal matrix is equivalent to 
            multiplying the corresponding rows (columns) of \(A\) by the diagonal elements of the diagonal matrix.
        \item 
            \[
            \mathrm{diag}(a_{1}, a_{2}, \dots, a_{n})^{-1} = 
            \mathrm{diag}\left( \frac{1}{a_{1}}, \frac{1}{a_{2}}, \dots, \frac{1}{a_{n}} \right),
            \] 
            \[
            \mathrm{sdiag}(a_{1}, a_{2}, \dots, a_{n})^{-1} = 
            \mathrm{sdiag}\left( \frac{1}{a_{n}}, \frac{1}{a_{n-1}}, \dots, \frac{1}{a_{1}} \right);
            \]
            \[
            \begin{bmatrix} A & O \\ O & C \\\end{bmatrix}^{-1} =
            \begin{bmatrix} A^{-1} & O \\ O & C^{-1} \\\end{bmatrix},
            \]
            \[
            \begin{bmatrix} O & B \\ D & O \\\end{bmatrix}^{-1} =
            \begin{bmatrix} O & D^{-1} \\ B^{-1} & O \\\end{bmatrix}.
            \]
    \end{enumerate} 
\end{property}




\begin{leftbarTitle}{Fundamental Matrix}\end{leftbarTitle}
\begin{definition}{Fundamental Matrix}
    \(E_{ij}\) is called a \textbf{fundamental matrix} if the element in the \(i\)-th row and \(j\)-th column is 1,
    and all other elements are 0.
\end{definition}

\begin{property}
    \begin{enumerate}
        \item Multiplying a matrix \(A\) on the left by a fundamental matrix is equivalent to 
            moving the \(j\)-th row of \(A\) to the \(i\)-th position, 
            with all other entries in the resulting matrix being zero; 
            multiplying \(A\) on the right by a fundamental matrix is equivalent to 
            moving the \(i\)-th column of \(A\) to the \(j\)-th position, 
            with all other entries in the resulting matrix being zero.\footnote{
                The mnemonic is:
                \[
                \text{moves row: }E_{i\leftarrow j}A;\qquad
                \text{moves column: }AE_{i\rightarrow j};
                \]
            }
        \item If \(A\) is commutative with all fundamental matrices, then \(A\) is a scalar matrix.
    \end{enumerate}
\end{property}

\begin{leftbarTitle}{Triangular Matrix}\end{leftbarTitle}
\begin{definition}{Triangular Matrix}
    A matrix \( A = (a_{ij})_{n \times n} \) is called an \textbf{upper triangular matrix} if:
    \[
    a_{ij} = 0, \quad i > j,
    \]
    denoted as:
    \[
    A = 
    \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    0 & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & a_{nn}
    \end{pmatrix}.
    \]

    Similarly, if:
    \[
    a_{ij} = 0, \quad i < j,
    \]
    then \( A \) is called a \textbf{lower triangular matrix},
    denoted as:
    \[
    A = 
    \begin{pmatrix}
    a_{11} & 0 & \cdots & 0 \\
    a_{21} & a_{22} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{pmatrix}.
    \]
\end{definition}

\begin{property}
    \begin{enumerate}
        \item The sum, difference, and product of two upper (lower) triangular matrices of the same order 
            are still upper (lower) triangular matrices. 
        \item For all square matrix \(A\), \(A\) can be expressed as the sum of 
            an upper triangular matrix and a lower triangular matrix:
            \[
            A = \frac{A + A^T}{2} + \frac{A - A^T}{2}.
            \]
        \item (\textbf{\(LU\) Decomposition}) If all leading principal minors of a square matrix \(A\) are non-zero, 
            then \(A\) can be uniquely decomposed into the product of a lower triangular matrix \(L\) 
            (with all diagonal elements equal to 1) and an invertible upper triangular matrix \(U\):
            \[
            A = LU.
            \]
            The converse is also true.
    \end{enumerate}
\end{property}


\begin{leftbarTitle}{Elementary Matrix}\end{leftbarTitle}
\begin{definition}{Elementary Matrix}
    An \textbf{elementary matrix} is obtained by performing a single elementary row operation on an identity matrix \(E\).
    There are three types of elementary matrices (also hold for column operations):
    \begin{description}
        \item [Row swapping] Interchanges rows \(i\) and \(j\) of \(E\): \(P(i,j)\).
        \item [Row scaling] Multiplies row \(i\) of \(E\) by a non-zero scalar \(c\): \(P(i(c))\).
        \item [Row addition] Adds \(k\) times row \(j\) to row \(i\) of \(E\): \(P(i,j(k))\).
    \end{description}
\end{definition}

\begin{note}
    It can be proven that \(P(i,j)\) can be implemented by a sequence of \(P(i(c))\) and \(P(i,j(k))\) operations.
\end{note}

\begin{leftbarTitle}{Symmetric Matrix}\end{leftbarTitle}
\begin{definition}{Symmetric Matrix}
    A matrix \( A = (a_{ij})_{n \times n} \) is called a \textbf{symmetric matrix} if:
    \[
    a_{ij} = a_{ji}, \quad i, j = 1, 2, \dots, n,
    \]
    or equivalently:
    \[
    A = A^T.
    \]
    
    Similarly, if:
    \[
    A = -A^T,
    \]
    then \( A \) is called a \textbf{skew-symmetric matrix}.
\end{definition}

\begin{property}
    \begin{enumerate}
        \item The linear combination of two symmetric (skew-symmetric) matrices of the same order 
            are still symmetric (skew-symmetric) matrices. 
        \item For any square matrix \(A\), \(A\) can be expressed as the sum of 
            a symmetric matrix and a skew-symmetric matrix:
            \[
            A = \frac{A + A^T}{2} + \frac{A - A^T}{2}.
            \]
        \item The determinant of an odd-order skew-symmetric matrix is zero; 
            the rank of a skew-symmetric matrix is even. 
    \end{enumerate}
\end{property}


\begin{leftbarTitle}{Circulant Matrix}\end{leftbarTitle}
Let 
\[
C = 
\begin{bmatrix} 
    0 & 1 & 0 & 0 & \cdots & 0 \\ 
    0 & 0 & 1 & 0 & \cdots & 0 \\ 
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 
    0 & 0 & 0 & 0 & \cdots & 1 \\
    1 & 0 & 0 & 0 & \cdots & 0
\end{bmatrix},
\]
it is called a \textbf{circulant-shift matrix}.

The matrix \(C\) performs specific cyclic operations when multiplied by another matrix:
\begin{description}
    \item [Left multiplication] Multiplying a matrix \(A\) from the left by \(C\) (\(CA\)) 
        is equivalent to cyclically shifting the rows of \(A\) upwards by one position. 
        The first row of \(A\) moves to the last row.
    \item [Right multiplication] Multiplying a matrix \(A\) from the right by \(C\) (\(AC\)) 
        is equivalent to cyclically shifting the columns of \(A\) to the right by one position. 
        The last column of \(A\) moves to the first column.
\end{description}

\begin{definition}{Circulant Matrix}
    A matrix \(A\) is called a \textbf{circulant matrix} if it can be expressed as:
    \[
    A = 
    \begin{bmatrix} 
        a_0 & a_1 & a_2 & \cdots & a_{n-1} \\ 
        a_{n-1} & a_0 & a_1 & \cdots & a_{n-2} \\ 
        a_{n-2} & a_{n-1} & a_0 & \cdots & a_{n-3} \\ 
        \vdots & \vdots & \vdots & \ddots & \vdots \\ 
        a_1 & a_2 & a_3 & \cdots & a_0
    \end{bmatrix},
    \]
    which is constructed such that each row is obtained by cyclically shifting 
    the elements of the first row one position to the right.

    It can also be expressed as:
    \[
    A = a_0 E + a_1 C + a_2 C^2 + \cdots + a_{n-1} C^{n-1} = \sum_{i=0}^{n-1} a_i C^i,
    \]
    where \(E\) is the identity matrix and \(C\) is the circulant-shift matrix.
\end{definition}


\begin{leftbarTitle}{Nilpotent Matrix}\end{leftbarTitle}
\begin{definition}{Nilpotent Matrix}
    A square matrix \(A\) is called a \textbf{nilpotent matrix} if there exists \(l\in \mathbb{N}_{+}\) such that:
    \[
    A^l = O,
    \]
    where \(O\) is the zero matrix.
    The smallest such \(l\) is called the \textbf{nilpotent index} of the nilpotent matrix \(A\).
\end{definition}

\begin{proposition}
    \begin{enumerate}
        \item A triangular matrix is nilpotent if and only if all its diagonal elements are zero. 
        \item If a \(n\)-order triangular matrix is nilpotent, then its nilpotent index \(l\) must satisfy \(l \leq n\).
    \end{enumerate}
\end{proposition}


\begin{leftbarTitle}{Full Row Rank Matrix}\end{leftbarTitle}
\begin{definition}{Full Row Rank Matrix}
    A matrix \(A\) is called a \textbf{full row rank matrix} if its row vectors are linearly independent.
    Similarly, if its column vectors are linearly independent, 
    then \(A\) is called a \textbf{full column rank matrix}.
\end{definition}

\begin{property}
    \begin{enumerate}
        \item \(A_{m\times n}\) is a full row rank matrix if and only if 
            there exists an invertible matrix \(Q_{n\times n}\) such that 
            \[A = \begin{bmatrix} E_{m} & O \end{bmatrix}Q.\]
        \item \(A_{m\times n}\) is a full column rank matrix if and only if 
            there exists an invertible matrix \(P_{m\times m}\) such that 
            \[A = P\begin{bmatrix} E_{n} \\ O \end{bmatrix}.\]
    \end{enumerate}
\end{property}

\section{Inverse Matrix}
\begin{leftbarTitle}{Inverse Matrix and Its Operations}\end{leftbarTitle}

\begin{leftbarTitle}{Equivalent Propositions and Method of Inversion}\end{leftbarTitle}

\begin{leftbarTitle}{Generalized Inverse}\end{leftbarTitle}

\section{Block Matrix}

\begin{theorem}{Determinant Reduction Formula}
    Let \(A_{m\times m}, B_{m\times n}, C_{n\times m}, D_{n\times n}\) be matrices. Then:
    \begin{enumerate}
        \item If \( A \) is invertible, then:
            \[
            \begin{vmatrix}
            A & B \\
            C & D
            \end{vmatrix}
            = |A| \cdot |D - CA^{-1}B|.
            \]
        \item If \( D \) is invertible, then:
            \[
            \begin{vmatrix}
            A & B \\
            C & D
            \end{vmatrix}
            = |D| \cdot |A - BD^{-1}C|.
            \]
        \item If both \( A \) and \( D \) are invertible, then:
            \[
            |D| \cdot |A - BD^{-1}C| = |A| \cdot |D - CA^{-1}B|.
            \]
    \end{enumerate}
\end{theorem}

\begin{remark}
    The mnemonic is: 
    For \( \begin{vmatrix} A & B \\ C & D \end{vmatrix} \), for example, if \( A \) is invertible, 
    one factor is \( |A| \), and the other factor is \( D \) (the diagonal element of \( A \)) minus 
    the product of the other three terms arranged \underline{clockwise}, where the middle one is the inverse matrix.
\end{remark}




\section{Operations of Rank}
\begin{proposition}
    The matrices \( A \) and \( B \) in the following operations do not need to be square matrices; 
    they only need to be compatible for multiplication or addition.
    \begin{description}
        \item [1. Addition]\footnote{
            Actually, on the basis of dimension formula (Thm~\ref{thm:dimension_formula}),
            let \( \mathcal{R}(A) \) and \( \mathcal{R}(B) \) be the column spaces of matrices \( A \) and \( B \) respectively,
            then:
            \[
            \operatorname{dim}(\mathcal{R}(A) + \mathcal{R}(B)) = 
            \operatorname{dim}(\mathcal{R}(A)) + \operatorname{dim}(\mathcal{R}(B)) - 
            \operatorname{dim}(\mathcal{R}(A) \cap \mathcal{R}(B)),
            \]
            i.e.,
            \[
            \operatorname{rank}(A + B) = \operatorname{rank}(A) + \operatorname{rank}(B) - 
            \operatorname{dim}(\mathcal{R}(A) \cap \mathcal{R}(B)).
            \]
        }
        \[
        \left| \operatorname{rank}(A) - \operatorname{rank}(B) \right| \leqslant 
        \operatorname{rank}(A + B) \leqslant \operatorname{rank}(A) + \operatorname{rank}(B).
        \]
        \item [2. Multiplication]
        \[
        \operatorname{rank}(AB) \leqslant \operatorname{rank}(A), \quad \operatorname{rank}(AB) \leqslant \operatorname{rank}(B).
        \]
        \item[2.1. Sylvester's Inequality]
        \[
        \operatorname{rank}(AB) \geqslant \operatorname{rank}(A) + \operatorname{rank}(B) - n \quad (A_{s \times n}, B_{n \times m}).
        \]
        Specially, if \( AB = O \), then:
        \[
        \operatorname{rank}(A) + \operatorname{rank}(B) \leqslant n.
        \]

        \item[2.2. Frobenius Inequality]
        \[
        \operatorname{rank}(ABC) \geqslant \operatorname{rank}(AB) + \operatorname{rank}(BC) - \operatorname{rank}(B).
        \]

        \item [3. Transpose]
        \[
        \operatorname{rank}(AA^T) = \operatorname{rank}(A^TA) = \operatorname{rank}(A) = \operatorname{rank}(A^T).
        \]

        \item [4. Inverse]
        \[
        \operatorname{rank}(A) = \operatorname{rank}(A^{-1}) = n.
        \]

        \item[5. Block Matrix]
        \[
        \operatorname{rank}\begin{pmatrix} A & O \\ O & D \end{pmatrix} = \operatorname{rank}(A) + \operatorname{rank}(D).
        \]
    \end{description}
\end{proposition}

\section{Low-Rank Corrections}
Due to all the row and column vectors of a rank-\(1\) matrix are linearly dependent,
it can be expressed as the outer product of two non-zero vectors;
in other words, a rank-\(1\) matrix can be expressed as \( \alpha \beta^T \),
where \( \alpha \) and \( \beta \) are non-zero column vectors.

Based on the decomposition \( A = \alpha \beta^T \), the matrix of rank-\(1\) has simplified calculation rules:
\begin{property}
    \begin{description}
        \item [Exponentiation]  
        For any positive integer \( k \geq 1 \),
        \[
        A^k = (\beta^T\alpha)^{k-1} \cdot A,
        \]
        where \( \beta^T\alpha \) is a constant (the inner product of vectors).
        \item [Rank Transmission]  
        If \( B \) is any matrix, then:
        \[
        \operatorname{rank}(AB) \leq 1 \quad \text{and} \quad \operatorname{rank}(BA) \leq 1,
        \]
        (rank 1 matrices multiplied by arbitrary matrices result in ranks not exceeding 1).
    \end{description}
\end{property}



\begin{theorem}{Sherman-Morrison Formula}
    If \( A\in \mathbb{R}^{n\times n} \) is an invertible matrix, 
    and \( \alpha, \beta\in \mathbb{R}^n \) are column vectors, 
    then \( A + \alpha \beta^T \) is invertible if and only if \( 1 + \beta^T A^{-1} \alpha \neq 0 \). 
    In this case, the inverse of \( A + \alpha \beta^T \) is given by:
    \[
    \left(A + \alpha \beta^T\right)^{-1} = A^{-1} - \frac{A^{-1} \alpha \beta^T A^{-1}}{1 + \beta^T A^{-1} \alpha},
    \]
    where \(\alpha\beta^{\mathrm{T}}\) is the outer product of \(\alpha\) and \(\beta\).
\end{theorem}

\begin{note}
    Combining the properties of determinants, 
    we can derive the determinant version of the Sherman-Morrison formula:
    \[
    \left|A + \alpha \beta^T\right| = |A| \cdot \left(1 + \beta^T A^{-1} \alpha\right),
    \]
    which is known as the \textbf{matrix determinant lemma}.

    The theorem can also be stated in terms of the adjugate matrix of \( A \):
    \[
    \det(A + uv^T) = \det(A) + v^T \operatorname{adj}(A) u,
    \]
    in which case it applies whether or not the matrix \( A \) is invertible.
\end{note}

\vspace{0.7cm}
Replace \( \alpha, \beta\) with general matrices \( U, V \in \mathbb{R}^{n\times k} \),
we can extend the Sherman-Morrison formula to the Woodbury matrix identity:
\begin{theorem}{Woodbury Matrix Identity}
    If \( A\in \mathbb{R}^{n\times n} \) is an invertible matrix, 
    \(C\in \mathbb{R}^{k\times k}\) is also an invertible matrix, 
    and \( U, V\in \mathbb{R}^{n\times k} \) are matrices, 
    then \( A + UCV^T \) is invertible if and only if \( C^{-1} + V^T A^{-1} U \) is invertible. 
    In this case, the inverse of \( A + UCV^T \) is given by:
    \[
    \left(A + UCV^T\right)^{-1} = A^{-1} - A^{-1} U \left(C^{-1} + V^T A^{-1} U\right)^{-1} V^T A^{-1}.
    \]
\end{theorem}

Without loss of generality, let \( A, C \) be conformable identity matrices,
i.e., \( A = E_n \) and \( C = E_k \). Then the Woodbury matrix identity simplifies to:
\[
\left(E_n + UV^T\right)^{-1} = E_n - U \left(E_k + V^T U\right)^{-1} V^T.
\]
This form is particularly useful in applications involving low-rank updates to identity matrices.


\chapter{Linear Spaces}
\section{Linear Spaces and Their Bases}

\begin{leftbarTitle}{Basis Transformation and Coordinate Transformation}\end{leftbarTitle}


\section{Subspaces}
\begin{leftbarTitle}{Intersection and Sum of Subspaces}\end{leftbarTitle}
\begin{definition}{Intersection and Sum of Subspaces}
    Let \( V \) be a linear space over field \( F \), 
    and let \( V_1, V_2 \subseteq V \) be two subspaces of \( V \).
    The \textbf{intersection} of \( V_1 \) and \( V_2 \) is defined as:
    \[
    V_1 \cap V_2 = \{ \alpha | \alpha \in V_1 \text{ and } \alpha \in V_2 \}.
    \]
    The \textbf{sum} of \( V_1 \) and \( V_2 \) is defined as:
    \[
    V_1 + V_2 = \{ \alpha | \alpha = \alpha_1 + \alpha_2, 
    \alpha_1 \in V_1, \alpha_2 \in V_2 \}.
    \]
\end{definition}

If \( V_1 \) and \( V_2 \) are two subspaces of \( V \), 
then both their intersection \( V_1 \cap V_2 \) and their sum \( V_1 + V_2 \) are also subspaces of \( V \).
Simultaneously, \( V_{1}+V_{2} \) is the smallest subspace of \( V \) 
that contains both \( V_1 \) and \( V_2 \) (\(V_{1}\cup V_{2}\)).

\begin{leftbarTitle}{Dimension Formula}\end{leftbarTitle}
\begin{proposition}
    In a finite-dimensional linear space \( V \),
    \[
    L(\alpha_1, \alpha_2, \dots, \alpha_s) + L(\beta_1, \beta_2, \dots, \beta_t) = 
    L(\alpha_1, \alpha_2, \dots, \alpha_s, \beta_1, \beta_2, \dots, \beta_t).
    \]
\end{proposition}

\begin{theorem}{Dimension Formula}\label{thm:dimension_formula}
    Let \( V \) be a finite-dimensional linear space over field \( F \), 
    and let \( V_1, V_2 \subseteq V \) be two subspaces of \( V \). Then:
    \[
    \dim(V_1) + \dim(V_2) = \dim(V_1 + V_2) + \dim(V_1 \cap V_2).
    \]
    
\end{theorem}

\begin{leftbarTitle}{Direct Sum of Subspaces}\end{leftbarTitle}
\begin{definition}{Direct Sum of Subspaces}
    Let \( V \) be a linear space over field \( F \), 
    and let \( V_1, V_2 \subseteq V \) be two subspaces of \( V \).
    If any vector \( \alpha \in V_1 + V_2 \) can be uniquely expressed as:
    \[
    \alpha = \alpha_1 + \alpha_2, \quad \alpha_1 \in V_1, \alpha_2 \in V_2,
    \]
    then \( V_1 + V_2 \) is called the \textbf{direct sum} of \( V_1 \) and \( V_2 \),
    denoted as \( V_1 \oplus V_2 \).
\end{definition}

\begin{proposition}
    The necessary and sufficient condition for \( V_1 + V_2 \) to be a direct sum is:
    \begin{enumerate}
        \item \(0 = 0 + 0\) (the zero vector can only be expressed as the sum of two zero vectors); 
        \item \( V_1 \cap V_2 = \{0\} \) (the intersection of the two subspaces is only the zero vector);
        \item \(\operatorname{dim}(V_{1}+V_{2})=\operatorname{dim}(V_{1})+\operatorname{dim}(V_{2})\),
            or equivalently, \(\operatorname{dim}(V_{1} \cap V_{2})=0\).
        \item Any bases of \( V_1 \) and \( V_2 \) together form a basis of \( V_1 + V_2 \).
    \end{enumerate}
\end{proposition}




\section{Isomorphisms}

\section{Quotient Spaces}

\section{Special Linear Spaces}
\begin{leftbarTitle}{Commutative Spaces}\end{leftbarTitle}
Let \( A \) is an \( n \)-order matrix over number field \( P \).
It is obvious that the set of all matrices satisfying \( AX = XA \) is a subspace of \( M_{n \times n}(P) \),
denoted as \( C(A) \).

\begin{property}
    \begin{enumerate}
        \item For any \( n \)-order matrix \( A, B \), 
            if \( A \) is similar to \( B \) (\(A \sim B\)), 
            then \( C(A) \) is isomorphic to \( C(B) \) (\(C(A) \cong C(B)\)),
            further \( \dim(C(A)) = \dim(C(B)) \).
    \end{enumerate}    
\end{property}

\begin{example}
    Find the dimension and a basis of the \( C(A) \) where:
    \begin{enumerate}[label=(\roman*)]
        \item \(A\) is \(3\)-order matrix over number field \(P\), and \(A\) has three distinct eigenvalues.
        \item 
            \[
            A = 
            \begin{pmatrix} 1 & 0 & 4 \\ 0 & 1 & 2 \\ 0 & 1 & 2 \\\end{pmatrix}
            \]
    \end{enumerate}
\end{example}

\chapter{Linear Mappings}
\section{Linear Mappings and Their Computation}
\begin{leftbarTitle}{Definition of Linear Mappings}\end{leftbarTitle}
\begin{definition}{Linear Mapping}
    Let \( V \) and \( V' \) be linear spaces over field \( F \).
    A mapping \( \mathcal{A}: V \to V' \) is called a \textbf{linear mapping} if:
    \[
    \mathcal{A}(\alpha + \beta) = \mathcal{A}\alpha + \mathcal{A}\beta, \quad \forall \alpha, \beta \in V,
    \]
    and
    \[
    \mathcal{A}(k\alpha) = k\mathcal{A}\alpha, \quad \forall \alpha \in V, k \in F.
    \]

    If \( V = V' \), then \( \mathcal{A} \) is called a \textbf{linear transformation} of \( V \).
    The linear mapping from \( V \) over field \( F \) to \(F\) is called a \textbf{linear function} on \( V \).
\end{definition}

\begin{remark}
    The field naturally constitutes a one-dimensional linear space over itself, 
    so a linear function is a special case of a linear mapping.
\end{remark}




\begin{leftbarTitle}{Existence and Uniqueness of Linear Mappings}\end{leftbarTitle}

\begin{leftbarTitle}{Operations of Linear Mappings}\end{leftbarTitle}
In the following, all linear spaces are over the field \(F\).
\begin{description}
    \item[Addition] 
        Let \(\mathcal{A}, \mathcal{B} \in \mathrm{Hom}(V,V')\). 
        The sum of \(\mathcal{A}\) and \(\mathcal{B}\), denoted \(\mathcal{A+B}\), is defined by:
        \[
        (\mathcal{A+B})(\alpha) = \mathcal{A}(\alpha) + \mathcal{B}(\alpha), \quad \forall \alpha \in V.
        \]
        The sum of linear maps is still a linear map, i.e., \(\mathcal{A+B} \in \mathrm{Hom}(V,V')\).
        \begin{enumerate}
            \item \textbf{Commutativity:} \(\mathcal{A+B = B+A}\),
            \item \textbf{Associativity:} \(\mathcal{(A+B)+C = A+(B+C)}\),
            \item \textbf{Additive Identity:} The zero map \(\mathcal{0}\) satisfies \(\mathcal{A+0 = A}\),
            \item \textbf{Additive Inverse:} For every \(\mathcal{A}\), the additive inverse \(-\mathcal{A}\) 
                satisfies \(\mathcal{A+(-A) = 0}\).
        \end{enumerate}

    \item[Scalar Multiplication] 
        Let \(k \in F\) and \(\mathcal{A} \in \mathrm{Hom}(V,V')\). The scalar product \(k \mathcal{A}\) is defined by:
        \[
        (k\mathcal{A})(\alpha) = k \mathcal{A}(\alpha), \quad \forall \alpha \in V.
        \]
        Scalar multiplication of a linear map is still a linear map, i.e., \(k \mathcal{A} \in \mathrm{Hom}(V,V')\).
        \begin{enumerate}
            \item \textbf{Unit Identity:} \(1 \cdot \mathcal{A} = \mathcal{A}\),
            \item \textbf{Associativity of Scalars:} \((kl)\mathcal{A} = k(l\mathcal{A})\),
            \item \textbf{Distributive Properties:}
            \begin{align*}
                k(\mathcal{A+B}) &= k\mathcal{A} + k\mathcal{B}, \\
                (k+l)\mathcal{A} &= k\mathcal{A} + l\mathcal{A}.
            \end{align*}
        \end{enumerate}
    \item[Multiplication (Composition of Linear Maps)]
        Let \(\mathcal{A} \in \mathrm{Hom}(V',W)\) and \(\mathcal{B} \in \mathrm{Hom}(U,V')\). 
        The product (composition) of \(\mathcal{A}\) and \(\mathcal{B}\), denoted \(\mathcal{AB}\), is defined by:
        \[
        (\mathcal{AB})(\alpha) = \mathcal{A}(\mathcal{B}(\alpha)), \quad \forall \alpha \in U.
        \]
        The composition of linear maps is still a linear map, i.e., \(\mathcal{AB} \in \mathrm{Hom}(U,W)\).
        \begin{enumerate}
            \item \textbf{Associativity of Composition:} \((\mathcal{AB})\mathcal{C} = \mathcal{A}(\mathcal{BC})\),
            \item \textbf{Distributive Properties:}
            \begin{align*}
                \mathcal{A}(\mathcal{B+C}) &= \mathcal{AB + AC}, \\
                (\mathcal{A+B})\mathcal{C} &= \mathcal{AC+BC},
            \end{align*}
            \item \textbf{Non-Commutativity:} In general, \(\mathcal{AB} \neq \mathcal{BA}\),
            \item \textbf{Identity Transformation:} The identity transformation \(\mathcal{E}\) satisfies:
            \[
            \mathcal{EA} = \mathcal{AE} = \mathcal{A}, \quad \forall \mathcal{A} \in \mathrm{Hom}(V).
            \]
        \end{enumerate}
\end{description}


\vspace{0.7cm}
Let \(V\) and \(V'\) be vector spaces over the field \(F\).
\begin{enumerate}
    \item The collection of all linear mappings from \(V\) to \(V'\), denoted by \(\mathrm{Hom}(V,V')\), 
        under the addition and scalar multiplication defined above, forms a linear space over the field \(F\).
    
    \item The collection of all linear transformations on \(V\), denoted by \(\mathrm{Hom}(V)\), 
        under the addition and composition of maps defined above, forms a unital ring. 
        In this case, it is referred to as the \textbf{endomorphism ring} and is denoted by \(\mathrm{End}(V)\).
\end{enumerate}


\begin{definition}{Algebra over a Field}
    Let \(V\) be a linear space over the field \(F\) 
    equipped with an additional binary operation from \(V\times V\) to \(V\), denoted by \(\cdot\).
    Then \(V\) is called an \textbf{algebra over the field \(F\)} if
    (forall \(\alpha, \beta, \gamma \in V\) and \(k, l \in F\)):
    \begin{description}
        \item [Right Distributivity] \((\alpha + \beta) \cdot \gamma = \alpha \cdot \gamma + \beta \cdot \gamma\);
        \item [Left Distributivity] \(\gamma \cdot (\alpha + \beta) = \gamma \cdot \alpha + \gamma \cdot \beta\);
        \item [Compatibility with Scalar Multiplication] \((k\alpha)\cdot(l \beta) = kl(\alpha \cdot \beta)\).
    \end{description}
\end{definition}
\begin{remark}
    The left and right distributivity conditions can be combined into a single condition:
    \(V\) forms a unital ring under the addition and scalar multiplication.
\end{remark}

It is evident that \(\mathrm{Hom}(V)\) is an algebra over the field \(F\).


\section{Kernel and Image of Linear Mappings}
\begin{definition}{Kernel and Image of Linear Mappings}
    Let \( V \) and \( V' \) be linear spaces over field \( F \), 
    and let \( \mathcal{A}: V \to V' \) be a linear mapping. 
    The \textbf{kernel} of \( \mathcal{A} \) is defined as:
    \[
    \operatorname{Ker}(\mathcal{A}) = \{ \alpha | \alpha \in V, \mathcal{A}(\alpha) = 0 \}.
    \]
    The \textbf{image} of \( \mathcal{A} \) is defined as:
    \[
    \operatorname{Im}(\mathcal{A}) = \{ \beta | \beta = \mathcal{A}(\alpha), 
    \alpha \in V, \beta \in V' \}.
    \]
    They can be also denoted as \( \mathcal{A}^{-1}(0) \) and \( \mathcal{A}(V) \) respectively.

    The rank of \( \operatorname{Im}(\mathcal{A} \) is called the \textbf{rank} of \( \mathcal{A} \);
    the dimension of \( \operatorname{Ker}\mathcal{A} \) is called the \textbf{nullity} of \( \mathcal{A} \).
\end{definition}

\begin{property}
    
\end{property}


\begin{theorem}{Rank-Nullity Theorem}
    Let \( V \) and \( V' \) be finite-dimensional linear spaces over field \( F \), 
    and let \( \mathcal{A}: V \to V' \) be a linear mapping. Then:
    \[
    \dim(\operatorname{Ker}\mathcal{A}) + \dim(\operatorname{Im}\mathcal{A}) = \dim(V).
    \]
\end{theorem}


\begin{definition}{Cokernel}
    Let \( V \) and \( V' \) be linear spaces over field \( F \), 
    and let \( \mathcal{A}: V \to V' \) be a linear mapping. 
    The \textbf{cokernel} of \( \mathcal{A} \) is defined as the quotient space:
    \[
    \operatorname{Coker}(\mathcal{A}) = V' / \operatorname{Im}(\mathcal{A}),
    \]
    denoted as \( \operatorname{Coker}(\mathcal{A}) \).
\end{definition}

\section{Matrix Representation of Linear Mappings}
Let \( V \) and \( V' \) be finite-dimensional linear spaces over field \( F \), 
\(\operatorname{dim}(V) = n\) and \(\operatorname{dim}(V') = m\),
and let \( \mathcal{A}: V \to V' \) be a linear mapping.
Let \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \) be a basis of \( V \),
and \( \{\eta_1, \eta_2, \dots, \eta_m\} \) be a basis of \( V' \).
Then the image of each basis vector of \( V \) under \( \mathcal{A} \) can be expressed as the basis vectors of \( V' \):
\[
\begin{cases} 
    \mathcal{A}(\varepsilon_1) = a_{11}\eta_{1}+a_{21}\eta_{2}+\cdots+a_{m1}\eta_{m},   \\ 
    \mathcal{A}(\varepsilon_i) = a_{12}\eta_{1}+a_{22}\eta_{2}+\cdots+a_{m2}\eta_{m},   \\
    \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\vdots \\
    \mathcal{A}(\varepsilon_n) = a_{1n}\eta_{1}+a_{2n}\eta_{2}+\cdots+a_{mn}\eta_{m}.
\end{cases}
\]
It can be expressed in matrix form as:
\[
\mathcal{A}(\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n) = 
(\mathcal{A}\varepsilon_1, \mathcal{A}\varepsilon_2, \dots, \mathcal{A}\varepsilon_n) =
(\eta_1, \eta_2, \dots, \eta_m)A,
\]
where
\[A = (a_{ij})_{m \times n} =
\begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}.
\]
The matrix \( A \) is called the \textbf{matrix representation} of the linear mapping \( \mathcal{A} \)
under the bases \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \) of \( V \) 
and \( \{\eta_1, \eta_2, \dots, \eta_m\} \) of \( V' \).

\vspace{0.7cm}


\section{Special Linear Transformations}
Some common linear transformations include:
\begin{description}
    \item[Identity Transformation] The identity transformation \( \mathcal{E} \) on \( V \) is defined by:
        \[
        \mathcal{E}(\alpha) = \alpha, \quad \forall \alpha \in V.
        \]

    \item[Zero Transformation] The zero transformation \( \mathcal{O} \) from \( V \) to \( V' \) is defined by:
    \[
    \mathcal{O}(\alpha) = 0, \quad \forall \alpha \in V.
    \]

    \item[Scalar Transformation] For a fixed scalar \( k \in F \), 
        the scalar transformation \( \mathcal{K} \) on \( V \) is defined by:
        \[
        \mathcal{K}(\alpha) = k\alpha, \quad \forall \alpha \in V.
        \]
        When \( k = 1 \), it is the identity transformation; 
        when \( k = 0 \), it is the zero transformation. 

    \item[Spin Axis Transformation] 
        The vectors on a plane form a two-dimensional linear space over real number field \( \mathbb{R} \).
        Rotate the plane counterclockwise by an angle \( \theta \) around the origin, 
        then the transformation of any vector on the plane is a linear transformation,
        denoted by \( \mathcal{P}_{\theta}\).
        Similarly, in three-dimensional space, rotating around a fixed axis by an angle \( \theta \) 
        also defines a linear transformation.
\end{description}

\vspace{0.7cm}
And some special linear transformations are given as follows:
\begin{definition}{Idempotent Transformation}
    A linear transformation \( \mathcal{A} \) on \( V \) is called an \textbf{idempotent transformation} if:
    \[
    \mathcal{A}^2 = \mathcal{A}.
    \]
\end{definition}

Two linear transformations \( \mathcal{A} \) and \( \mathcal{B} \) on \( V \) are called \textbf{orthogonal} if:
\[
\mathcal{A} \mathcal{B} = \mathcal{B} \mathcal{A} = \mathcal{O}.
\]

With the definitions above, we can define the projection transformation as follows:
\begin{definition}{Projection Transformation}
    Let \( V \) be a linear space over field \( F \),
    and let \( U, W \subseteq V \) be two subspaces of \( V \) such that \( V = U \oplus W \).
    For all \( \alpha \in V \), there exist unique \( \alpha_1 \in U \) and \( \alpha_2 \in W \) such that:
    \[
    \alpha = \alpha_1 + \alpha_2.
    \]
    Let
    \[ 
        \mathcal{P}_{U}: 
        \begin{aligned}
        &V \to V \\
        &\alpha \mapsto \alpha_1
        \end{aligned}
    \] 
    then \( \mathcal{P}_{U} \) is called the \textbf{projection} onto \( U \) along \( W \),
    which is a linear transformation on \( V \).
    Similarly, the projection onto \( W \) along \( U \) can be defined.
\end{definition}
\( \mathcal{P}_{U}\) satisfies and uniquely satisfies:
\[
\mathcal{P}_{U}( \alpha ) = 
\begin{cases} 
    \alpha, & \alpha \in U, \\ 
    0, & \alpha \in W .
\end{cases}
\]



\section{Linear Functions and Dual Spaces}



\chapter{Diagonalization}
\section{Similarity of Matrices}
\begin{definition}{Similar Matrices}
    Let \( A, B \in F^{n \times n} \) be two square matrices over field \( F \).
    If there exists an invertible matrix \( P \in F^{n \times n} \) such that:
    \[
    B = P^{-1} A P,
    \]
    then \( A \) is said to be \textbf{similar} to \( B \), denoted as \( A \sim B \),
    and \( B \) is called a \textbf{similarity transformation} of \( A \).
\end{definition}

\begin{definition}{Trace}
    Let \( A = (a_{ij})_{n \times n} \in F^{n \times n} \) be a square matrix over field \( F \).
    The sum of the diagonal elements of \( A \):
    \[
    \operatorname{tr}(A) = a_{11} + a_{22} + \cdots + a_{nn},
    \]
    is called the \textbf{trace} of \( A \).
\end{definition}

Obviously, similarity is an equivalence relation on the set of square matrices of the same order,
i.e, it satisfies reflexivity, symmetry, and transitivity.

It is easy to verify that the arithmetic properties of traces hold, for any \( A, B \in F^{n \times n}, k \in F \):
\begin{gather*}
    \operatorname{tr}(A + B) = \operatorname{tr}(A) + \operatorname{tr}(B), \\
    \operatorname{tr}(kA) = k \operatorname{tr}(A), \\
    \operatorname{tr}(AB) = \operatorname{tr}(BA).
\end{gather*}
From the third property, it can be derived that the trace of an \(n\times n\) matrix is a commutative quantity 
extracted from the non-commutativity of matrix multiplication.

In the meanwhile, we have the arithmetic properties of similarity, if \(B_{1} = P^{-1} A_{1} P, B_{2} = P^{-1} A_{2} P\):
\begin{gather*}
    B_{1} + B_{2} = P^{-1}(A_{1} + A_{2})P, \\
    B_{1} B_{2} = P^{-1}(A_{1} A_{2})P, \\
    B_{1}^{k} = P^{-1} A_{1}^{k} P, \quad k = 1, 2, \dots
\end{gather*}
The third property is called the \textbf{similarity invariance of matrix powers}.
From it, we can derive some important applications:
\begin{description}
    \item[Eigenvalue invariance] Matrix similarity guarantees the preservation of eigenvalues. 
        Therefore, if \(A\) and \(B\) are similar matrices, then the eigenvalues of \(A^k\) and \(B^k\) are also identical. 
        For any eigenvalue \( \lambda \): If \( \lambda \) is an eigenvalue of \( A \), 
        then \( \lambda^k \) is an eigenvalue of \( A^k \) (the power corresponds to the operation).
    \item[The result of diagonalization is invariant] If a matrix \( A \) is diagonalizable, that is, 
        there exists an invertible matrix \( P \) and a diagonal matrix \( D \) 
        such that \( A = P^{-1} D P \), then: \( A^k = P^{-1} D^k P \). 
        In other words, calculating \( A^k \) after diagonalization becomes simple. 
        You only need to perform the power operation on the diagonal matrix \( D \), 
        and then transform the result back to the original basis by a similarity transformation.
    \item[The Convenience of Jordan Form] Even if a matrix \( A \) is not diagonalizable, 
        it can still be similar to a Jordan form matrix \( J \). 
        In this case: \( A^k = P^{-1} J^k P \), 
        where the calculation of \( J^k \) can still be done block by block on the Jordan blocks. 
        This property is extremely important when dealing with powers of non-diagonalizable matrices.
    \item[Generalization of Matrix Functions] The power relations of matrices can be used to derive operations of matrix functions. 
        For example, the exponential function of a matrix is defined as \(e^{A} = \sum_{k=0}^{\infty} \frac{A^{k}}{k!} \). 
        If \( B = P^{-1}AP \), then: \( e^{B} = P^{-1}e^{A}P \). 
        Similarly, other matrix functions (such as logarithm, power functions, etc.) also possess similar properties.
\end{description}

\vspace{0.7cm}
In summary, the determinant, rank, and trace of a matrix are invariants\footnote{
    And also include the characteristic polynomial, eigenvalues, and eigenvectors in next section.
} under similarity relations, 
and are collectively referred to as \textbf{similarity invariants}.


\section{Eigenvectors and Eigenvalues}
\begin{definition}{Eigenvalues and Eigenvectors}
    Let \( V \) be a linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    A non-zero vector \( \xi \in V \) is called an \textbf{eigenvector} of \( \mathcal{A} \)
    if there exists a scalar \( \lambda \in F \) such that:
    \[
    \mathcal{A}(\xi) = \lambda \xi.
    \]
    The scalar \( \lambda \) is called the \textbf{eigenvalue} corresponding to the eigenvector \( \xi \).
    
    From the perspective of matrices,
    let \( A \in F^{n \times n} \) be a square matrix over field \( F \).
    A non-zero vector \( x \in F^n \) is called an \textbf{eigenvector} of \( A \)
    if there exists a scalar \( \lambda \in F \) such that:
    \[
    A x = \lambda x.
    \]
    The scalar \( \lambda \) is called the \textbf{eigenvalue} corresponding to the eigenvector \( x \).
\end{definition}

\begin{definition}{Eigenpolynomial}
    Let \( V \) be a linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    The polynomial
    \[
    f(\lambda) = |\lambda E - A| = \begin{vmatrix}
        \lambda - a_{11} & -a_{12} & \cdots & -a_{1n} \\
        -a_{21} & \lambda - a_{22} & \cdots & -a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        -a_{n1} & -a_{n2} & \cdots & \lambda - a_{nn}
    \end{vmatrix}
    \]
    is called the \textbf{eigenpolynomial} (or characteristic polynomial) of \( A \),
    where \( A = (a_{ij})_{n\times n} \) is the matrix representation of \( \mathcal{A} \)
    under some basis of \( V \), and \( E \) is the identity matrix.
\end{definition}

\begin{remark}
    For example, in \(P[x]_{n}\quad (n>1)\), the eigenpolynomial of linear transformation \( \mathcal{D} \) (derivation)
    is \( f(\lambda) = \lambda^n \).
\end{remark}

The roots of the eigenpolynomial $\lvert \lambda E-A \rvert$ correspond to the eigenvalues of $A$; 
the non-zero solutions to the homogeneous system of equations $(\lambda_{0}E-A)x=0$ 
correspond to the eigenvectors belonging to the eigenvalue $\lambda_{0}$.
It is easy to verify that the eigenpolynomial as well as the eigenvalues, eigenvectors is invariant under similarity relations.

\vspace{0.7cm}
The following proposition summarizes the properties of the eigenpolynomial:
\begin{proposition}
    Let \( A \in F^{n \times n} \), the eigenpolynomial of \( A \) is \( f(\lambda) = |\lambda E - A| \).
    Then \( f(\lambda) \) is a monic polynomial of degree \( n \):
    \[
    f(\lambda) = \lambda^n - \operatorname{tr}(A)\lambda^{n-1} + \cdots + (-1)^n |A|,
    \]
    where the coefficient of \( \lambda^{n-k}\quad(1 \leqslant k < n) \) is 
    the sum of all \( k \)-order principal minors of \( A \) multiplied by \( (-1)^k \).  
\end{proposition}

\begin{leftbarTitle}{Multiplicity and Eigen-Subspaces}\end{leftbarTitle}
\begin{definition}{Eigen-Subspace}
    Let \( V \) be a linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    For an eigenvalue \( \lambda \) of \( \mathcal{A} \), 
    the set of all eigenvectors corresponding to \( \lambda \) \underline{along with the zero vector}:
    \[
    V_{\lambda} = \{ \alpha | \alpha \in V, \mathcal{A}(\alpha) = \lambda \alpha \} = 
    \operatorname{Ker}(\mathcal{A} - \lambda \mathcal{E}),
    \]
    is called the \textbf{eigen-subspace} (or eigenspace) corresponding to the eigenvalue \( \lambda \).
    It is evident that \( V_{\lambda} \) is a subspace of \( V \)
    and solution space of the equation system \( ( \lambda E - \mathcal{A})X = 0 \).
\end{definition}

\begin{definition}{Geometric and Algebraic Multiplicity of Eigenvalues}
    Let \( V \) be a linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    For an eigenvalue \( \lambda \) of \( \mathcal{A} \):
    \begin{itemize}
        \item The dimension of the eigen-subspace \( V_{\lambda} = \operatorname{Ker}(\mathcal{A} - \lambda \mathcal{E}) \)
            is called the \textbf{geometric multiplicity} of the eigenvalue \( \lambda \).
        \item The multiplicity of \( \lambda \) as a root of the eigenpolynomial \( f(\lambda) = |\lambda E - A| \)
            is called the \textbf{algebraic multiplicity} of the eigenvalue \( \lambda \).
    \end{itemize}
\end{definition}

\begin{property}
    For an eigenvalue \( \lambda \) of \( \mathcal{A} \),
    let \( g(\lambda) \) and \( a(\lambda) \) be the geometric and algebraic multiplicities of \( \lambda \), respectively.
    Then:
    \[
    1 \leqslant g(\lambda) \leqslant a(\lambda) \leqslant n.
    \]
\end{property}

\begin{leftbarTitle}{Estimation of Eigenvalues}\end{leftbarTitle}
\begin{theorem}{Gershgorin Circle Theorem}
    Let \( A = (a_{ij})_{n \times n} \in C^{n \times n} \) be a square matrix over complex number field \( C \).
    For each \( i = 1, 2, \dots, n \), define
    \[
    R_i = \sum_{\substack{j=1 \\ j \neq i}}^{n} |a_{ij}|,
    C_i = \sum_{\substack{j=1 \\ j \neq i}}^{n} |a_{ji}|,
    \]
    and
    \[
    S_{i} = \{ z \in \mathbb{C} | |z - a_{ii}| \leqslant R_i \},
    G_{i} = \{ z \in \mathbb{C} | |z - a_{ii}| \leqslant C_i \},
    \]
    as Gershgorin row circle and column circle, respectively.
    Then all eigenvalues of \( A \) lie within the union of these \( n \) circles:
    \[
    \bigcup_{i=1}^{n} S_{i} \cup G_{i}.
    \]
\end{theorem}


\section{Space Decomposition}

\begin{leftbarTitle}{Invariant Subspace}\end{leftbarTitle}
\begin{definition}{Invariant Subspace}
    Let \( V \) be a linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    A subspace \( W \subseteq V \) is called an \textbf{invariant subspace} of \( V \)
    under the linear transformation \( \mathcal{A} \) if:
    \[
    \mathcal{A}(\alpha) \subseteq W, \quad \forall \alpha \in W,
    \]
    simplified as \( \mathcal{A} \)-subspace.
\end{definition}

\begin{property}
    \begin{enumerate}
        \item The zero subspace \( \{0\} \) and the entire space \( V \) are invariant subspaces of \( V \) 
            under any linear transformation \( \mathcal{A} \), which are called trivial invariant subspaces. 
        \item Let \( \mathcal{A}, \mathcal{B} \in \operatorname{Hom}(V) \). 
            If \( \mathcal{A}, \mathcal{B} \) is commutative (\( \mathcal{AB} = \mathcal{BA} \)), 
            then \( \operatorname{Ker}(\mathcal{A}) \), \( \operatorname{Im}(\mathcal{A}) \) and 
            eigenspace of \( \mathcal{A} \) are both invariant subspaces of \( V \) under \( \mathcal{B} \).
        \item Any subspace of scalar transformation is an invariant subspace.
        \item The intersection and sum of any two invariant subspaces are also invariant subspaces.
    \end{enumerate}
\end{property}

\begin{proposition}
    Let \( V \) be a linear space over field \( F \), and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    \( W = \langle \xi \rangle \quad (\xi\neq 0)\), the subspace of \( V \) is \( \mathcal{A} \)-subspace if and only if 
    \( \xi \) is an eigenvector of \( \mathcal{A} \).
\end{proposition}

\begin{theorem}
    Let \( V \) be a \(n\)-dimensional linear space over field \( F \), and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    \begin{enumerate}
        \item Let \( W \) be a nontrivial \( \mathcal{A} \)-subspace,
            take a basis \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_r\} \) of \( W \),
            and extend it to a basis \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \) of \( V \).
            Then the matrix representation of \( \mathcal{A} \) under this basis takes the block upper triangular form:
            \[
            \begin{pmatrix}
                A & B \\
                O & C
            \end{pmatrix}
            \]
            where:
            \begin{itemize}
                \item \( A \) is the matrix representation of \( \mathcal{A} \) restricted to \( W \) (i.e., \( \mathcal{A}|_W \)) 
                    under the basis \(\varepsilon_1, \dots, \varepsilon_r\),
                \item \( C \) is the matrix representation of \( \widetilde{\mathcal{A}} \), 
                    the linear transformation on the quotient space \( V/W \) induced by \( \mathcal{A} \), 
                    under the basis \(\varepsilon_{r+1} + W, \dots, \varepsilon_n + W\).
            \end{itemize}
            Let the characteristic polynomials of \( \mathcal{A} \), \( \mathcal{A}|_W \), and \( \widetilde{\mathcal{A}} \) 
            be \( f(\lambda) \), \( f_1(\lambda) \), and \( f_2(\lambda) \), respectively. Then:
            \[ 
            f(\lambda) = f_1(\lambda) f_2(\lambda).
            \]
    
        \item Let the matrix representation of \( \mathcal{A} \) under a basis 
            \(\varepsilon_1, \varepsilon_2, \dots, \varepsilon_r, \varepsilon_{r+1}, \dots, \varepsilon_n\) 
            of \( V \) take the block upper triangular form:
            \[
            \mathcal{A} = 
            \begin{pmatrix}
            A & B \\
            O & C
            \end{pmatrix}.
            \]
            Define \( W = \langle \varepsilon_1, \dots, \varepsilon_r \rangle \). 
            Then \( W \) is an \( \mathcal{A} \)-subspace, 
            and the matrix representation of \( \mathcal{A}|_W \)
            under the basis \(\varepsilon_1, \dots, \varepsilon_r\) is given by \( A \).
        
        \item The matrix representation of \( \mathcal{A} \) under a basis of \( V \) 
            is a block diagonal form 
            \[
            \begin{pmatrix}
                A_1 & O & \cdots & O \\
                O & A_2 & \cdots & O \\
                \vdots & \vdots & \ddots & \vdots \\
                O & O & \cdots & A_s
            \end{pmatrix}
            \]
            if and only if \( V \) can be decomposed into a direct sum of nontrivial \( \mathcal{A} \)-subspaces (\(W_{i}\)):
            \[
            V = W_1 \oplus W_2 \oplus \dots \oplus W_s,
            \]
            then \( A_i \) represents the matrix form of \( \mathcal{A}|{W_i} \) under a basis of \( W_i \).
    \end{enumerate}
\end{theorem}

\begin{proposition}
    Let \( V \) be a linear space over field \( F \) (finite or infinite dimensional), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    On \(F[x]\), 
    \[
    f(x) = f_{1}(x)f_{2}(x)\cdots f_{s}(x),
    \]
    where \( f_{i}(x) \) are pairwise coprime polynomials.
    Then
    \[
    \operatorname{Ker}f(\mathcal{A}) 
    = \operatorname{Ker}f_{1}(\mathcal{A}) \oplus 
    \operatorname{Ker}f_{2}(\mathcal{A}) \oplus \cdots \oplus \operatorname{Ker}f_{s}(\mathcal{A}).
    \]
\end{proposition}



\begin{leftbarTitle}{Hamilton-Cayley Theorem}\end{leftbarTitle}
\begin{theorem}{Hamilton-Cayley Theorem}
    Let \( V \) be an \( n \)-dimensional linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    Let \( A \) be the matrix representation of \( \mathcal{A} \) under some basis of \( V \),
    and let \( f(\lambda) = |\lambda E - A| \) be the characteristic polynomial of \( A \).
    Then:
    \[
    f(\mathcal{A}) = 0.
    \]
\end{theorem}

\begin{definition}{Root Subspaces}
\end{definition}


\section{Minimal Polynomial}
\begin{definition}{Minimal Polynomial}
    Let \( V \) be a linear space over field \( F \), 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    A polynomial \( f(\lambda) \) is called an \textbf{annihilating polynomial} of \( \mathcal{A} \) if:
    \[
    f(\mathcal{A}) = 0.
    \]

    The monic polynomial \( m(\lambda) \) of least degree such that:
    \[
    m(\mathcal{A}) = 0,
    \]
    is called the \textbf{minimal polynomial} of \( \mathcal{A} \).

    It is similar for a square matrix \( A \in F^{n \times n} \).
\end{definition}


\section{Diagonalization}
\begin{lemma}
    Eigenvectors corresponding to distinct eigenvalues are linearly independent.

    Furthermore, if a linear transformation \( \mathcal{A} \) on a linear space \( V \)
    has \( k \) distinct eigenvalues \( \lambda_1, \lambda_2, \dots, \lambda_k \),
    and \( \alpha_{i1}, \alpha_{i2}, \dots, \alpha_{ir_{i}} \) (\(i = 1, 2, \dots, k\)) 
    are the corresponding \underline{linearly independent} eigenvectors of \( \lambda_i \),
    then the set of vectors
    \[
    \{ \alpha_{ij} | i = 1, 2, \dots, k, j = 1, 2, \dots, r_i \}
    \]
    is linearly independent.
\end{lemma}

\begin{theorem}
    Let \( V \) be an \( n \)-dimensional linear space over field \( F \),
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    Then the following statements are equivalent:
    \begin{enumerate}
        \item \( \mathcal{A} \) is diagonalizable, i.e., there exists a basis of \( V \)
            such that the matrix representation of \( \mathcal{A} \) under this basis is a diagonal matrix;
        \item There are \( n \) linearly independent eigenvectors (\( \xi_{1}, \xi_{2}, \cdots, \xi_{n} \)) of \( \mathcal{A} \);
        \item For each eigenvalue of \( \mathcal{A} \), the geometric multiplicity equals the algebraic multiplicity;
        \item \( V = V_{\lambda_1} \oplus V_{\lambda_2} \oplus \cdots \oplus V_{\lambda_k} \),
            where \( \lambda_1, \lambda_2, \dots, \lambda_k \) are the \underline{distinct} eigenvalues of \( \mathcal{A} \).
        \item The minimal polynomial\footnote{
            Since the minimal polynomial is the monic polynomial with least degree 
            of all annihilating polynomials of \( \mathcal{A} \),
            the condition also holds for any annihilating polynomial of \( \mathcal{A} \).
        } of \( \mathcal{A} \) has not repeated roots, 
            i.e. it can be factored into \underline{distinct} linear factors over field \( F \):
            \[
            m(\lambda) = (\lambda - \lambda_1)(\lambda - \lambda_2) \cdots (\lambda - \lambda_k).
            \]
    \end{enumerate}
\end{theorem}

\section{Diagonalization of Special Matrices}
\begin{leftbarTitle}{Invertible Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Idempotent Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Nilpotent Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Convolution Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{\(1\) Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Circulant Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Triangular Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Frobenius Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Jordan Block}\end{leftbarTitle}


\chapter{Jordan Forms}
\section{Polynomial Matrices}

\section{Invariant Factors}

\section{Rational Canonical Form}
\begin{definition}{Companion Matrix}
    The Frobenius companion matrix of the monic polynomial
    \[
    p(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_1 x + a_0
    \]
    is the square matrix defined as follows:
    \[
    C(p) = \begin{pmatrix}
    0 & 0 & \cdots & 0 & -a_0 \\
    1 & 0 & \cdots & 0 & -a_1 \\
    0 & 1 & \cdots & 0 & -a_2 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \cdots & 1 & -a_{n-1}
    \end{pmatrix}.
    \]
\end{definition}

Some authors use the transpose of this matrix, \(C(p)^{T}\), 
which is more convenient for some purposes such as linear recurrence relations.

\(C(p)\) is defined from the coefficients of \(p(x)\), 
while the characteristic polynomial as well as the minimal polynomial of \(C(p)\) are equal to \(p(x)\). 
In this sense, the matrix \(C(p)\) and the polynomial \(p(x)\) are "companions".

\begin{property}
    \begin{enumerate}
        \item The characteristic polynomial and minimal polynomial of \( C(p) \) is \( p(x) \) (monic).
        \item  
        \item 
    \end{enumerate}
\end{property}

\begin{definition}{Frobenius Normal Form (Rational Canonical Form)}
    Let \( V \) be an \( n \)-dimensional linear space over field \( F \),
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    If there exists a basis of \( V \) such that the matrix representation of \( \mathcal{A} \) under this basis is:
    \[
    F = 
    \begin{pmatrix}
        C(p_1) & 0 & \cdots & 0 \\
        0 & C(p_2) & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & C(p_s)
    \end{pmatrix},
    \]
    where \( p_1, p_2, \dots, p_s \) are monic polynomials over field \( F \) such that
    \( p_1 | p_2 | \cdots | p_s \),
    then \( F \) is called the \textbf{Frobenius normal form} (or rational canonical form) of \( \mathcal{A} \).
\end{definition}


\section{Elementary Divisors}

\section{Jordan Canonical Form}

\chapter{Quadratic Forms}
\section{Quadratic Forms and Their Standard Forms}
\begin{definition}{Quadratic Form}
    Let \(P\) be a number field, a quadratic homogeneous polynomial in \( n \) variables over \( P \)\footnote{
        That is, the coefficients of the polynomial belong to the field \( P \).
    }:
    \begin{align*}
        f( x_{1}, x_{2}, \cdots, x_{n}) &= \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ij} x_{i} x_{j} \\
    &= a_{11}x_{1}^{2} + 2a_{12}x_{1}x_{2} + \cdots + 2a_{1n}x_{1}x_{n} + a_{22}x_{2}^{2} + \cdots + 2a_{2n}x_{2}x_{n} + \cdots + a_{nn}x_{n}^{2},
    \end{align*}
    is called a \textbf{quadratic form} in \( n \) variables over field \( P \).

    It can be expressed in matrix form as:
    \[
    f( x_{1}, x_{2}, \cdots, x_{n}) = X^{\mathrm{T}} A X,
    \]
    where 
    \[
    X = \begin{pmatrix}
        x_{1} \\
        x_{2} \\
        \vdots \\
        x_{n}
    \end{pmatrix}, \quad
    A = (a_{ij})_{n \times n}, \quad a_{ij} = a_{ji} \quad (1 \leqslant i, j \leqslant n).
    \]
\end{definition}
It is easy to verify that the matrix \( A \) of a quadratic form is symmetric.
\begin{note}
    In fact, for any square matrix \( B \)\footnote{
        For a skew-symmetric matrix \( S \) (\( S^{T} = -S \)),
        \[
        X^{T} S X = - (X^{T} S X)^{T} = - X^{T} S^{T} X = - X^{T} S X \implies X^{T} S X = 0.
        \]
    }, we have:
    \begin{align*}
        X^{T}BX &= X^{T}\left( \frac{B + B^{T}}{2} \right)X + X^{T}\left( \frac{B - B^{T}}{2} \right)X \\
        &= X^{T}\left( \frac{B + B^{T}}{2} \right)X + 0 \\
        &= X^{T}\left( \frac{B + B^{T}}{2} \right)X.
    \end{align*}
    It shows that any quadratic form can be represented by a symmetric matrix.
\end{note}




\section{Canonical Forms}
\section{Definite Quadratic Forms}
\begin{definition}{Positive Definite Quadratic Form}
    A real quadratic form \( f( x_{1}, x_{2}, \cdots, x_{n})=X^{\mathrm{T}}AX \) is called \textbf{positive definite} if:
    \[
    f( x_{1}, x_{2}, \cdots, x_{n}) > 0, \quad \forall X \neq 0.
    \]
    And \( A \) is called a \textbf{positive definite matrix}.
\end{definition}

\begin{theorem}{Sufficient and Necessary Condition for Positive Definiteness}
    A real quadratic form \( f( x_{1}, x_{2}, \cdots, x_{n})=X^{\mathrm{T}}AX \) is positive definite if and only if:
    \begin{enumerate}
        \item The positive inertia index of \( f \) is \( n \);
        \item \(A\) is congruent to the identity matrix \( E \);
        \item All eigenvalues of \( A \) are positive;
        \item All leading principal minors\footnote{
            The leading principal minors of a matrix are the determinants of 
            the top-left \( k \times k \) submatrices for \( k = 1, 2, \ldots, n \).
        } of \( A \) are positive.
    \end{enumerate}
    
\end{theorem}

\chapter{Inner Product Spaces}
\section{Bilinear Forms}
\begin{definition}{Bilinear Form}
    Let \( V \) be a linear space over field \( F \).
    A function \( f: V \times V \to F \) is called a \textbf{bilinear form} on \( V \) if:
    \begin{enumerate}
        \item For any fixed \( \beta \in V \), the function \( f(\cdot, \beta): V \to F \) defined by
            \( f(\alpha, \beta) \) is a linear function on \( V \);
        \item For any fixed \( \alpha \in V \), the function \( f(\alpha, \cdot): V \to F \) defined by
            \( f(\alpha, \beta) \) is a linear function on \( V \).
    \end{enumerate}
    
\end{definition}

\section{Real Inner Product Spaces}
\begin{definition}{Real Inner Product Space}
    A \textbf{real inner product space} is a real linear space \( V \) 
    equipped with a function \( (\cdot, \cdot): V \times V \to \mathbb{R} \) 
    satisfying the following properties:
    \begin{description}
        \item [Positivity] \( (\alpha, \alpha) \geq 0, \quad \forall \alpha \in V\),
            and \( (\alpha, \alpha) = 0 \) if and only if \( \alpha = 0 \);
        \item [Symmetry] \( (\alpha, \beta) = (\beta, \alpha), \quad \forall \alpha, \beta \in V\);
        \item [Linearity in the First Argument] 
            \( (k_1\alpha_1 + k_2\alpha_2, \beta) = k_1(\alpha_1, \beta) + k_2(\alpha_2, \beta), 
            \quad \forall k_1, k_2 \in \mathbb{R}, 
            \alpha_1, \alpha_2, \beta \in V. \)
    \end{description}
    The function \( (\cdot, \cdot) \) is called the (real) inner product on \( V \).

    Real inner product spaces with finite dimensions are called \textbf{Euclidean spaces}. 
\end{definition}

\section{Metric Matrices and Standard Orthonormal Bases}
\begin{definition}{Gram Matrix}
    Let \( V \) be a \(n\)-dimensional Euclidean space, 
    and let \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \) be a basis of \( V \).
    The matrix
    \[
    G = ((\varepsilon_i, \varepsilon_j))_{n \times n} =
    \begin{pmatrix}
        (\varepsilon_1, \varepsilon_1) & (\varepsilon_1, \varepsilon_2) & \cdots & (\varepsilon_1, \varepsilon_n) \\
        (\varepsilon_2, \varepsilon_1) & (\varepsilon_2, \varepsilon_2) & \cdots & (\varepsilon_2, \varepsilon_n) \\
        \vdots & \vdots & \ddots & \vdots \\
        (\varepsilon_n, \varepsilon_1) & (\varepsilon_n, \varepsilon_2) & \cdots & (\varepsilon_n, \varepsilon_n)
    \end{pmatrix}
    \]
    is called the \textbf{Gram matrix} (or metric matrix) of the inner product on \( V \)
    under the basis \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \).
\end{definition}

For all vectors
\begin{gather*}
    \alpha = x_{1}\varepsilon_{1}+x_{2}\varepsilon_{2}+\cdots+x_{n}\varepsilon_{n},  \\
    \beta = y_{1}\varepsilon_{1}+y_{2}\varepsilon_{2}+\cdots+y_{n}\varepsilon_{n} \in V ,
\end{gather*}
since \(( \alpha, \beta)=\sum_{i=1}^{n} \sum_{j=1}^{n}  (\varepsilon_i, \varepsilon_j)x_{i}y_{j} \), 
it can be expressed in matrix form as:
\[
( \alpha, \beta) = X^{\mathrm{T}} G Y, \quad
X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}, \quad
Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}.
\]


\section{Isomorphism of Real Inner Product Spaces}

\section{Orthogonal Completion and Orthogonal Projection}
\begin{leftbarTitle}{Orthogonal Completion}\end{leftbarTitle}

\begin{leftbarTitle}{Least Squares Method}\end{leftbarTitle}


\section{Orthogonal Transformations and Symmetric Transformations}
\begin{leftbarTitle}{Orthogonal Transformations}\end{leftbarTitle}
\begin{leftbarTitle}{Symmetric Transformations}\end{leftbarTitle}
\begin{definition}{Symmetric Transformation}
    Let \( V \) be a real inner product space, 
    and let \( \mathcal{A}\in \operatorname{Hom}(V) \).
    If
    \[
    (\mathcal{A}\alpha, \beta) = (\alpha, \mathcal{A}\beta), \quad \forall \alpha, \beta \in V,
    \]
    then \( \mathcal{A} \) is called a \textbf{symmetric transformation} on \( V \).
\end{definition}
Obviously, symmetric transformations are linear transformations.

\begin{lemma}
    \begin{enumerate}
        \item If \( A \) is real symmetric matrix, then \( A \) has and only has \( n \) real eigenvalues (counting multiplicities). 
        \item Let \(V\) be a \(n\)-dimensional Euclidean space, 
            then any linear transformation \( \mathcal{A}: V \to V \) is symmetric if and only if
            there exists a standard orthonormal basis of \( V \) such that 
            the matrix representation of \( \mathcal{A} \) under this basis is a symmetric matrix.
        \item If \( \mathcal{A} \) is a symmetric transformation on a real inner product space \( V \),
            then if \(W\) is an invariant subspace of \( V \) under \( \mathcal{A} \),
            then its orthogonal complement \( W^{\perp} \) is also an invariant subspace of \( V \) under \( \mathcal{A} \).
        \item If \(A\) is a symmetric transformation on a Euclidean space \( V \),
            then eigenvectors corresponding to distinct eigenvalues are definitely orthogonal.
    \end{enumerate}
\end{lemma}

\begin{theorem}{Orthogonal Diagonalization of Symmetric Transformations}
    Let \( V \) be a \( n \)-dimensional Euclidean space, 
    and let \( \mathcal{A}: V \to V \) be a symmetric transformation on \( V \).
    Then there exists a standard orthonormal basis of \( V \) such that 
    the matrix representation of \( \mathcal{A} \) under this basis is a diagonal matrix.
    
    From the perspective of matrices,
    let \( A \in \mathbb{R}^{n \times n} \) be a real symmetric matrix.
    Then there exists an orthogonal matrix \( T \) such that:
    \[
    T^{-1} A T = T^{\mathrm{T}} A T = \Lambda = 
        \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
        \end{pmatrix},
    \]
    where \( \lambda_1, \lambda_2, \dots, \lambda_n \) are the eigenvalues of \( A \).
\end{theorem}

\begin{note}
    According to the theorem above,
    for all real quadratic forms \( f(x_1, x_2, \dots, x_n) = X^{\mathrm{T}} A X \),
    there exists an orthogonal transformation \( X = T Y \) such that:
    \[
    f(x_1, x_2, \dots, x_n) = Y^{\mathrm{T}} \Lambda Y = \sum_{i=1}^{n} \lambda_i y_i^2.
    \]
\end{note}

\section{Unitary Spaces and Unitary Transformations}

\section{Symplectic Spaces}

\begin{thebibliography}{99} 
\bibitem{ch1} 丘维声. \emph{ 高等代数 (2nd edition) }, 清华大学出版社, 2019. 
\bibitem{ch2} 谢启鸿, 姚慕生, 吴泉水. \emph{ 高等代数学 (4th edition) }, 复旦大学出版社, 2022.
\bibitem{ch3} 王萼芳, 石生明. \emph{ 高等代数 (5th edition) }, 高等教育出版社, 2019.
\bibitem{ch4} 樊启斌. \emph{ 高等代数典型问题与方法 (1st edition) }, 高等教育出版社, 2021.
\bibitem{9} Wikipedia. \url{https://en.wikipedia.org/wiki/}.
\end{thebibliography}

\end{document}