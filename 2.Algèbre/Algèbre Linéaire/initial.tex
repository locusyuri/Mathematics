\documentclass[11pt]{../../TexTemplate/elegantbook} % 这里是文档类，默认使用 elegantbook

\title{Algèbre Linéaire} % 这里放置书名
% \subtitle{Subtitle} % 这里放置副标题

\author{CatMono} % 这里放置作者名
\date{July, 2025} % 这里放置日期
\version{0.1} % 这里放置版本号
% \institute{Elegant\LaTeX{} Program} % 这里放置机构名
% \bioinfo{Custom Key}{Custom Value} % 这里放置自定义信息

% \extrainfo{extra information} % 这里放置额外信息，将显示在最下方中央

\setcounter{tocdepth}{2} % 设置目录深度
\setcounter{secnumdepth}{2} % 设置章节编号深度


% \logo{logo-blue.png} % 这里放置封面logo，默认从figure目录下寻找
% \cover{LogiqueMathematique.png} % 这里放置封面图片，默认从figure目录下寻找

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170} % 自定义颜色
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect} % 保护命令参数不被 LaTeX 解析器过早处理，允许在某些特殊环境中使用脆弱命令（fragile commands）。
\usepackage{xeCJK} % 使用 xeCJK 包支持中文


% ===== 开始文档 =====
\begin{document}

\maketitle %生成文档的标题页，根据之前定义的标题信息（如标题、作者、日期等）自动创建一个格式化的标题页

% === 前言部分 ===
\frontmatter        % 开始前言，页码为 i, ii, iii...
\tableofcontents    % 目录 (页码: i, ii)
% \listoffigures      % 图表目录 (页码: iii)
% \listoftables       % 表格目录 (页码: iv)

\chapter{Preface}   % 前言章节（无编号，页码: v, vi...）
This is the preface of the book...

% \chapter{Acknowledgments}  % 致谢（无编号）
% I would like to thank...
% === 正文部分 ===
\mainmatter         % 开始正文，页码从 1 重新开始

\chapter{Determinants} % 这里放置章节标题
\section{Permutations} % 排列

\section{Determinant and Its Properties} % 行列式及其性质

\section{Expanding by Rows (Columns)} % 按行（列）展开
\begin{leftbarTitle}{Expanding by One Row}\end{leftbarTitle}

\begin{leftbarTitle}{Cramer's Rule}\end{leftbarTitle}

\begin{leftbarTitle}{Expanding by \(k\) Rows}\end{leftbarTitle}

\section{Special Determinants} % 这里放置小节标题
\begin{definition}{Vandermonde Determinant}
    The Vandermonde determinant is defined as
    \[
    V_n = \begin{vmatrix}
    1 & 1 & 1 & \cdots & 1 \\
    x_1 & x_2 & x_3 & \cdots & x_n \\
    x_1^2 & x_2^2 & x_3^2 & \cdots & x_n^2 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_1^{n-1} & x_2^{n-1} & x_3^{n-1} & \cdots & x_n^{n-1}
    \end{vmatrix}
    \]
    where \( x_1, x_2, \ldots, x_n \) are distinct variables.
\end{definition}

The value of the Vandermonde determinant is given by
\[
V_n = \prod_{1 \leq i < j \leq n} (x_j - x_i).
\]  

\begin{definition}{Arrow Determinant}
    The Arrow determinant (\(\nwarrow\)) is defined as
    \[
    A_n = \begin{vmatrix}
    a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
    a_{21} & a_{22} & 0 & \cdots & 0 \\
    a_{31} & 0 & a_{33} & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & 0 & 0 & \cdots & a_{nn}
    \end{vmatrix}.
    \]

    The value of the Arrow determinant is given by
    \[
    A_n = \left( a_{11}- \sum_{k=2}^{n} \frac{a_{1k}a_{k1}}{a_{kk}}  \right)\prod_{k=2}^{n} a_{kk}.
    \]
\end{definition}

From the first column sequentially, subtract \( \frac{a_{21}}{a_{22}} \) times the second column, \(\cdots\), 
\( \frac{a_{n1}}{a_{nn}} \) times the \( n \)-th column, so that the first column becomes:
\[
\begin{bmatrix}
a_{11} - \sum\limits_{k=2}^n \frac{a_{1k}a_{k1}}{a_{kk}} &
0 &
0 &
\vdots &
0
\end{bmatrix}^{\mathrm{T}}.
\]
Then expand along the first column.

\begin{definition}{Two-Triangular Determinant}
    If the determinant satisfies 
    \[ 
    a_{ij} = \begin{cases} 
    a, & i < j, \\ 
    x_{i}, & i = j, \\ 
    b, & i > j,
    \end{cases} 
    \], 
    then \( D_{n} \) is called a two-triangular determinant.
\end{definition}

The value of the two-triangular determinant is given by
\[
\begin{vmatrix}
x_{1} & a & a & \dots & a \\
b & x_{2} & a & \dots & a \\
b & b & x_{3} & \dots & a \\
\vdots & \vdots & \vdots & & \vdots \\
b & b & b & \dots & x_{n}
\end{vmatrix}
=
\begin{cases}
\left[ x_{1} + a \sum\limits^{n}_{k=2} \frac{x_{1}-a}{x_{k}-a} \right] \cdot \prod\limits^{n}_{k=2} (x_{k}-a), & a = b \\
(x_{n}-b) D_{n-1} + \prod\limits^{n-1}_{k=1} (x_{k}-a), & a \neq b 
\end{cases}
\]


\chapter{System of Linear Equations}
\begin{definition}{System of Linear Equations}
    A system of linear equations is a collection of one or more linear equations involving the same set of variables. 
    For example, a system of \( m \) linear equations in \( n \) variables can be written as:
    \[
    \begin{cases}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
    \end{cases}
    \]
    where \( x_1, x_2, \ldots, x_n \) are the variables, \( a_{ij} \) are the coefficients, and \( b_i \) are the constants.

    A solution to the system is an ordered set of values for the variables that satisfies all equations simultaneously.
    If two systems have the same solution set, they are called equivalent systems, 
    whose relationship are equivalence.
\end{definition}

\section{Elimination Method}
\section{Linear Space}
\begin{definition}{Linear Space over Field \(F\)}
    Let \( V \) be a non-empty set, and \( F \) be a field.
    If the following two operations (binary mapping) are defined on \( V \):
    \begin{description}
        \item [Vector addition] For any \( \alpha, \beta \in V \), 
            there exists \( \gamma \in V \) such that \( \gamma = \alpha + \beta \).
        \item [Scalar multiplication] For any \( \alpha \in V \) and \( c \in F \), 
            there exists \( \gamma \in V \) such that \( \gamma = c \cdot \alpha \).
    \end{description}
    and the following axioms are satisfied (for any \( \alpha, \beta, \gamma \in V \) and \( k, l \in F \)):
    \begin{description}
        \item [A1. Commutativity of Addition] \( \alpha + \beta = \beta + \alpha \).
        \item [A2. Associativity of Addition] \( (\alpha + \beta) + \gamma = \alpha + (\beta + \gamma) \).
        \item [A3. Existence of Additive Identity] There exists an element \( 0 \in V \) such that 
            \( \alpha + 0 = 0 + \alpha = \alpha \).
        \item [A4. Existence of Additive Inverse] There exists an element 
            \( -\alpha \in V \) such that \( \alpha + (-\alpha) = (-\alpha) + \alpha = 0 \).
        \item [M1. Compatibility of Scalar Multiplication with Field Multiplication] \( k(l\alpha) = (kl)\alpha\).
        \item [M2. Identity Element of Scalar Multiplication] \( 1\cdot\alpha = \alpha\), where \( 1\) is the multiplicative identity in \( F\).
        \item [D1. Distributivity of Scalar Multiplication with respect to Vector Addition] 
            \( k(\alpha + \beta) = k\alpha + k\beta\).
        \item [D2. Distributivity of Scalar Multiplication with respect to Field Addition] \( (k+l)\alpha = k\alpha + l\alpha\).
    \end{description}
    Then \( V \) is called a linear space (or vector space) over the field \( F \).
\end{definition}

\begin{remark}
    In the definition of linear space, \textbf{A1. Commutativity of Addition} can be derived from the other axioms,
    so it is not strictly necessary to include it as an axiom.
\end{remark}

\vspace{0.7cm}
Give some examples of linear spaces:
\begin{enumerate}
    \item The ring of univariate polynomials \( P[x] \) over the number field \( P \), 
        with the usual polynomial addition and scalar multiplication by elements from \( P \), 
        forms a linear space over the number field \( P \). 
        If we consider only polynomials of degree less than \( n \), along with the zero polynomial, 
        this also forms a linear space over \( P \), denoted as \( P[x]_n \).
        
    \item The set of all positive real numbers \( \mathbb{R}^+ \), with addition and scalar multiplication defined as:
        \[
        a \oplus b = ab, \quad k \circ a = a^k,
        \]
        forms a linear space over the number field \( \mathbb{R} \).
    
    \item The set of all real-valued functions, 
        under function addition and scalar multiplication by elements from \( \mathbb{R} \), 
        forms a linear space over the field \( \mathbb{R} \).
    
    \item The field \( P \) itself, under its own addition and multiplication, forms a linear space over itself.
    
    \item The set of all \(n\)-dimensional vectors (ordered \( n \)-tuples of elements from \( P \))
        over the number field \( P \),
        with the usual vector addition and scalar multiplication, 
        forms a linear space over the number field \( P \).
\end{enumerate}

\vspace{0.7cm}
In this section, we primarily examine linear spaces consisting of n-dimensional vectors over the real number field \( \mathbb{R} \).
As a matter of fact, all elements of a linear space can be called vectors,
and the definitions and properties of vectors in \( \mathbb{R}^n \) can be generalized to any linear space.


\begin{leftbarTitle}{Linear Independence}\end{leftbarTitle}
\begin{definition}{Linear Combination}
    Let \( V \) be a linear space over the field \( F \), 
    and let \( \alpha_1, \alpha_2, \ldots, \alpha_n \in V \).
    For any scalars \( k_1, k_2, \ldots, k_n \in F \), the vector
    \[
    \beta = k_1\alpha_1 + k_2\alpha_2 + \cdots + k_n\alpha_n
    \]
    is called a \textbf{linear combination} of the vectors \( \alpha_1, \alpha_2, \ldots, \alpha_n \)
    (we also say that \( \beta \) is linearly expressed by \( \alpha_1, \alpha_2, \ldots, \alpha_n \)),
    where \( k_i \) are called the coefficients of the linear combination.
\end{definition}

{Linear Representation and Equivalence of Vector Groups}

If every vector \( \alpha_i \) (\( i = 1, 2, \dots, t \)) in the vector group 
\( \alpha_1, \alpha_2, \dots, \alpha_t \) can be linearly expressed by the vector group 
\( \beta_1, \beta_2, \dots, \beta_s \), then the vector group 
\( \alpha_1, \alpha_2, \dots, \alpha_t \) is said to be linearly expressed by the vector group 
\( \beta_1, \beta_2, \dots, \beta_s \).

If two vector groups can be linearly expressed by each other, 
they are said to be \textbf{equivalent}.

\begin{remark}
    Linear representation can be used to identify redundant equations in a system of linear equations. 
    In a system of linear equations, if one equation can be linearly expressed by other equations, 
    then through elementary transformations, the corresponding row in the matrix can be reduced to a row of zeros.

    If two vector groups are equivalent, then their corresponding systems of linear equations are equivalent.
\end{remark}

\begin{definition}{Linear Dependence and Independence of Vector Groups}
    A vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 2) \) is said to be \textbf{linearly dependent} 
    if there exists a vector in the group that can be linearly expressed by the other vectors. 
    
    Another equivalent definition is:
    A vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 1) \) is linearly dependent 
    if there exist \underline{not all zero} scalars \( k_1, k_2, \dots, k_t \) in the field \( F \) such that:
    \[
    k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_t \alpha_t = 0.
    \]

    Conversely, a vector group \( \alpha_1, \alpha_2, \dots, \alpha_t (t \geq 1) \) is \textbf{linearly independent} if:
    the fact that
    \[
    k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_t \alpha_t = 0
    \]
    implies:
    \[
    k_1 = k_2 = \dots = k_t = 0.
    \]    
\end{definition}

\begin{note}
    If a subset of a vector group is linearly dependent, then the entire vector group is linearly dependent.
    If a vector group is linearly independent, then any non-empty subset of the group is also linearly independent.
    (\textbf{Partial dependence implies overall dependence; overall independence implies partial independence.})
    
    Particularly, since two proportional vectors are linearly dependent, a linearly independent vector group cannot contain two proportional vectors.
\end{note}

A few propositions about linear dependence and independence are given below:
\begin{proposition}
\begin{enumerate}
    \item Generally, for a vector group \( \alpha_i = (a_{i1}, a_{i2}, \dots, a_{in}), \, i = 1, 2, \dots, s \), 
        the necessary and sufficient condition for linear dependence is that the equation:
        \[
        x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_s \alpha_s = 0
        \]
        or the homogeneous system of linear equations:
        \[
        \begin{cases}
        a_{11}x_1 + a_{21}x_2 + \dots + a_{s1}x_s = 0 \\
        a_{12}x_1 + a_{22}x_2 + \dots + a_{s2}x_s = 0 \\
        \vdots \\
        a_{1n}x_1 + a_{2n}x_2 + \dots + a_{sn}x_s = 0
        \end{cases}
        \]
        has a non-zero solution.

    \item  If a vector group \( \alpha_i = (a_{i1}, a_{i2}, \dots, a_{in}), \, i = 1, 2, \dots, s \) 
        is linearly independent, then adding one component to each vector to form an \( n+1 \)-dimensional 
        vector group \( \beta_i = (a_{i1}, a_{i2}, \dots, a_{in}, a_{i,n+1}), \, i = 1, 2, \dots, s \), 
        will also be linearly independent.

    \item For two vector groups \( \alpha_1, \alpha_2, \dots, \alpha_r \) and \( \beta_1, \beta_2, \dots, \beta_s \),
        if the former can be linearly expressed by the latter:
        \begin{enumerate}[label=\roman*.]
            \item and \( r > s \), then the former is linearly dependent.
            \item and \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent, then \( r \leqslant s \).
        \end{enumerate}

    \item Any \( n+1 \) \( n \)-dimensional vectors must be linearly dependent.
    
    \item Two equivalent linearly independent vector groups must contain the same number of vectors.

    \item If a vector group \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent, 
        and adding a vector \( \beta \) to form the vector group \( \alpha_1, \alpha_2, \dots, \alpha_r, \beta \) 
        makes it linearly dependent, then \( \beta \) can be uniquely expressed as a linear combination of 
        \( \alpha_1, \alpha_2, \dots, \alpha_r \).

    \item If a vector \( \beta \) can be linearly expressed by a vector group \( \alpha_1, \alpha_2, \dots, \alpha_r \), 
        then \( \alpha_1, \alpha_2, \dots, \alpha_r \) is linearly independent
        if and only if the expression for \( \beta \) is unique.
\end{enumerate}
\end{proposition}

\begin{leftbarTitle}{Dimension, Basis, and Coordinates}\end{leftbarTitle}
\begin{definition}{Dimension}
    If a linear space \( V \) over field \(F\) contains \(n\) vectors that are linearly independent,
    but any more than \(n\) vectors are linearly dependent, 
    then \( V \) is called an \( n \)-dimensional linear space,
    noted as \( \dim V_{F} = n \), simplified as \( \dim V = n \) when the field is clear from context.

    If a linear space does contain as many vectors that are linearly independent as you wish, 
    it is called an infinite-dimensional linear space.
\end{definition}

\begin{definition}{Basis and Coordinates}
    In an \( n \)-dimensional linear space \( V \), 
    if \( \alpha_1, \alpha_2, \dots, \alpha_n \) are \( n \) linearly independent vectors in \( V \),
    then they form a \textbf{basis} of \( V \).

    Any vector \( \alpha \in V \) can be uniquely expressed as a linear combination of the basis vectors:
    \[
    \alpha = k_1\alpha_1 + k_2\alpha_2 + \dots + k_n\alpha_n,
    \]
    where \( k_1, k_2, \dots, k_n \in F \) are called the \textbf{coordinates} of \( \alpha \) with respect to the basis 
    \( \alpha_1, \alpha_2, \dots, \alpha_n \).
\end{definition}

The definition leads to a theorem that is easy to see:
If there are \( n \) linearly independent vectors in a linear space \( V \),
and any vector in \( V \) can be expressed as a linear combination of these \( n \) vectors,
then these \( n \) vectors form a basis of \( V \), and \( V \) is an \( n \)-dimensional linear space.


\begin{leftbarTitle}{Linear Subspaces}\end{leftbarTitle}
\begin{definition}{Linear Subspace}
    Let \( V \) be a linear space over field \( F \), 
    and let \( W \subseteq V \) be a non-empty subset of \( V \).
    If \( W \) itself is a linear space under the same addition and scalar multiplication as in \( V \),
    then \( W \) is called a \textbf{linear subspace} of \( V \).
    
    Zero Subspace and whole space \( V \) itself are called trivial subspaces of \( V \).
\end{definition}

It can be easily derived that:
Let \(W\) be a non-empty subset of a linear space \(V\).
Then \(W\) is a linear subspace of \(V\) if and only if:
\begin{enumerate}
    \item For any \( \alpha, \beta \in W \), \( \alpha + \beta \in W \).
    \item For any \( \alpha \in W \) and \( k \in F \), \( k\alpha \in W \).
\end{enumerate}


\section{Rank of Vector Groups and Matrices}
\begin{leftbarTitle}{Maximal Linearly Independent Group and Rank of Vector Groups}\end{leftbarTitle}
\begin{definition}{Maximal Linearly Independent Group}
    A subset of a vector group is called a \textbf{maximal linearly independent group} if:
    \begin{enumerate}
        \item The subset itself is linearly independent.
        \item Adding any vector (if available) from the vector group to this subset makes the new subset linearly dependent.
    \end{enumerate}
\end{definition}

\begin{property}
    \begin{enumerate}
    \item The maximal linearly independent group is not unique; 
        any maximal linearly independent group is equivalent to the original vector group.
    \item All maximal linearly independent groups of a vector group contain the same number of vectors.
\end{enumerate}
\end{property}

\begin{definition}{Rank of Vector Groups}
    The number of vectors contained in the maximal linearly independent group of a vector group 
    is called the \textbf{rank} of the vector group.
\end{definition}

\begin{property}
    \begin{enumerate}
    \item A vector group is linearly independent if and only if its rank is equal to the number of vectors it contains.

    \item Equivalent vector groups have the same rank, but vector groups with the same rank are not necessarily equivalent.

    \item If rank of a vector group is \( r \), 
        then any \( r \) linearly independent vectors from the group form a maximal linearly independent group.

    \item A vector group containing non-zero vectors always has a maximal linearly independent group, 
        and any linearly independent subset of vectors can be expanded into a maximal linearly independent group.

    \item If vector group (I) can be linearly expressed by vector group (II), then the rank of (I) does not exceed the rank of (II).
\end{enumerate}
\end{property}




\begin{leftbarTitle}{Rank of Matrices}\end{leftbarTitle}

\section{Solution to Linear Systems}
\begin{leftbarTitle}{Necessary and Sufficient Conditions for the Existence of Solutions}\end{leftbarTitle}

\begin{leftbarTitle}{The Structure of Solutions to Homogeneous Systems of Linear Equations}\end{leftbarTitle}
For a homogeneous system of linear equations, 
it is evident that the linear combination of solutions is still a solution to the system. 
Thus, the concept of a \textbf{fundamental system of solutions} is introduced.

\begin{definition}{Fundamental System of Solutions}
    The homogeneous system of linear equations:
    \[
    \begin{cases}
    a_{11}x_{1} + a_{12}x_{2} + \dots + a_{1n}x_{n} = 0 \\
    a_{21}x_{1} + a_{22}x_{2} + \dots + a_{2n}x_{n} = 0 \\
    \vdots \\
    a_{s1}x_{1} + a_{s2}x_{2} + \dots + a_{sn}x_{n} = 0
    \end{cases}
    \]
    (or in matrix form \( AX = O \))
    has a set of solutions \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \), which is called a \textbf{fundamental system of solutions}, 
    if:
    \begin{enumerate}
        \item Any solution of the system can be expressed as a linear combination of \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \).
        \item \( \eta_{1}, \eta_{2}, \dots, \eta_{t} \) are linearly independent\footnote{
            ensuring there are no redundant solutions in the fundamental system of solutions
        }.
    \end{enumerate}
\end{definition}

\begin{remark}
    This definition is similar to the maximal linearly independent group of a vector group. 
    In fact, the fundamental solution set is a maximal linearly independent group of the solution vector group. 
    Therefore, the number of solutions it contains equals its rank.
\end{remark}


\vspace{0.7cm}
All solutions of a homogeneous system of linear equations (\( AX = O \)) form a linear space (solution space),
whose basis is the fundamental system of solutions.
Dimension of solution space is \( n - r(A) \) (where \( n \) is the number of variables, 
and \( r(A) \) is the rank of the coefficient matrix \( A \)). 
Thus, a fundamental system of solutions contains exactly \( n - r(A) \) solutions.
    



\begin{leftbarTitle}{The Structure of Solutions to Nonhomogeneous Linear Systems}\end{leftbarTitle}
The homogeneous system of linear equations \( AX = O \) is called the \textbf{derived system} of 
the nonhomogeneous system \( AX = B \).

It can be easily proven that:
\begin{itemize}
    \item The difference between two solutions of \( AX = B \) is a solution of its derived system \( AX = 0 \).
    \item The sum of a solution of \( AX = B \) and a solution of \( AX = 0 \) is still a solution of \( AX = B \).
\end{itemize}

\vspace{0.7cm}
If \( \gamma_{0} \) is a particular solution of \( AX = B \), 
then any solution \( \gamma \) of \( AX = B \) can be expressed as:
\[
\gamma = \gamma_{0} + \eta,
\]
where \( \eta \) is a solution of \( AX = 0 \).

Thus, for any particular solution \( \gamma_{0} \) of \( AX = B \), 
when \( \eta \) takes all the solutions of its derived system \( AX = 0 \), 
the above formula provides all the solutions of \( AX = B \).

\chapter{Matrices}
\section{Basic Operations}
\begin{leftbarTitle}{Addition}\end{leftbarTitle}
\begin{leftbarTitle}{Scalar Multiplication}\end{leftbarTitle}
\begin{leftbarTitle}{Transpose}\end{leftbarTitle}
\begin{leftbarTitle}{Matrix Multiplication}\end{leftbarTitle}

\begin{theorem}{Determinant of a Product}
    For square matrices \( A \) and \( B \) of the same order, 
    the determinant of their product is equal to the product of their determinants:
    \[
    |AB| = |A| \cdot |B|.
    \]

    It can be easily extended to multiple matrices:
    \[
    |A_1 A_2 \cdots A_k| = |A_1| \cdot |A_2| \cdots |A_k|.
    \]
\end{theorem}

\begin{theorem}{Cauchy-Binet Formula}
    Let \( A = (a_{ij})_{m \times n} \) and \( B = (b_{ij})_{n \times m} \):  
    \begin{enumerate}
        \item If \( m > n \), then \( |AB| = 0 \);
        \item If \( m \leq n \), then \( |AB| \) is equal to 
        the sum of products of all \( m \)-step minors of \( A \) and 
        the corresponding \( m \)-step minors of \( B \), that is:
        \[
        |AB| = \sum_{1 \leq v_1 < v_2 < \dots < v_m \leq n} 
        \begin{vmatrix}
        A\begin{pmatrix}
        1, 2, \dots, m \\
        v_1, v_2, \dots, v_m
        \end{pmatrix}
        \end{vmatrix}
        \cdot 
        \begin{vmatrix}
        B\begin{pmatrix}
        v_1, v_2, \dots, v_m \\
        1, 2, \dots, m
        \end{pmatrix}
        \end{vmatrix}.
        \]
    \end{enumerate}
\end{theorem}




\begin{leftbarTitle}{Matrix Equivalence}\end{leftbarTitle}
\begin{definition}{Matrix Equivalence}
    Two matrices \( A \) and \( B \) are said to be \textbf{equivalent}, 
    denoted as \( A \cong B \),
    if they can be transformed into one another by a combination of elementary row and column operations.
\end{definition}

\begin{note}
    Obviously, two matrices are equivalent if and only if they have the same rank.
\end{note}

\begin{definition}{Canonical Form of Matrix Equivalence}
    For any matrix \( A \) with rank \( r \), 
    it can be equivalent to the matrix
    \[
    J_r = \begin{pmatrix}
    I_r & O \\
    O & O
    \end{pmatrix}_{m \times n},
    \]
    which is called the \textbf{canonical form} of matrix equivalence.
\end{definition}



\section{Special Matrices}
\begin{leftbarTitle}{Diagonal Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Fundamental Matrix}\end{leftbarTitle}
\begin{definition}{Fundamental Matrix}
    \(E_{ij}\) is called a \textbf{fundamental matrix} if the element in the \(i\)-th row and \(j\)-th column is 1,
    and all other elements are 0.
\end{definition}

\begin{property}
    
\end{property}

\begin{leftbarTitle}{Triangular Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Elementary Matrix}\end{leftbarTitle}
\begin{definition}{Elementary Matrix}
    An \textbf{elementary matrix} is obtained by performing a single elementary row operation on an identity matrix \(E\).
    There are three types of elementary matrices (also hold for column operations):
    \begin{description}
        \item [Row swapping] Interchanges rows \(i\) and \(j\) of \(E\): \(P(i,j)\).
        \item [Row scaling] Multiplies row \(i\) of \(E\) by a non-zero scalar \(c\): \(P(i(c))\).
        \item [Row addition] Adds \(k\) times row \(j\) to row \(i\) of \(E\): \(P(i,j(k))\).
    \end{description}
\end{definition}

\begin{note}
    It can be proven that \(P(i,j)\) can be implemented by a sequence of \(P(i(c))\) and \(P(i,j(k))\) operations.
\end{note}

\begin{leftbarTitle}{Symmetric Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Circulant-Shift Permutation Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Nilpotent Matrix}\end{leftbarTitle}

\begin{leftbarTitle}{Row Full Rank Matrix}\end{leftbarTitle}

\section{Inverse Matrix}
\begin{leftbarTitle}{Inverse Matrix and Its Operations}\end{leftbarTitle}

\begin{leftbarTitle}{Equivalent Propositions and Method of Inversion}\end{leftbarTitle}

\begin{leftbarTitle}{Generalized Inverse}\end{leftbarTitle}

\section{Block Matrix}

\begin{theorem}{Determinant Reduction Formula}
    Let \(A_{m\times m}, B_{m\times n}, C_{n\times m}, D_{n\times n}\) be matrices. Then:
    \begin{enumerate}
        \item If \( A \) is invertible, then:
            \[
            \begin{vmatrix}
            A & B \\
            C & D
            \end{vmatrix}
            = |A| \cdot |D - CA^{-1}B|.
            \]
        \item If \( D \) is invertible, then:
            \[
            \begin{vmatrix}
            A & B \\
            C & D
            \end{vmatrix}
            = |D| \cdot |A - BD^{-1}C|.
            \]
        \item If both \( A \) and \( D \) are invertible, then:
            \[
            |D| \cdot |A - BD^{-1}C| = |A| \cdot |D - CA^{-1}B|.
            \]
    \end{enumerate}
\end{theorem}

\begin{remark}
    The mnemonic is: 
    For \( \begin{vmatrix} A & B \\ C & D \end{vmatrix} \), for example, if \( A \) is invertible, 
    one factor is \( |A| \), and the other factor is \( D \) (the diagonal element of \( A \)) minus 
    the product of the other three terms arranged clockwise, where the middle one is the inverse matrix.
\end{remark}




\section{Operations of Rank}
\begin{proposition}
    The matrices \( A \) and \( B \) in the following operations do not need to be square matrices; 
    they only need to be compatible for multiplication or addition.
    \begin{description}
        \item [1. Addition]
        \[
        \operatorname{rank}(A + B) \leqslant \operatorname{rank}(A) + \operatorname{rank}(B).
        \]
        \item [2. Multiplication]
        \[
        \operatorname{rank}(AB) \leqslant \operatorname{rank}(A), \quad \operatorname{rank}(AB) \leqslant \operatorname{rank}(B).
        \]
        \item[2.1. Sylvester's Inequality]
        \[
        \operatorname{rank}(AB) \geqslant \operatorname{rank}(A) + \operatorname{rank}(B) - n \quad (A_{s \times n}, B_{n \times m}).
        \]
        Specially, if \( AB = O \), then:
        \[
        \operatorname{rank}(A) + \operatorname{rank}(B) \leqslant n.
        \]

        \item[2.2. Frobenius Inequality]
        \[
        \operatorname{rank}(ABC) \geqslant \operatorname{rank}(AB) + \operatorname{rank}(BC) - \operatorname{rank}(B).
        \]

        \item [3. Transpose]
        \[
        \operatorname{rank}(AA^T) = \operatorname{rank}(A^TA) = \operatorname{rank}(A) = \operatorname{rank}(A^T).
        \]

        \item [4. Inverse]
        \[
        \operatorname{rank}(A) = \operatorname{rank}(A^{-1}) = n.
        \]

        \item[5. Block Matrix]
        \[
        \operatorname{rank}\begin{pmatrix} A & O \\ O & D \end{pmatrix} = \operatorname{rank}(A) + \operatorname{rank}(D).
        \]
    \end{description}
\end{proposition}

\section{Low-Rank Update}
Due to all the row and column vectors of a rank-\(1\) matrix are linearly dependent,
it can be expressed as the outer product of two non-zero vectors;
in other words, a rank-\(1\) matrix can be expressed as \( \alpha \beta^T \),
where \( \alpha \) and \( \beta \) are non-zero column vectors.

Based on the decomposition \( A = \alpha \beta^T \), the matrix of rank-\(1\) has simplified calculation rules:
\begin{property}
    \begin{description}
        \item [Exponentiation]  
        For any positive integer \( k \geq 1 \),
        \[
        A^k = (\beta^T\alpha)^{k-1} \cdot A,
        \]
        where \( \beta^T\alpha \) is a constant (the inner product of vectors).
        \item [Rank Transmission]  
        If \( B \) is any matrix, then:
        \[
        \operatorname{rank}(AB) \leq 1 \quad \text{and} \quad \operatorname{rank}(BA) \leq 1,
        \]
        (rank 1 matrices multiplied by arbitrary matrices result in ranks not exceeding 1).
    \end{description}
\end{property}



\begin{theorem}{Sherman-Morrison Formula}
    If \( A\in \mathbb{R}^{n\times n} \) is an invertible matrix, 
    and \( \alpha, \beta\in \mathbb{R}^n \) are column vectors, 
    then \( A + \alpha \beta^T \) is invertible if and only if \( 1 + \beta^T A^{-1} \alpha \neq 0 \). 
    In this case, the inverse of \( A + \alpha \beta^T \) is given by:
    \[
    \left(A + \alpha \beta^T\right)^{-1} = A^{-1} - \frac{A^{-1} \alpha \beta^T A^{-1}}{1 + \beta^T A^{-1} \alpha},
    \]
    where \(\alpha\beta^{\mathrm{T}}\) is the outer product of \(\alpha\) and \(\beta\).
\end{theorem}

\begin{note}
    Combining the properties of determinants, 
    we can derive the determinant version of the Sherman-Morrison formula:
    \[
    \left|A + \alpha \beta^T\right| = |A| \cdot \left(1 + \beta^T A^{-1} \alpha\right),
    \]
    which is known as the \textbf{matrix determinant lemma}.

    The theorem can also be stated in terms of the adjugate matrix of \( A \):
    \[
    \det(A + uv^T) = \det(A) + v^T \operatorname{adj}(A) u,
    \]
    in which case it applies whether or not the matrix \( A \) is invertible.
\end{note}



\chapter{Linear Spaces}
\section{Linear Spaces and Their Bases}

\begin{leftbarTitle}{Basis Transformation and Coordinate Transformation}\end{leftbarTitle}


\section{Subspaces}
\begin{leftbarTitle}{Intersection and Sum of Subspaces}\end{leftbarTitle}
\begin{definition}{Intersection and Sum of Subspaces}
    Let \( V \) be a linear space over field \( F \), 
    and let \( V_1, V_2 \subseteq V \) be two subspaces of \( V \).
    The \textbf{intersection} of \( V_1 \) and \( V_2 \) is defined as:
    \[
    V_1 \cap V_2 = \{ \alpha | \alpha \in V_1 \text{ and } \alpha \in V_2 \}.
    \]
    The \textbf{sum} of \( V_1 \) and \( V_2 \) is defined as:
    \[
    V_1 + V_2 = \{ \alpha | \alpha = \alpha_1 + \alpha_2, 
    \alpha_1 \in V_1, \alpha_2 \in V_2 \}.
    \]
\end{definition}

If \( V_1 \) and \( V_2 \) are two subspaces of \( V \), 
then both their intersection \( V_1 \cap V_2 \) and their sum \( V_1 + V_2 \) are also subspaces of \( V \).
Simultaneously, \( V_{1}+V_{2} \) is the smallest subspace of \( V \) 
that contains both \( V_1 \) and \( V_2 \) (\(V_{1}\cup V_{2}\)).

\begin{leftbarTitle}{Dimension Formula}\end{leftbarTitle}
\begin{proposition}
    In a finite-dimensional linear space \( V \),
    \[
    L(\alpha_1, \alpha_2, \dots, \alpha_s) + L(\beta_1, \beta_2, \dots, \beta_t) = 
    L(\alpha_1, \alpha_2, \dots, \alpha_s, \beta_1, \beta_2, \dots, \beta_t).
    \]
\end{proposition}

\begin{theorem}{Dimension Formula}
    Let \( V \) be a finite-dimensional linear space over field \( F \), 
    and let \( V_1, V_2 \subseteq V \) be two subspaces of \( V \). Then:
    \[
    \dim(V_1) + \dim(V_2) = \dim(V_1 + V_2) + \dim(V_1 \cap V_2).
    \]
    
\end{theorem}

\begin{leftbarTitle}{Direct Sum of Subspaces}\end{leftbarTitle}
\begin{definition}{Direct Sum of Subspaces}
    Let \( V \) be a linear space over field \( F \), 
    and let \( V_1, V_2 \subseteq V \) be two subspaces of \( V \).
    If any vector \( \alpha \in V_1 + V_2 \) can be uniquely expressed as:
    \[
    \alpha = \alpha_1 + \alpha_2, \quad \alpha_1 \in V_1, \alpha_2 \in V_2,
    \]
    then \( V_1 + V_2 \) is called the \textbf{direct sum} of \( V_1 \) and \( V_2 \),
    denoted as \( V_1 \oplus V_2 \).
\end{definition}

\begin{proposition}
    The necessary and sufficient condition for \( V_1 + V_2 \) to be a direct sum is:
    \begin{enumerate}
        \item \(0 = 0 + 0\) (the zero vector can only be expressed as the sum of two zero vectors); 
        \item \( V_1 \cap V_2 = \{0\} \) (the intersection of the two subspaces is only the zero vector);
        \item \(\operatorname{dim}(V_{1}+V_{2})=\operatorname{dim}(V_{1})+\operatorname{dim}(V_{2})\),
            or equivalently, \(\operatorname{dim}(V_{1} \cap V_{2})=0\).
        \item Any bases of \( V_1 \) and \( V_2 \) together form a basis of \( V_1 + V_2 \).
    \end{enumerate}
\end{proposition}




\section{Isomorphisms}

\section{Quotient Spaces}

\chapter{Linear Mappings}
\section{Linear Mappings and Their Computation}
\begin{leftbarTitle}{Definition of Linear Mappings}\end{leftbarTitle}
\begin{definition}{Linear Mapping}
    Let \( V \) and \( V' \) be linear spaces over field \( F \).
    A mapping \( \mathcal{A}: V \to V' \) is called a \textbf{linear mapping} if:
    \[
    \mathcal{A}(\alpha + \beta) = \mathcal{A}\alpha + \mathcal{A}\beta, \quad \forall \alpha, \beta \in V,
    \]
    and
    \[
    \mathcal{A}(k\alpha) = k\mathcal{A}\alpha, \quad \forall \alpha \in V, k \in F.
    \]

    If \( V = V' \), then \( \mathcal{A} \) is called a \textbf{linear transformation} of \( V \).
    The linear mapping from \( V \) over field \( F \) to \(F\) is called a \textbf{linear function} on \( V \).
\end{definition}

\begin{remark}
    The field naturally constitutes a one-dimensional linear space over itself, 
    so a linear function is a special case of a linear mapping.
\end{remark}


\begin{remark}
    The set of all linear mappings from $V$ to $V'$ over the field $F$ is denoted by $\mathrm{Hom}(V,V')$ or $\mathcal{L}(V,V')$.

    The set of all linear transformations is denoted by $\mathrm{Hom}(V)$, $\mathcal{L}(V)$, or $\mathrm{End}(V)$.

    The set of all $m \times n$ matrices over the number field $P$ is denoted by \(M_{m \times n}(P)\) or \(P^{m\times n}\).
\end{remark}


\begin{leftbarTitle}{Existence and Uniqueness of Linear Mappings}\end{leftbarTitle}

\begin{leftbarTitle}{Operations of Linear Mappings}\end{leftbarTitle}
In the following, all linear spaces are over the field \(F\).
\begin{description}
    \item[Addition] 
        Let \(\mathcal{A}, \mathcal{B} \in \mathrm{Hom}(V,V')\). 
        The sum of \(\mathcal{A}\) and \(\mathcal{B}\), denoted \(\mathcal{A+B}\), is defined by:
        \[
        (\mathcal{A+B})(\alpha) = \mathcal{A}(\alpha) + \mathcal{B}(\alpha), \quad \forall \alpha \in V.
        \]
        The sum of linear maps is still a linear map, i.e., \(\mathcal{A+B} \in \mathrm{Hom}(V,V')\).
        \begin{enumerate}
            \item \textbf{Commutativity:} \(\mathcal{A+B = B+A}\),
            \item \textbf{Associativity:} \(\mathcal{(A+B)+C = A+(B+C)}\),
            \item \textbf{Additive Identity:} The zero map \(\mathcal{0}\) satisfies \(\mathcal{A+0 = A}\),
            \item \textbf{Additive Inverse:} For every \(\mathcal{A}\), the additive inverse \(-\mathcal{A}\) 
                satisfies \(\mathcal{A+(-A) = 0}\).
        \end{enumerate}

    \item[Scalar Multiplication] 
        Let \(k \in F\) and \(\mathcal{A} \in \mathrm{Hom}(V,V')\). The scalar product \(k \mathcal{A}\) is defined by:
        \[
        (k\mathcal{A})(\alpha) = k \mathcal{A}(\alpha), \quad \forall \alpha \in V.
        \]
        Scalar multiplication of a linear map is still a linear map, i.e., \(k \mathcal{A} \in \mathrm{Hom}(V,V')\).
        \begin{enumerate}
            \item \textbf{Unit Identity:} \(1 \cdot \mathcal{A} = \mathcal{A}\),
            \item \textbf{Associativity of Scalars:} \((kl)\mathcal{A} = k(l\mathcal{A})\),
            \item \textbf{Distributive Properties:}
            \begin{align*}
                k(\mathcal{A+B}) &= k\mathcal{A} + k\mathcal{B}, \\
                (k+l)\mathcal{A} &= k\mathcal{A} + l\mathcal{A}.
            \end{align*}
        \end{enumerate}
    \item[Multiplication (Composition of Linear Maps)]
        Let \(\mathcal{A} \in \mathrm{Hom}(V',W)\) and \(\mathcal{B} \in \mathrm{Hom}(U,V')\). 
        The product (composition) of \(\mathcal{A}\) and \(\mathcal{B}\), denoted \(\mathcal{AB}\), is defined by:
        \[
        (\mathcal{AB})(\alpha) = \mathcal{A}(\mathcal{B}(\alpha)), \quad \forall \alpha \in U.
        \]
        The composition of linear maps is still a linear map, i.e., \(\mathcal{AB} \in \mathrm{Hom}(U,W)\).
        \begin{enumerate}
            \item \textbf{Associativity of Composition:} \((\mathcal{AB})\mathcal{C} = \mathcal{A}(\mathcal{BC})\),
            \item \textbf{Distributive Properties:}
            \begin{align*}
                \mathcal{A}(\mathcal{B+C}) &= \mathcal{AB + AC}, \\
                (\mathcal{A+B})\mathcal{C} &= \mathcal{AC+BC},
            \end{align*}
            \item \textbf{Non-Commutativity:} In general, \(\mathcal{AB} \neq \mathcal{BA}\),
            \item \textbf{Identity Transformation:} The identity transformation \(\mathcal{E}\) satisfies:
            \[
            \mathcal{EA} = \mathcal{AE} = \mathcal{A}, \quad \forall \mathcal{A} \in \mathrm{Hom}(V).
            \]
        \end{enumerate}
\end{description}


\vspace{0.7cm}
Let \(V\) and \(V'\) be vector spaces over the field \(F\).
\begin{enumerate}
    \item The collection of all linear mappings from \(V\) to \(V'\), denoted by \(\mathrm{Hom}(V,V')\), 
        under the addition and scalar multiplication defined above, forms a linear space over the field \(F\).
    
    \item The collection of all linear transformations on \(V\), denoted by \(\mathrm{Hom}(V)\), 
        under the addition and composition of maps defined above, forms a unital ring. 
        In this case, it is referred to as the \textbf{endomorphism ring} and is denoted by \(\mathrm{End}(V)\).
\end{enumerate}


\begin{definition}{Algebra over a Field}
    Let \(V\) be a linear space over the field \(F\) 
    equipped with an additional binary operation from \(V\times V\) to \(V\), denoted by \(\cdot\).
    Then \(V\) is called an \textbf{algebra over the field \(F\)} if
    (forall \(\alpha, \beta, \gamma \in V\) and \(k, l \in F\)):
    \begin{description}
        \item [Right Distributivity] \((\alpha + \beta) \cdot \gamma = \alpha \cdot \gamma + \beta \cdot \gamma\);
        \item [Left Distributivity] \(\gamma \cdot (\alpha + \beta) = \gamma \cdot \alpha + \gamma \cdot \beta\);
        \item [Compatibility with Scalar Multiplication] \((k\alpha)\cdot(l \beta) = kl(\alpha \cdot \beta)\).
    \end{description}
\end{definition}
\begin{remark}
    The left and right distributivity conditions can be combined into a single condition:
    \(V\) forms a unital ring under the addition and scalar multiplication.
\end{remark}

It is evident that \(\mathrm{Hom}(V)\) is an algebra over the field \(F\).


\section{Kernel and Image of Linear Mappings}
\begin{definition}{Kernel and Image of Linear Mappings}
    Let \( V \) and \( V' \) be linear spaces over field \( F \), 
    and let \( \mathcal{A}: V \to V' \) be a linear mapping. 
    The \textbf{kernel} of \( \mathcal{A} \) is defined as:
    \[
    \operatorname{Ker}(\mathcal{A}) = \{ \alpha | \alpha \in V, \mathcal{A}(\alpha) = 0 \}.
    \]
    The \textbf{image} of \( \mathcal{A} \) is defined as:
    \[
    \operatorname{Im}(\mathcal{A}) = \{ \beta | \beta = \mathcal{A}(\alpha), 
    \alpha \in V, \beta \in V' \}.
    \]
    They can be also denoted as \( \mathcal{A}^{-1}(0) \) and \( \mathcal{A}(V) \) respectively.

    The rank of \( \operatorname{Im}(\mathcal{A} \) is called the \textbf{rank} of \( \mathcal{A} \);
    the dimension of \( \operatorname{Ker}\mathcal{A} \) is called the \textbf{nullity} of \( \mathcal{A} \).
\end{definition}

\begin{property}
    
\end{property}


\begin{theorem}{Rank-Nullity Theorem}
    Let \( V \) and \( V' \) be finite-dimensional linear spaces over field \( F \), 
    and let \( \mathcal{A}: V \to V' \) be a linear mapping. Then:
    \[
    \dim(\operatorname{Ker}\mathcal{A}) + \dim(\operatorname{Im}\mathcal{A}) = \dim(V).
    \]
\end{theorem}


\begin{definition}{Cokernel}
    Let \( V \) and \( V' \) be linear spaces over field \( F \), 
    and let \( \mathcal{A}: V \to V' \) be a linear mapping. 
    The \textbf{cokernel} of \( \mathcal{A} \) is defined as the quotient space:
    \[
    \operatorname{Coker}(\mathcal{A}) = V' / \operatorname{Im}(\mathcal{A}),
    \]
    denoted as \( \operatorname{Coker}(\mathcal{A}) \).
\end{definition}

\section{Matrix Representation of Linear Mappings}
Let \( V \) and \( V' \) be finite-dimensional linear spaces over field \( F \), 
\(\operatorname{dim}(V) = n\) and \(\operatorname{dim}(V') = m\),
and let \( \mathcal{A}: V \to V' \) be a linear mapping.
Let \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \) be a basis of \( V \),
and \( \{\eta_1, \eta_2, \dots, \eta_m\} \) be a basis of \( V' \).
Then the image of each basis vector of \( V \) under \( \mathcal{A} \) can be expressed as the basis vectors of \( V' \):
\[
\begin{cases} 
    \mathcal{A}(\varepsilon_1) = a_{11}\eta_{1}+a_{21}\eta_{2}+\cdots+a_{m1}\eta_{m},   \\ 
    \mathcal{A}(\varepsilon_i) = a_{12}\eta_{1}+a_{22}\eta_{2}+\cdots+a_{m2}\eta_{m},   \\
    \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\vdots \\
    \mathcal{A}(\varepsilon_n) = a_{1n}\eta_{1}+a_{2n}\eta_{2}+\cdots+a_{mn}\eta_{m}.
\end{cases}
\]
It can be expressed in matrix form as:
\[
\mathcal{A}(\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n) = 
(\mathcal{A}\varepsilon_1, \mathcal{A}\varepsilon_2, \dots, \mathcal{A}\varepsilon_n) =
(\eta_1, \eta_2, \dots, \eta_m)A,
\]
where
\[A = (a_{ij})_{m \times n} =
\begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}.
\]
The matrix \( A \) is called the \textbf{matrix representation} of the linear mapping \( \mathcal{A} \)
under the bases \( \{\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n\} \) of \( V \) 
and \( \{\eta_1, \eta_2, \dots, \eta_m\} \) of \( V' \).

\vspace{0.7cm}


\section{Special Linear Transformations}
Some common linear transformations include:
\begin{description}
    \item[Identity Transformation] The identity transformation \( \mathcal{E} \) on \( V \) is defined by:
        \[
        \mathcal{E}(\alpha) = \alpha, \quad \forall \alpha \in V.
        \]

    \item[Zero Transformation] The zero transformation \( \mathcal{O} \) from \( V \) to \( V' \) is defined by:
    \[
    \mathcal{O}(\alpha) = 0, \quad \forall \alpha \in V.
    \]

    \item[Scalar Transformation] For a fixed scalar \( k \in F \), 
        the scalar transformation \( \mathcal{K} \) on \( V \) is defined by:
        \[
        \mathcal{K}(\alpha) = k\alpha, \quad \forall \alpha \in V.
        \]
        When \( k = 1 \), it is the identity transformation; 
        when \( k = 0 \), it is the zero transformation. 

    \item[Spin Axis Transformation] 
        The vectors on a plane form a two-dimensional linear space over real number field \( \mathbb{R} \).
        Rotate the plane counterclockwise by an angle \( \theta \) around the origin, 
        then the transformation of any vector on the plane is a linear transformation,
        denoted by \( \mathcal{P}_{\theta}\).
        Similarly, in three-dimensional space, rotating around a fixed axis by an angle \( \theta \) 
        also defines a linear transformation.
\end{description}

\vspace{0.7cm}
And some special linear transformations are given as follows:
\begin{definition}{Idempotent Transformation}
    A linear transformation \( \mathcal{A} \) on \( V \) is called an \textbf{idempotent transformation} if:
    \[
    \mathcal{A}^2 = \mathcal{A}.
    \]
\end{definition}

Two linear transformations \( \mathcal{A} \) and \( \mathcal{B} \) on \( V \) are called \textbf{orthogonal} if:
\[
\mathcal{A} \mathcal{B} = \mathcal{B} \mathcal{A} = \mathcal{O}.
\]

With the definitions above, we can define the projection transformation as follows:
\begin{definition}{Projection Transformation}
    Let \( V \) be a linear space over field \( F \),
    and let \( U, W \subseteq V \) be two subspaces of \( V \) such that \( V = U \oplus W \).
    For all \( \alpha \in V \), there exist unique \( \alpha_1 \in U \) and \( \alpha_2 \in W \) such that:
    \[
    \alpha = \alpha_1 + \alpha_2.
    \]
    Let
    \[ 
        \mathcal{P}_{U}: 
        \begin{aligned}
        &V \to V \\
        &\alpha \mapsto \alpha_1
        \end{aligned}
    \] 
    then \( \mathcal{P}_{U} \) is called the \textbf{projection} onto \( U \) along \( W \),
    which is a linear transformation on \( V \).
    Similarly, the projection onto \( W \) along \( U \) can be defined.
\end{definition}
\( \mathcal{P}_{U}\) satisfies and uniquely satisfies:
\[
\mathcal{P}_{U}( \alpha ) = 
\begin{cases} 
    \alpha, & \alpha \in U, \\ 
    0, & \alpha \in W .
\end{cases}
\]



\section{Linear Functions and Dual Spaces}



\chapter{Diagonalization}
\section{Similarity of Matrices}
\section{Eigenvectors and Diagonalization}
\subsection{Eigenvalues and Eigenvectors}

\subsection{Necessary and Sufficient Conditions for Diagonalization}
\begin{leftbarTitle}{Geometric Multiplicity of Eigenvectors}\end{leftbarTitle}
\begin{leftbarTitle}{Algebraic Multiplicity}\end{leftbarTitle}


\section{Space Decomposition and Diagonalization}

\subsection{Invariant Subspace}

\subsection{Hamilton-Cayley Theorem}

\section{Least Squares and Diagonalization}

\chapter{Jordan Forms}
\section{Polynomial Matrices}

\section{Invariant Factors}

\section{Rational Canonical Form}

\section{Elementary Divisors}

\section{Jordan Canonical Form}

\chapter{Quadratic Forms}
\section{Quadratic Forms and Their Standard Forms}
\section{Canonical Forms}
\section{Definite Quadratic Forms}


\chapter{Inner Product Spaces}
\section{Bilinear Forms}

\section{Real Inner Product Spaces}

\section{Metric Matrices and Standard Orthonormal Bases}

\section{Isomorphism of Real Inner Product Spaces}

\section{Orthogonal Completion and Orthogonal Projection}
\subsection{Orthogonal Completion}

\subsection{Least Squares Method}

\section{Orthogonal Transformations and Symmetric Transformations}
\subsection{Orthogonal Transformations}

\subsection{Symmetric Transformations}

\section{Unitary Spaces and Unitary Transformations}

\section{Symplectic Spaces}

\begin{thebibliography}{99} 
\bibitem{ch1} 丘维声, \emph{ 高等代数 (2nd edition) }, 清华大学出版社, 2019. 
\bibitem{ch2} 谢启鸿, 姚慕生, 吴泉水, \emph{ 高等代数学 (4th edition) }, 复旦大学出版社, 2022.
\bibitem{ch3} 王萼芳, 石生明 \emph{ 高等代数 (5th edition) }, 高等教育出版社, 2019.
\bibitem{ch4} 樊启斌, \emph{ 高等代数典型问题与方法 (1st edition) }, 高等教育出版社, 2021.
\bibitem{9} Wikipedia. \url{https://en.wikipedia.org/wiki/}.
\end{thebibliography}

\end{document}